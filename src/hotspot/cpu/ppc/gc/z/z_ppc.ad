//
// Copyright (c) 2021, 2023, Oracle and/or its affiliates. All rights reserved.
// Copyright (c) 2021 SAP SE. All rights reserved.
// DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
//
// This code is free software; you can redistribute it and/or modify it
// under the terms of the GNU General Public License version 2 only, as
// published by the Free Software Foundation.
//
// This code is distributed in the hope that it will be useful, but WITHOUT
// ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
// FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
// version 2 for more details (a copy is included in the LICENSE file that
// accompanied this code).
//
// You should have received a copy of the GNU General Public License version
// 2 along with this work; if not, write to the Free Software Foundation,
// Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
//
// Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
// or visit www.oracle.com if you need additional information or have any
// questions.
//

source_hpp %{

#include "gc/shared/gc_globals.hpp"
#include "gc/z/c2/zBarrierSetC2.hpp"
#include "gc/z/zThreadLocalData.hpp"

%}

source %{

#include "gc/z/zBarrierSetAssembler.hpp"

static void z_color(MacroAssembler* masm, Register dst, Register src) {
  assert_different_registers(dst, src);
  __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodBits);
  __ li(dst, barrier_Relocation::unpatched); // Load color bits.
  if (src == noreg) { // noreg encodes null.
    if (ZPointerLoadShift >= 16) {
      __ rldicl(dst, dst, 0, 64 - ZPointerLoadShift); // Clear sign extension from li.
    }
  } else {
    __ rldimi(dst, src, ZPointerLoadShift, 0); // Insert shifted pointer.
  }
}

static void z_uncolor(MacroAssembler* masm, Register ref) {
  __ srdi(ref, ref, ZPointerLoadShift);
}

static void check_color(MacroAssembler* masm, Register ref, bool on_non_strong) {
  int relocFormat = on_non_strong ? ZBarrierRelocationFormatMarkBadMask
                                  : ZBarrierRelocationFormatLoadBadMask;
  __ relocate(barrier_Relocation::spec(), relocFormat);
  __ andi_(R0, ref, barrier_Relocation::unpatched);
}

static void z_load_barrier(MacroAssembler* masm, const MachNode* node, Address ref_addr, Register ref) {
  Assembler::InlineSkippedInstructionsCounter skipped_counter(masm);
  if (node->barrier_data() == ZBarrierElided) {
    z_uncolor(masm, ref);
  } else {
    const bool on_non_strong =
      ((node->barrier_data() & ZBarrierWeak) != 0) ||
      ((node->barrier_data() & ZBarrierPhantom) != 0);

    check_color(masm, ref, on_non_strong);

    ZLoadBarrierStubC2* const stub = ZLoadBarrierStubC2::create(node, ref_addr, ref);
    __ bne_far(CCR0, *stub->entry(), MacroAssembler::bc_far_optimize_on_relocate);

    z_uncolor(masm, ref);
    __ bind(*stub->continuation());
  }
}

static void z_store_barrier(MacroAssembler* masm, const MachNode* node, Register ref_base, intptr_t disp, Register rnew_zaddress, Register rnew_zpointer, bool is_atomic) {
  Assembler::InlineSkippedInstructionsCounter skipped_counter(masm);
  if (node->barrier_data() == ZBarrierElided) {
    z_color(masm, rnew_zpointer, rnew_zaddress);
  } else {
    bool is_native = (node->barrier_data() & ZBarrierNative) != 0;
    ZStoreBarrierStubC2* const stub = ZStoreBarrierStubC2::create(node, Address(ref_base, disp), rnew_zaddress, rnew_zpointer, is_native, is_atomic);
    ZBarrierSetAssembler* bs_asm = ZBarrierSet::assembler();
    bs_asm->store_barrier_fast(masm, ref_base, disp, rnew_zaddress, rnew_zpointer, true /* in_nmethod */, is_atomic, *stub->entry(), *stub->continuation());
  }
}

static void z_compare_and_swap(MacroAssembler* masm, const MachNode* node,
                              Register res, Register mem, Register oldval, Register newval,
                              Register tmp1, Register tmp2, bool acquire) {

  Register rold_zpointer = tmp1, rnew_zpointer = tmp2;
  z_store_barrier(masm, node, mem, 0, newval, rnew_zpointer, true /* is_atomic */);
  z_color(masm, rold_zpointer, oldval);
  __ cmpxchgd(CCR0, R0, rold_zpointer, rnew_zpointer, mem,
              MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), res, nullptr, true,
              false /* we could support weak, but benefit is questionable */);

  if (acquire) {
    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
      // Uses the isync instruction as an acquire barrier.
      // This exploits the compare and the branch in the z load barrier (load, compare and branch, isync).
      __ isync();
    } else {
      __ sync();
    }
  }
}

static void z_compare_and_exchange(MacroAssembler* masm, const MachNode* node,
                                   Register res, Register mem, Register oldval, Register newval,
                                   Register tmp, bool acquire) {

  Register rold_zpointer = R0, rnew_zpointer = tmp;
  z_store_barrier(masm, node, mem, 0, newval, rnew_zpointer, true /* is_atomic */);
  z_color(masm, rold_zpointer, oldval);
  __ cmpxchgd(CCR0, res, rold_zpointer, rnew_zpointer, mem,
              MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), noreg, nullptr, true,
              false /* we could support weak, but benefit is questionable */);
  z_uncolor(masm, res);

  if (acquire) {
    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
      // Uses the isync instruction as an acquire barrier.
      // This exploits the compare and the branch in the z load barrier (load, compare and branch, isync).
      __ isync();
    } else {
      __ sync();
    }
  }
}

%}

instruct zLoadP(iRegPdst dst, memoryAlg4 mem, flagsRegCR0 cr0)
%{
  match(Set dst (LoadP mem));
  effect(TEMP_DEF dst, KILL cr0);
  ins_cost(MEMORY_REF_COST);

  predicate((UseZGC && ZGenerational && n->as_Load()->barrier_data() != 0)
            && (n->as_Load()->is_unordered() || followed_by_acquire(n)));

  format %{ "LD $dst, $mem" %}
  ins_encode %{
    assert($mem$$index == 0, "sanity");
    __ ld($dst$$Register, $mem$$disp, $mem$$base$$Register);
    z_load_barrier(masm, this, Address($mem$$base$$Register, $mem$$disp), $dst$$Register);
  %}
  ins_pipe(pipe_class_default);
%}

// Load Pointer Volatile
instruct zLoadP_acq(iRegPdst dst, memoryAlg4 mem, flagsRegCR0 cr0)
%{
  match(Set dst (LoadP mem));
  effect(TEMP_DEF dst, KILL cr0);
  ins_cost(3 * MEMORY_REF_COST);

  // Predicate on instruction order is implicitly present due to the predicate of the cheaper zLoadP operation
  predicate(UseZGC && ZGenerational && n->as_Load()->barrier_data() != 0);

  format %{ "LD acq $dst, $mem" %}
  ins_encode %{
    __ ld($dst$$Register, $mem$$disp, $mem$$base$$Register);
    z_load_barrier(masm, this, Address($mem$$base$$Register, $mem$$disp), $dst$$Register);

    // Uses the isync instruction as an acquire barrier.
    // This exploits the compare and the branch in the z load barrier (load, compare and branch, isync).
    if (barrier_data() == ZBarrierElided) __ twi_0($dst$$Register);
    __ isync();
  %}
  ins_pipe(pipe_class_default);
%}

// Store Pointer
instruct zStoreP(memoryAlg4 mem, iRegPsrc src, iRegPdst tmp, flagsRegCR0 cr0)
%{
  predicate(UseZGC && ZGenerational && n->as_Store()->barrier_data() != 0);
  match(Set mem (StoreP mem src));
  effect(TEMP tmp, KILL cr0);
  ins_cost(2 * MEMORY_REF_COST);
  format %{ "std    $mem, $src\t# ptr" %}
  ins_encode %{
    z_store_barrier(masm, this, $mem$$base$$Register, $mem$$disp, $src$$Register, $tmp$$Register, false /* is_atomic */);
    __ std($tmp$$Register, $mem$$disp, $mem$$base$$Register);
  %}
  ins_pipe(pipe_class_default);
%}

instruct zStorePNull(memoryAlg4 mem, immP_0 zero, iRegPdst tmp, flagsRegCR0 cr0)
%{
  predicate(UseZGC && ZGenerational && n->as_Store()->barrier_data() != 0);
  match(Set mem (StoreP mem zero));
  effect(TEMP tmp, KILL cr0);
  ins_cost(MEMORY_REF_COST);
  format %{ "std    $mem, null\t# ptr" %}
  ins_encode %{
    z_store_barrier(masm, this, $mem$$base$$Register, $mem$$disp, noreg, $tmp$$Register, false /* is_atomic */);
    __ std($tmp$$Register, $mem$$disp, $mem$$base$$Register);
  %}
  ins_pipe(pipe_class_default);
%}

instruct zCompareAndSwapP(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,
                          iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0) %{
  match(Set res (CompareAndSwapP mem (Binary oldval newval)));
  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));
  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr0);

  predicate((UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0)
            && (((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*) n)->order() != MemNode::seqcst));

  format %{ "CMPXCHG $res, $mem, $oldval, $newval; as bool; ptr" %}
  ins_encode %{
    z_compare_and_swap(masm, this,
                       $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register,
                       $tmp1$$Register, $tmp2$$Register,
                       false /* acquire */);
  %}
  ins_pipe(pipe_class_default);
%}

instruct zCompareAndSwapP_acq(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,
                              iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0) %{
  match(Set res (CompareAndSwapP mem (Binary oldval newval)));
  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));
  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr0);

  predicate((UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0)
            && (((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*) n)->order() == MemNode::seqcst));

  format %{ "CMPXCHG acq $res, $mem, $oldval, $newval; as bool; ptr" %}
  ins_encode %{
    z_compare_and_swap(masm, this,
                       $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register,
                       $tmp1$$Register, $tmp2$$Register,
                       true /* acquire */);
  %}
  ins_pipe(pipe_class_default);
%}

instruct zCompareAndExchangeP(iRegPdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,
                              iRegPdst tmp, flagsRegCR0 cr0) %{
  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));
  effect(TEMP_DEF res, TEMP tmp, KILL cr0);

  predicate((UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0)
            && (
              ((CompareAndSwapNode*)n)->order() != MemNode::acquire
              && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst
            ));

  format %{ "CMPXCHG $res, $mem, $oldval, $newval; as ptr; ptr" %}
  ins_encode %{
    z_compare_and_exchange(masm, this,
                           $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register, $tmp$$Register,
                           false /* acquire */);
  %}
  ins_pipe(pipe_class_default);
%}

instruct zCompareAndExchangeP_acq(iRegPdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,
                                  iRegPdst tmp, flagsRegCR0 cr0) %{
  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));
  effect(TEMP_DEF res, TEMP tmp, KILL cr0);

  predicate((UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0)
            && (
              ((CompareAndSwapNode*)n)->order() == MemNode::acquire
              || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst
            ));

  format %{ "CMPXCHG acq $res, $mem, $oldval, $newval; as ptr; ptr" %}
  ins_encode %{
    z_compare_and_exchange(masm, this,
                           $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register, $tmp$$Register,
                           true /* acquire */);
  %}
  ins_pipe(pipe_class_default);
%}

instruct zGetAndSetP(iRegPdst res, iRegPdst mem, iRegPsrc newval, iRegPdst tmp, flagsRegCR0 cr0) %{
  match(Set res (GetAndSetP mem newval));
  effect(TEMP_DEF res, TEMP tmp, KILL cr0);

  predicate(UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0);

  format %{ "GetAndSetP $res, $mem, $newval" %}
  ins_encode %{
    Register rnew_zpointer = $tmp$$Register, result = $res$$Register;
    z_store_barrier(masm, this, $mem$$Register, 0, $newval$$Register, rnew_zpointer, true /* is_atomic */);
    __ getandsetd(result, rnew_zpointer, $mem$$Register, MacroAssembler::cmpxchgx_hint_atomic_update());
    z_uncolor(masm, result);

    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
      __ isync();
    } else {
      __ sync();
    }
  %}
  ins_pipe(pipe_class_default);
%}
