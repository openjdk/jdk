//
// Copyright (c) 2011, 2025, Oracle and/or its affiliates. All rights reserved.
// DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
//
// This code is free software; you can redistribute it and/or modify it
// under the terms of the GNU General Public License version 2 only, as
// published by the Free Software Foundation.
//
// This code is distributed in the hope that it will be useful, but WITHOUT
// ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
// FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
// version 2 for more details (a copy is included in the LICENSE file that
// accompanied this code).
//
// You should have received a copy of the GNU General Public License version
// 2 along with this work; if not, write to the Free Software Foundation,
// Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
//
// Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
// or visit www.oracle.com if you need additional information or have any
// questions.
//
//

// X86 AMD64 Architecture Description File

//----------REGISTER DEFINITION BLOCK------------------------------------------
// This information is used by the matcher and the register allocator to
// describe individual registers and classes of registers within the target
// architecture.

register %{
//----------Architecture Description Register Definitions----------------------
// General Registers
// "reg_def"  name ( register save type, C convention save type,
//                   ideal register type, encoding );
// Register Save Types:
//
// NS  = No-Save:       The register allocator assumes that these registers
//                      can be used without saving upon entry to the method, &
//                      that they do not need to be saved at call sites.
//
// SOC = Save-On-Call:  The register allocator assumes that these registers
//                      can be used without saving upon entry to the method,
//                      but that they must be saved at call sites.
//
// SOE = Save-On-Entry: The register allocator assumes that these registers
//                      must be saved before using them upon entry to the
//                      method, but they do not need to be saved at call
//                      sites.
//
// AS  = Always-Save:   The register allocator assumes that these registers
//                      must be saved before using them upon entry to the
//                      method, & that they must be saved at call sites.
//
// Ideal Register Type is used to determine how to save & restore a
// register.  Op_RegI will get spilled with LoadI/StoreI, Op_RegP will get
// spilled with LoadP/StoreP.  If the register supports both, use Op_RegI.
//
// The encoding number is the actual bit-pattern placed into the opcodes.

// General Registers
// R8-R15 must be encoded with REX.  (RSP, RBP, RSI, RDI need REX when
// used as byte registers)

// Previously set RBX, RSI, and RDI as save-on-entry for java code
// Turn off SOE in java-code due to frequent use of uncommon-traps.
// Now that allocator is better, turn on RSI and RDI as SOE registers.

reg_def RAX  (SOC, SOC, Op_RegI,  0, rax->as_VMReg());
reg_def RAX_H(SOC, SOC, Op_RegI,  0, rax->as_VMReg()->next());

reg_def RCX  (SOC, SOC, Op_RegI,  1, rcx->as_VMReg());
reg_def RCX_H(SOC, SOC, Op_RegI,  1, rcx->as_VMReg()->next());

reg_def RDX  (SOC, SOC, Op_RegI,  2, rdx->as_VMReg());
reg_def RDX_H(SOC, SOC, Op_RegI,  2, rdx->as_VMReg()->next());

reg_def RBX  (SOC, SOE, Op_RegI,  3, rbx->as_VMReg());
reg_def RBX_H(SOC, SOE, Op_RegI,  3, rbx->as_VMReg()->next());

reg_def RSP  (NS,  NS,  Op_RegI,  4, rsp->as_VMReg());
reg_def RSP_H(NS,  NS,  Op_RegI,  4, rsp->as_VMReg()->next());

// now that adapter frames are gone RBP is always saved and restored by the prolog/epilog code
reg_def RBP  (NS, SOE, Op_RegI,  5, rbp->as_VMReg());
reg_def RBP_H(NS, SOE, Op_RegI,  5, rbp->as_VMReg()->next());

#ifdef _WIN64

reg_def RSI  (SOC, SOE, Op_RegI,  6, rsi->as_VMReg());
reg_def RSI_H(SOC, SOE, Op_RegI,  6, rsi->as_VMReg()->next());

reg_def RDI  (SOC, SOE, Op_RegI,  7, rdi->as_VMReg());
reg_def RDI_H(SOC, SOE, Op_RegI,  7, rdi->as_VMReg()->next());

#else

reg_def RSI  (SOC, SOC, Op_RegI,  6, rsi->as_VMReg());
reg_def RSI_H(SOC, SOC, Op_RegI,  6, rsi->as_VMReg()->next());

reg_def RDI  (SOC, SOC, Op_RegI,  7, rdi->as_VMReg());
reg_def RDI_H(SOC, SOC, Op_RegI,  7, rdi->as_VMReg()->next());

#endif

reg_def R8   (SOC, SOC, Op_RegI,  8, r8->as_VMReg());
reg_def R8_H (SOC, SOC, Op_RegI,  8, r8->as_VMReg()->next());

reg_def R9   (SOC, SOC, Op_RegI,  9, r9->as_VMReg());
reg_def R9_H (SOC, SOC, Op_RegI,  9, r9->as_VMReg()->next());

reg_def R10  (SOC, SOC, Op_RegI, 10, r10->as_VMReg());
reg_def R10_H(SOC, SOC, Op_RegI, 10, r10->as_VMReg()->next());

reg_def R11  (SOC, SOC, Op_RegI, 11, r11->as_VMReg());
reg_def R11_H(SOC, SOC, Op_RegI, 11, r11->as_VMReg()->next());

reg_def R12  (SOC, SOE, Op_RegI, 12, r12->as_VMReg());
reg_def R12_H(SOC, SOE, Op_RegI, 12, r12->as_VMReg()->next());

reg_def R13  (SOC, SOE, Op_RegI, 13, r13->as_VMReg());
reg_def R13_H(SOC, SOE, Op_RegI, 13, r13->as_VMReg()->next());

reg_def R14  (SOC, SOE, Op_RegI, 14, r14->as_VMReg());
reg_def R14_H(SOC, SOE, Op_RegI, 14, r14->as_VMReg()->next());

reg_def R15  (SOC, SOE, Op_RegI, 15, r15->as_VMReg());
reg_def R15_H(SOC, SOE, Op_RegI, 15, r15->as_VMReg()->next());

reg_def R16  (SOC, SOC, Op_RegI, 16, r16->as_VMReg());
reg_def R16_H(SOC, SOC, Op_RegI, 16, r16->as_VMReg()->next());

reg_def R17  (SOC, SOC, Op_RegI, 17, r17->as_VMReg());
reg_def R17_H(SOC, SOC, Op_RegI, 17, r17->as_VMReg()->next());

reg_def R18  (SOC, SOC, Op_RegI, 18, r18->as_VMReg());
reg_def R18_H(SOC, SOC, Op_RegI, 18, r18->as_VMReg()->next());

reg_def R19  (SOC, SOC, Op_RegI, 19, r19->as_VMReg());
reg_def R19_H(SOC, SOC, Op_RegI, 19, r19->as_VMReg()->next());

reg_def R20  (SOC, SOC, Op_RegI, 20, r20->as_VMReg());
reg_def R20_H(SOC, SOC, Op_RegI, 20, r20->as_VMReg()->next());

reg_def R21  (SOC, SOC, Op_RegI, 21, r21->as_VMReg());
reg_def R21_H(SOC, SOC, Op_RegI, 21, r21->as_VMReg()->next());

reg_def R22  (SOC, SOC, Op_RegI, 22, r22->as_VMReg());
reg_def R22_H(SOC, SOC, Op_RegI, 22, r22->as_VMReg()->next());

reg_def R23  (SOC, SOC, Op_RegI, 23, r23->as_VMReg());
reg_def R23_H(SOC, SOC, Op_RegI, 23, r23->as_VMReg()->next());

reg_def R24  (SOC, SOC, Op_RegI, 24, r24->as_VMReg());
reg_def R24_H(SOC, SOC, Op_RegI, 24, r24->as_VMReg()->next());

reg_def R25  (SOC, SOC, Op_RegI, 25, r25->as_VMReg());
reg_def R25_H(SOC, SOC, Op_RegI, 25, r25->as_VMReg()->next());

reg_def R26  (SOC, SOC, Op_RegI, 26, r26->as_VMReg());
reg_def R26_H(SOC, SOC, Op_RegI, 26, r26->as_VMReg()->next());

reg_def R27  (SOC, SOC, Op_RegI, 27, r27->as_VMReg());
reg_def R27_H(SOC, SOC, Op_RegI, 27, r27->as_VMReg()->next());

reg_def R28  (SOC, SOC, Op_RegI, 28, r28->as_VMReg());
reg_def R28_H(SOC, SOC, Op_RegI, 28, r28->as_VMReg()->next());

reg_def R29  (SOC, SOC, Op_RegI, 29, r29->as_VMReg());
reg_def R29_H(SOC, SOC, Op_RegI, 29, r29->as_VMReg()->next());

reg_def R30  (SOC, SOC, Op_RegI, 30, r30->as_VMReg());
reg_def R30_H(SOC, SOC, Op_RegI, 30, r30->as_VMReg()->next());

reg_def R31  (SOC, SOC, Op_RegI, 31, r31->as_VMReg());
reg_def R31_H(SOC, SOC, Op_RegI, 31, r31->as_VMReg()->next());

// Floating Point Registers

// Specify priority of register selection within phases of register
// allocation.  Highest priority is first.  A useful heuristic is to
// give registers a low priority when they are required by machine
// instructions, like EAX and EDX on I486, and choose no-save registers
// before save-on-call, & save-on-call before save-on-entry.  Registers
// which participate in fixed calling sequences should come last.
// Registers which are used as pairs must fall on an even boundary.

alloc_class chunk0(R10,         R10_H,
                   R11,         R11_H,
                   R8,          R8_H,
                   R9,          R9_H,
                   R12,         R12_H,
                   RCX,         RCX_H,
                   RBX,         RBX_H,
                   RDI,         RDI_H,
                   RDX,         RDX_H,
                   RSI,         RSI_H,
                   RAX,         RAX_H,
                   RBP,         RBP_H,
                   R13,         R13_H,
                   R14,         R14_H,
                   R15,         R15_H,
                   R16,         R16_H,
                   R17,         R17_H,
                   R18,         R18_H,
                   R19,         R19_H,
                   R20,         R20_H,
                   R21,         R21_H,
                   R22,         R22_H,
                   R23,         R23_H,
                   R24,         R24_H,
                   R25,         R25_H,
                   R26,         R26_H,
                   R27,         R27_H,
                   R28,         R28_H,
                   R29,         R29_H,
                   R30,         R30_H,
                   R31,         R31_H,
                   RSP,         RSP_H);

// XMM registers.  512-bit registers or 8 words each, labeled (a)-p.
// Word a in each register holds a Float, words ab hold a Double.
// The whole registers are used in SSE4.2 version intrinsics,
// array copy stubs and superword operations (see UseSSE42Intrinsics,
// UseXMMForArrayCopy and UseSuperword flags).
// For pre EVEX enabled architectures:
//      XMM8-XMM15 must be encoded with REX (VEX for UseAVX)
// For EVEX enabled architectures:
//      XMM8-XMM31 must be encoded with REX (EVEX for UseAVX).
//
// Linux ABI:   No register preserved across function calls
//              XMM0-XMM7 might hold parameters
// Windows ABI: XMM6-XMM15 preserved across function calls
//              XMM0-XMM3 might hold parameters

reg_def XMM0 ( SOC, SOC, Op_RegF, 0, xmm0->as_VMReg());
reg_def XMM0b( SOC, SOC, Op_RegF, 0, xmm0->as_VMReg()->next(1));
reg_def XMM0c( SOC, SOC, Op_RegF, 0, xmm0->as_VMReg()->next(2));
reg_def XMM0d( SOC, SOC, Op_RegF, 0, xmm0->as_VMReg()->next(3));
reg_def XMM0e( SOC, SOC, Op_RegF, 0, xmm0->as_VMReg()->next(4));
reg_def XMM0f( SOC, SOC, Op_RegF, 0, xmm0->as_VMReg()->next(5));
reg_def XMM0g( SOC, SOC, Op_RegF, 0, xmm0->as_VMReg()->next(6));
reg_def XMM0h( SOC, SOC, Op_RegF, 0, xmm0->as_VMReg()->next(7));
reg_def XMM0i( SOC, SOC, Op_RegF, 0, xmm0->as_VMReg()->next(8));
reg_def XMM0j( SOC, SOC, Op_RegF, 0, xmm0->as_VMReg()->next(9));
reg_def XMM0k( SOC, SOC, Op_RegF, 0, xmm0->as_VMReg()->next(10));
reg_def XMM0l( SOC, SOC, Op_RegF, 0, xmm0->as_VMReg()->next(11));
reg_def XMM0m( SOC, SOC, Op_RegF, 0, xmm0->as_VMReg()->next(12));
reg_def XMM0n( SOC, SOC, Op_RegF, 0, xmm0->as_VMReg()->next(13));
reg_def XMM0o( SOC, SOC, Op_RegF, 0, xmm0->as_VMReg()->next(14));
reg_def XMM0p( SOC, SOC, Op_RegF, 0, xmm0->as_VMReg()->next(15));

reg_def XMM1 ( SOC, SOC, Op_RegF, 1, xmm1->as_VMReg());
reg_def XMM1b( SOC, SOC, Op_RegF, 1, xmm1->as_VMReg()->next(1));
reg_def XMM1c( SOC, SOC, Op_RegF, 1, xmm1->as_VMReg()->next(2));
reg_def XMM1d( SOC, SOC, Op_RegF, 1, xmm1->as_VMReg()->next(3));
reg_def XMM1e( SOC, SOC, Op_RegF, 1, xmm1->as_VMReg()->next(4));
reg_def XMM1f( SOC, SOC, Op_RegF, 1, xmm1->as_VMReg()->next(5));
reg_def XMM1g( SOC, SOC, Op_RegF, 1, xmm1->as_VMReg()->next(6));
reg_def XMM1h( SOC, SOC, Op_RegF, 1, xmm1->as_VMReg()->next(7));
reg_def XMM1i( SOC, SOC, Op_RegF, 1, xmm1->as_VMReg()->next(8));
reg_def XMM1j( SOC, SOC, Op_RegF, 1, xmm1->as_VMReg()->next(9));
reg_def XMM1k( SOC, SOC, Op_RegF, 1, xmm1->as_VMReg()->next(10));
reg_def XMM1l( SOC, SOC, Op_RegF, 1, xmm1->as_VMReg()->next(11));
reg_def XMM1m( SOC, SOC, Op_RegF, 1, xmm1->as_VMReg()->next(12));
reg_def XMM1n( SOC, SOC, Op_RegF, 1, xmm1->as_VMReg()->next(13));
reg_def XMM1o( SOC, SOC, Op_RegF, 1, xmm1->as_VMReg()->next(14));
reg_def XMM1p( SOC, SOC, Op_RegF, 1, xmm1->as_VMReg()->next(15));

reg_def XMM2 ( SOC, SOC, Op_RegF, 2, xmm2->as_VMReg());
reg_def XMM2b( SOC, SOC, Op_RegF, 2, xmm2->as_VMReg()->next(1));
reg_def XMM2c( SOC, SOC, Op_RegF, 2, xmm2->as_VMReg()->next(2));
reg_def XMM2d( SOC, SOC, Op_RegF, 2, xmm2->as_VMReg()->next(3));
reg_def XMM2e( SOC, SOC, Op_RegF, 2, xmm2->as_VMReg()->next(4));
reg_def XMM2f( SOC, SOC, Op_RegF, 2, xmm2->as_VMReg()->next(5));
reg_def XMM2g( SOC, SOC, Op_RegF, 2, xmm2->as_VMReg()->next(6));
reg_def XMM2h( SOC, SOC, Op_RegF, 2, xmm2->as_VMReg()->next(7));
reg_def XMM2i( SOC, SOC, Op_RegF, 2, xmm2->as_VMReg()->next(8));
reg_def XMM2j( SOC, SOC, Op_RegF, 2, xmm2->as_VMReg()->next(9));
reg_def XMM2k( SOC, SOC, Op_RegF, 2, xmm2->as_VMReg()->next(10));
reg_def XMM2l( SOC, SOC, Op_RegF, 2, xmm2->as_VMReg()->next(11));
reg_def XMM2m( SOC, SOC, Op_RegF, 2, xmm2->as_VMReg()->next(12));
reg_def XMM2n( SOC, SOC, Op_RegF, 2, xmm2->as_VMReg()->next(13));
reg_def XMM2o( SOC, SOC, Op_RegF, 2, xmm2->as_VMReg()->next(14));
reg_def XMM2p( SOC, SOC, Op_RegF, 2, xmm2->as_VMReg()->next(15));

reg_def XMM3 ( SOC, SOC, Op_RegF, 3, xmm3->as_VMReg());
reg_def XMM3b( SOC, SOC, Op_RegF, 3, xmm3->as_VMReg()->next(1));
reg_def XMM3c( SOC, SOC, Op_RegF, 3, xmm3->as_VMReg()->next(2));
reg_def XMM3d( SOC, SOC, Op_RegF, 3, xmm3->as_VMReg()->next(3));
reg_def XMM3e( SOC, SOC, Op_RegF, 3, xmm3->as_VMReg()->next(4));
reg_def XMM3f( SOC, SOC, Op_RegF, 3, xmm3->as_VMReg()->next(5));
reg_def XMM3g( SOC, SOC, Op_RegF, 3, xmm3->as_VMReg()->next(6));
reg_def XMM3h( SOC, SOC, Op_RegF, 3, xmm3->as_VMReg()->next(7));
reg_def XMM3i( SOC, SOC, Op_RegF, 3, xmm3->as_VMReg()->next(8));
reg_def XMM3j( SOC, SOC, Op_RegF, 3, xmm3->as_VMReg()->next(9));
reg_def XMM3k( SOC, SOC, Op_RegF, 3, xmm3->as_VMReg()->next(10));
reg_def XMM3l( SOC, SOC, Op_RegF, 3, xmm3->as_VMReg()->next(11));
reg_def XMM3m( SOC, SOC, Op_RegF, 3, xmm3->as_VMReg()->next(12));
reg_def XMM3n( SOC, SOC, Op_RegF, 3, xmm3->as_VMReg()->next(13));
reg_def XMM3o( SOC, SOC, Op_RegF, 3, xmm3->as_VMReg()->next(14));
reg_def XMM3p( SOC, SOC, Op_RegF, 3, xmm3->as_VMReg()->next(15));

reg_def XMM4 ( SOC, SOC, Op_RegF, 4, xmm4->as_VMReg());
reg_def XMM4b( SOC, SOC, Op_RegF, 4, xmm4->as_VMReg()->next(1));
reg_def XMM4c( SOC, SOC, Op_RegF, 4, xmm4->as_VMReg()->next(2));
reg_def XMM4d( SOC, SOC, Op_RegF, 4, xmm4->as_VMReg()->next(3));
reg_def XMM4e( SOC, SOC, Op_RegF, 4, xmm4->as_VMReg()->next(4));
reg_def XMM4f( SOC, SOC, Op_RegF, 4, xmm4->as_VMReg()->next(5));
reg_def XMM4g( SOC, SOC, Op_RegF, 4, xmm4->as_VMReg()->next(6));
reg_def XMM4h( SOC, SOC, Op_RegF, 4, xmm4->as_VMReg()->next(7));
reg_def XMM4i( SOC, SOC, Op_RegF, 4, xmm4->as_VMReg()->next(8));
reg_def XMM4j( SOC, SOC, Op_RegF, 4, xmm4->as_VMReg()->next(9));
reg_def XMM4k( SOC, SOC, Op_RegF, 4, xmm4->as_VMReg()->next(10));
reg_def XMM4l( SOC, SOC, Op_RegF, 4, xmm4->as_VMReg()->next(11));
reg_def XMM4m( SOC, SOC, Op_RegF, 4, xmm4->as_VMReg()->next(12));
reg_def XMM4n( SOC, SOC, Op_RegF, 4, xmm4->as_VMReg()->next(13));
reg_def XMM4o( SOC, SOC, Op_RegF, 4, xmm4->as_VMReg()->next(14));
reg_def XMM4p( SOC, SOC, Op_RegF, 4, xmm4->as_VMReg()->next(15));

reg_def XMM5 ( SOC, SOC, Op_RegF, 5, xmm5->as_VMReg());
reg_def XMM5b( SOC, SOC, Op_RegF, 5, xmm5->as_VMReg()->next(1));
reg_def XMM5c( SOC, SOC, Op_RegF, 5, xmm5->as_VMReg()->next(2));
reg_def XMM5d( SOC, SOC, Op_RegF, 5, xmm5->as_VMReg()->next(3));
reg_def XMM5e( SOC, SOC, Op_RegF, 5, xmm5->as_VMReg()->next(4));
reg_def XMM5f( SOC, SOC, Op_RegF, 5, xmm5->as_VMReg()->next(5));
reg_def XMM5g( SOC, SOC, Op_RegF, 5, xmm5->as_VMReg()->next(6));
reg_def XMM5h( SOC, SOC, Op_RegF, 5, xmm5->as_VMReg()->next(7));
reg_def XMM5i( SOC, SOC, Op_RegF, 5, xmm5->as_VMReg()->next(8));
reg_def XMM5j( SOC, SOC, Op_RegF, 5, xmm5->as_VMReg()->next(9));
reg_def XMM5k( SOC, SOC, Op_RegF, 5, xmm5->as_VMReg()->next(10));
reg_def XMM5l( SOC, SOC, Op_RegF, 5, xmm5->as_VMReg()->next(11));
reg_def XMM5m( SOC, SOC, Op_RegF, 5, xmm5->as_VMReg()->next(12));
reg_def XMM5n( SOC, SOC, Op_RegF, 5, xmm5->as_VMReg()->next(13));
reg_def XMM5o( SOC, SOC, Op_RegF, 5, xmm5->as_VMReg()->next(14));
reg_def XMM5p( SOC, SOC, Op_RegF, 5, xmm5->as_VMReg()->next(15));

reg_def XMM6 ( SOC, SOC, Op_RegF, 6, xmm6->as_VMReg());
reg_def XMM6b( SOC, SOC, Op_RegF, 6, xmm6->as_VMReg()->next(1));
reg_def XMM6c( SOC, SOC, Op_RegF, 6, xmm6->as_VMReg()->next(2));
reg_def XMM6d( SOC, SOC, Op_RegF, 6, xmm6->as_VMReg()->next(3));
reg_def XMM6e( SOC, SOC, Op_RegF, 6, xmm6->as_VMReg()->next(4));
reg_def XMM6f( SOC, SOC, Op_RegF, 6, xmm6->as_VMReg()->next(5));
reg_def XMM6g( SOC, SOC, Op_RegF, 6, xmm6->as_VMReg()->next(6));
reg_def XMM6h( SOC, SOC, Op_RegF, 6, xmm6->as_VMReg()->next(7));
reg_def XMM6i( SOC, SOC, Op_RegF, 6, xmm6->as_VMReg()->next(8));
reg_def XMM6j( SOC, SOC, Op_RegF, 6, xmm6->as_VMReg()->next(9));
reg_def XMM6k( SOC, SOC, Op_RegF, 6, xmm6->as_VMReg()->next(10));
reg_def XMM6l( SOC, SOC, Op_RegF, 6, xmm6->as_VMReg()->next(11));
reg_def XMM6m( SOC, SOC, Op_RegF, 6, xmm6->as_VMReg()->next(12));
reg_def XMM6n( SOC, SOC, Op_RegF, 6, xmm6->as_VMReg()->next(13));
reg_def XMM6o( SOC, SOC, Op_RegF, 6, xmm6->as_VMReg()->next(14));
reg_def XMM6p( SOC, SOC, Op_RegF, 6, xmm6->as_VMReg()->next(15));

reg_def XMM7 ( SOC, SOC, Op_RegF, 7, xmm7->as_VMReg());
reg_def XMM7b( SOC, SOC, Op_RegF, 7, xmm7->as_VMReg()->next(1));
reg_def XMM7c( SOC, SOC, Op_RegF, 7, xmm7->as_VMReg()->next(2));
reg_def XMM7d( SOC, SOC, Op_RegF, 7, xmm7->as_VMReg()->next(3));
reg_def XMM7e( SOC, SOC, Op_RegF, 7, xmm7->as_VMReg()->next(4));
reg_def XMM7f( SOC, SOC, Op_RegF, 7, xmm7->as_VMReg()->next(5));
reg_def XMM7g( SOC, SOC, Op_RegF, 7, xmm7->as_VMReg()->next(6));
reg_def XMM7h( SOC, SOC, Op_RegF, 7, xmm7->as_VMReg()->next(7));
reg_def XMM7i( SOC, SOC, Op_RegF, 7, xmm7->as_VMReg()->next(8));
reg_def XMM7j( SOC, SOC, Op_RegF, 7, xmm7->as_VMReg()->next(9));
reg_def XMM7k( SOC, SOC, Op_RegF, 7, xmm7->as_VMReg()->next(10));
reg_def XMM7l( SOC, SOC, Op_RegF, 7, xmm7->as_VMReg()->next(11));
reg_def XMM7m( SOC, SOC, Op_RegF, 7, xmm7->as_VMReg()->next(12));
reg_def XMM7n( SOC, SOC, Op_RegF, 7, xmm7->as_VMReg()->next(13));
reg_def XMM7o( SOC, SOC, Op_RegF, 7, xmm7->as_VMReg()->next(14));
reg_def XMM7p( SOC, SOC, Op_RegF, 7, xmm7->as_VMReg()->next(15));

reg_def XMM8 ( SOC, SOC, Op_RegF, 8, xmm8->as_VMReg());
reg_def XMM8b( SOC, SOC, Op_RegF, 8, xmm8->as_VMReg()->next(1));
reg_def XMM8c( SOC, SOC, Op_RegF, 8, xmm8->as_VMReg()->next(2));
reg_def XMM8d( SOC, SOC, Op_RegF, 8, xmm8->as_VMReg()->next(3));
reg_def XMM8e( SOC, SOC, Op_RegF, 8, xmm8->as_VMReg()->next(4));
reg_def XMM8f( SOC, SOC, Op_RegF, 8, xmm8->as_VMReg()->next(5));
reg_def XMM8g( SOC, SOC, Op_RegF, 8, xmm8->as_VMReg()->next(6));
reg_def XMM8h( SOC, SOC, Op_RegF, 8, xmm8->as_VMReg()->next(7));
reg_def XMM8i( SOC, SOC, Op_RegF, 8, xmm8->as_VMReg()->next(8));
reg_def XMM8j( SOC, SOC, Op_RegF, 8, xmm8->as_VMReg()->next(9));
reg_def XMM8k( SOC, SOC, Op_RegF, 8, xmm8->as_VMReg()->next(10));
reg_def XMM8l( SOC, SOC, Op_RegF, 8, xmm8->as_VMReg()->next(11));
reg_def XMM8m( SOC, SOC, Op_RegF, 8, xmm8->as_VMReg()->next(12));
reg_def XMM8n( SOC, SOC, Op_RegF, 8, xmm8->as_VMReg()->next(13));
reg_def XMM8o( SOC, SOC, Op_RegF, 8, xmm8->as_VMReg()->next(14));
reg_def XMM8p( SOC, SOC, Op_RegF, 8, xmm8->as_VMReg()->next(15));

reg_def XMM9 ( SOC, SOC, Op_RegF, 9, xmm9->as_VMReg());
reg_def XMM9b( SOC, SOC, Op_RegF, 9, xmm9->as_VMReg()->next(1));
reg_def XMM9c( SOC, SOC, Op_RegF, 9, xmm9->as_VMReg()->next(2));
reg_def XMM9d( SOC, SOC, Op_RegF, 9, xmm9->as_VMReg()->next(3));
reg_def XMM9e( SOC, SOC, Op_RegF, 9, xmm9->as_VMReg()->next(4));
reg_def XMM9f( SOC, SOC, Op_RegF, 9, xmm9->as_VMReg()->next(5));
reg_def XMM9g( SOC, SOC, Op_RegF, 9, xmm9->as_VMReg()->next(6));
reg_def XMM9h( SOC, SOC, Op_RegF, 9, xmm9->as_VMReg()->next(7));
reg_def XMM9i( SOC, SOC, Op_RegF, 9, xmm9->as_VMReg()->next(8));
reg_def XMM9j( SOC, SOC, Op_RegF, 9, xmm9->as_VMReg()->next(9));
reg_def XMM9k( SOC, SOC, Op_RegF, 9, xmm9->as_VMReg()->next(10));
reg_def XMM9l( SOC, SOC, Op_RegF, 9, xmm9->as_VMReg()->next(11));
reg_def XMM9m( SOC, SOC, Op_RegF, 9, xmm9->as_VMReg()->next(12));
reg_def XMM9n( SOC, SOC, Op_RegF, 9, xmm9->as_VMReg()->next(13));
reg_def XMM9o( SOC, SOC, Op_RegF, 9, xmm9->as_VMReg()->next(14));
reg_def XMM9p( SOC, SOC, Op_RegF, 9, xmm9->as_VMReg()->next(15));

reg_def XMM10 ( SOC, SOC, Op_RegF, 10, xmm10->as_VMReg());
reg_def XMM10b( SOC, SOC, Op_RegF, 10, xmm10->as_VMReg()->next(1));
reg_def XMM10c( SOC, SOC, Op_RegF, 10, xmm10->as_VMReg()->next(2));
reg_def XMM10d( SOC, SOC, Op_RegF, 10, xmm10->as_VMReg()->next(3));
reg_def XMM10e( SOC, SOC, Op_RegF, 10, xmm10->as_VMReg()->next(4));
reg_def XMM10f( SOC, SOC, Op_RegF, 10, xmm10->as_VMReg()->next(5));
reg_def XMM10g( SOC, SOC, Op_RegF, 10, xmm10->as_VMReg()->next(6));
reg_def XMM10h( SOC, SOC, Op_RegF, 10, xmm10->as_VMReg()->next(7));
reg_def XMM10i( SOC, SOC, Op_RegF, 10, xmm10->as_VMReg()->next(8));
reg_def XMM10j( SOC, SOC, Op_RegF, 10, xmm10->as_VMReg()->next(9));
reg_def XMM10k( SOC, SOC, Op_RegF, 10, xmm10->as_VMReg()->next(10));
reg_def XMM10l( SOC, SOC, Op_RegF, 10, xmm10->as_VMReg()->next(11));
reg_def XMM10m( SOC, SOC, Op_RegF, 10, xmm10->as_VMReg()->next(12));
reg_def XMM10n( SOC, SOC, Op_RegF, 10, xmm10->as_VMReg()->next(13));
reg_def XMM10o( SOC, SOC, Op_RegF, 10, xmm10->as_VMReg()->next(14));
reg_def XMM10p( SOC, SOC, Op_RegF, 10, xmm10->as_VMReg()->next(15));

reg_def XMM11 ( SOC, SOC, Op_RegF, 11, xmm11->as_VMReg());
reg_def XMM11b( SOC, SOC, Op_RegF, 11, xmm11->as_VMReg()->next(1));
reg_def XMM11c( SOC, SOC, Op_RegF, 11, xmm11->as_VMReg()->next(2));
reg_def XMM11d( SOC, SOC, Op_RegF, 11, xmm11->as_VMReg()->next(3));
reg_def XMM11e( SOC, SOC, Op_RegF, 11, xmm11->as_VMReg()->next(4));
reg_def XMM11f( SOC, SOC, Op_RegF, 11, xmm11->as_VMReg()->next(5));
reg_def XMM11g( SOC, SOC, Op_RegF, 11, xmm11->as_VMReg()->next(6));
reg_def XMM11h( SOC, SOC, Op_RegF, 11, xmm11->as_VMReg()->next(7));
reg_def XMM11i( SOC, SOC, Op_RegF, 11, xmm11->as_VMReg()->next(8));
reg_def XMM11j( SOC, SOC, Op_RegF, 11, xmm11->as_VMReg()->next(9));
reg_def XMM11k( SOC, SOC, Op_RegF, 11, xmm11->as_VMReg()->next(10));
reg_def XMM11l( SOC, SOC, Op_RegF, 11, xmm11->as_VMReg()->next(11));
reg_def XMM11m( SOC, SOC, Op_RegF, 11, xmm11->as_VMReg()->next(12));
reg_def XMM11n( SOC, SOC, Op_RegF, 11, xmm11->as_VMReg()->next(13));
reg_def XMM11o( SOC, SOC, Op_RegF, 11, xmm11->as_VMReg()->next(14));
reg_def XMM11p( SOC, SOC, Op_RegF, 11, xmm11->as_VMReg()->next(15));

reg_def XMM12 ( SOC, SOC, Op_RegF, 12, xmm12->as_VMReg());
reg_def XMM12b( SOC, SOC, Op_RegF, 12, xmm12->as_VMReg()->next(1));
reg_def XMM12c( SOC, SOC, Op_RegF, 12, xmm12->as_VMReg()->next(2));
reg_def XMM12d( SOC, SOC, Op_RegF, 12, xmm12->as_VMReg()->next(3));
reg_def XMM12e( SOC, SOC, Op_RegF, 12, xmm12->as_VMReg()->next(4));
reg_def XMM12f( SOC, SOC, Op_RegF, 12, xmm12->as_VMReg()->next(5));
reg_def XMM12g( SOC, SOC, Op_RegF, 12, xmm12->as_VMReg()->next(6));
reg_def XMM12h( SOC, SOC, Op_RegF, 12, xmm12->as_VMReg()->next(7));
reg_def XMM12i( SOC, SOC, Op_RegF, 12, xmm12->as_VMReg()->next(8));
reg_def XMM12j( SOC, SOC, Op_RegF, 12, xmm12->as_VMReg()->next(9));
reg_def XMM12k( SOC, SOC, Op_RegF, 12, xmm12->as_VMReg()->next(10));
reg_def XMM12l( SOC, SOC, Op_RegF, 12, xmm12->as_VMReg()->next(11));
reg_def XMM12m( SOC, SOC, Op_RegF, 12, xmm12->as_VMReg()->next(12));
reg_def XMM12n( SOC, SOC, Op_RegF, 12, xmm12->as_VMReg()->next(13));
reg_def XMM12o( SOC, SOC, Op_RegF, 12, xmm12->as_VMReg()->next(14));
reg_def XMM12p( SOC, SOC, Op_RegF, 12, xmm12->as_VMReg()->next(15));

reg_def XMM13 ( SOC, SOC, Op_RegF, 13, xmm13->as_VMReg());
reg_def XMM13b( SOC, SOC, Op_RegF, 13, xmm13->as_VMReg()->next(1));
reg_def XMM13c( SOC, SOC, Op_RegF, 13, xmm13->as_VMReg()->next(2));
reg_def XMM13d( SOC, SOC, Op_RegF, 13, xmm13->as_VMReg()->next(3));
reg_def XMM13e( SOC, SOC, Op_RegF, 13, xmm13->as_VMReg()->next(4));
reg_def XMM13f( SOC, SOC, Op_RegF, 13, xmm13->as_VMReg()->next(5));
reg_def XMM13g( SOC, SOC, Op_RegF, 13, xmm13->as_VMReg()->next(6));
reg_def XMM13h( SOC, SOC, Op_RegF, 13, xmm13->as_VMReg()->next(7));
reg_def XMM13i( SOC, SOC, Op_RegF, 13, xmm13->as_VMReg()->next(8));
reg_def XMM13j( SOC, SOC, Op_RegF, 13, xmm13->as_VMReg()->next(9));
reg_def XMM13k( SOC, SOC, Op_RegF, 13, xmm13->as_VMReg()->next(10));
reg_def XMM13l( SOC, SOC, Op_RegF, 13, xmm13->as_VMReg()->next(11));
reg_def XMM13m( SOC, SOC, Op_RegF, 13, xmm13->as_VMReg()->next(12));
reg_def XMM13n( SOC, SOC, Op_RegF, 13, xmm13->as_VMReg()->next(13));
reg_def XMM13o( SOC, SOC, Op_RegF, 13, xmm13->as_VMReg()->next(14));
reg_def XMM13p( SOC, SOC, Op_RegF, 13, xmm13->as_VMReg()->next(15));

reg_def XMM14 ( SOC, SOC, Op_RegF, 14, xmm14->as_VMReg());
reg_def XMM14b( SOC, SOC, Op_RegF, 14, xmm14->as_VMReg()->next(1));
reg_def XMM14c( SOC, SOC, Op_RegF, 14, xmm14->as_VMReg()->next(2));
reg_def XMM14d( SOC, SOC, Op_RegF, 14, xmm14->as_VMReg()->next(3));
reg_def XMM14e( SOC, SOC, Op_RegF, 14, xmm14->as_VMReg()->next(4));
reg_def XMM14f( SOC, SOC, Op_RegF, 14, xmm14->as_VMReg()->next(5));
reg_def XMM14g( SOC, SOC, Op_RegF, 14, xmm14->as_VMReg()->next(6));
reg_def XMM14h( SOC, SOC, Op_RegF, 14, xmm14->as_VMReg()->next(7));
reg_def XMM14i( SOC, SOC, Op_RegF, 14, xmm14->as_VMReg()->next(8));
reg_def XMM14j( SOC, SOC, Op_RegF, 14, xmm14->as_VMReg()->next(9));
reg_def XMM14k( SOC, SOC, Op_RegF, 14, xmm14->as_VMReg()->next(10));
reg_def XMM14l( SOC, SOC, Op_RegF, 14, xmm14->as_VMReg()->next(11));
reg_def XMM14m( SOC, SOC, Op_RegF, 14, xmm14->as_VMReg()->next(12));
reg_def XMM14n( SOC, SOC, Op_RegF, 14, xmm14->as_VMReg()->next(13));
reg_def XMM14o( SOC, SOC, Op_RegF, 14, xmm14->as_VMReg()->next(14));
reg_def XMM14p( SOC, SOC, Op_RegF, 14, xmm14->as_VMReg()->next(15));

reg_def XMM15 ( SOC, SOC, Op_RegF, 15, xmm15->as_VMReg());
reg_def XMM15b( SOC, SOC, Op_RegF, 15, xmm15->as_VMReg()->next(1));
reg_def XMM15c( SOC, SOC, Op_RegF, 15, xmm15->as_VMReg()->next(2));
reg_def XMM15d( SOC, SOC, Op_RegF, 15, xmm15->as_VMReg()->next(3));
reg_def XMM15e( SOC, SOC, Op_RegF, 15, xmm15->as_VMReg()->next(4));
reg_def XMM15f( SOC, SOC, Op_RegF, 15, xmm15->as_VMReg()->next(5));
reg_def XMM15g( SOC, SOC, Op_RegF, 15, xmm15->as_VMReg()->next(6));
reg_def XMM15h( SOC, SOC, Op_RegF, 15, xmm15->as_VMReg()->next(7));
reg_def XMM15i( SOC, SOC, Op_RegF, 15, xmm15->as_VMReg()->next(8));
reg_def XMM15j( SOC, SOC, Op_RegF, 15, xmm15->as_VMReg()->next(9));
reg_def XMM15k( SOC, SOC, Op_RegF, 15, xmm15->as_VMReg()->next(10));
reg_def XMM15l( SOC, SOC, Op_RegF, 15, xmm15->as_VMReg()->next(11));
reg_def XMM15m( SOC, SOC, Op_RegF, 15, xmm15->as_VMReg()->next(12));
reg_def XMM15n( SOC, SOC, Op_RegF, 15, xmm15->as_VMReg()->next(13));
reg_def XMM15o( SOC, SOC, Op_RegF, 15, xmm15->as_VMReg()->next(14));
reg_def XMM15p( SOC, SOC, Op_RegF, 15, xmm15->as_VMReg()->next(15));

reg_def XMM16 ( SOC, SOC, Op_RegF, 16, xmm16->as_VMReg());
reg_def XMM16b( SOC, SOC, Op_RegF, 16, xmm16->as_VMReg()->next(1));
reg_def XMM16c( SOC, SOC, Op_RegF, 16, xmm16->as_VMReg()->next(2));
reg_def XMM16d( SOC, SOC, Op_RegF, 16, xmm16->as_VMReg()->next(3));
reg_def XMM16e( SOC, SOC, Op_RegF, 16, xmm16->as_VMReg()->next(4));
reg_def XMM16f( SOC, SOC, Op_RegF, 16, xmm16->as_VMReg()->next(5));
reg_def XMM16g( SOC, SOC, Op_RegF, 16, xmm16->as_VMReg()->next(6));
reg_def XMM16h( SOC, SOC, Op_RegF, 16, xmm16->as_VMReg()->next(7));
reg_def XMM16i( SOC, SOC, Op_RegF, 16, xmm16->as_VMReg()->next(8));
reg_def XMM16j( SOC, SOC, Op_RegF, 16, xmm16->as_VMReg()->next(9));
reg_def XMM16k( SOC, SOC, Op_RegF, 16, xmm16->as_VMReg()->next(10));
reg_def XMM16l( SOC, SOC, Op_RegF, 16, xmm16->as_VMReg()->next(11));
reg_def XMM16m( SOC, SOC, Op_RegF, 16, xmm16->as_VMReg()->next(12));
reg_def XMM16n( SOC, SOC, Op_RegF, 16, xmm16->as_VMReg()->next(13));
reg_def XMM16o( SOC, SOC, Op_RegF, 16, xmm16->as_VMReg()->next(14));
reg_def XMM16p( SOC, SOC, Op_RegF, 16, xmm16->as_VMReg()->next(15));

reg_def XMM17 ( SOC, SOC, Op_RegF, 17, xmm17->as_VMReg());
reg_def XMM17b( SOC, SOC, Op_RegF, 17, xmm17->as_VMReg()->next(1));
reg_def XMM17c( SOC, SOC, Op_RegF, 17, xmm17->as_VMReg()->next(2));
reg_def XMM17d( SOC, SOC, Op_RegF, 17, xmm17->as_VMReg()->next(3));
reg_def XMM17e( SOC, SOC, Op_RegF, 17, xmm17->as_VMReg()->next(4));
reg_def XMM17f( SOC, SOC, Op_RegF, 17, xmm17->as_VMReg()->next(5));
reg_def XMM17g( SOC, SOC, Op_RegF, 17, xmm17->as_VMReg()->next(6));
reg_def XMM17h( SOC, SOC, Op_RegF, 17, xmm17->as_VMReg()->next(7));
reg_def XMM17i( SOC, SOC, Op_RegF, 17, xmm17->as_VMReg()->next(8));
reg_def XMM17j( SOC, SOC, Op_RegF, 17, xmm17->as_VMReg()->next(9));
reg_def XMM17k( SOC, SOC, Op_RegF, 17, xmm17->as_VMReg()->next(10));
reg_def XMM17l( SOC, SOC, Op_RegF, 17, xmm17->as_VMReg()->next(11));
reg_def XMM17m( SOC, SOC, Op_RegF, 17, xmm17->as_VMReg()->next(12));
reg_def XMM17n( SOC, SOC, Op_RegF, 17, xmm17->as_VMReg()->next(13));
reg_def XMM17o( SOC, SOC, Op_RegF, 17, xmm17->as_VMReg()->next(14));
reg_def XMM17p( SOC, SOC, Op_RegF, 17, xmm17->as_VMReg()->next(15));

reg_def XMM18 ( SOC, SOC, Op_RegF, 18, xmm18->as_VMReg());
reg_def XMM18b( SOC, SOC, Op_RegF, 18, xmm18->as_VMReg()->next(1));
reg_def XMM18c( SOC, SOC, Op_RegF, 18, xmm18->as_VMReg()->next(2));
reg_def XMM18d( SOC, SOC, Op_RegF, 18, xmm18->as_VMReg()->next(3));
reg_def XMM18e( SOC, SOC, Op_RegF, 18, xmm18->as_VMReg()->next(4));
reg_def XMM18f( SOC, SOC, Op_RegF, 18, xmm18->as_VMReg()->next(5));
reg_def XMM18g( SOC, SOC, Op_RegF, 18, xmm18->as_VMReg()->next(6));
reg_def XMM18h( SOC, SOC, Op_RegF, 18, xmm18->as_VMReg()->next(7));
reg_def XMM18i( SOC, SOC, Op_RegF, 18, xmm18->as_VMReg()->next(8));
reg_def XMM18j( SOC, SOC, Op_RegF, 18, xmm18->as_VMReg()->next(9));
reg_def XMM18k( SOC, SOC, Op_RegF, 18, xmm18->as_VMReg()->next(10));
reg_def XMM18l( SOC, SOC, Op_RegF, 18, xmm18->as_VMReg()->next(11));
reg_def XMM18m( SOC, SOC, Op_RegF, 18, xmm18->as_VMReg()->next(12));
reg_def XMM18n( SOC, SOC, Op_RegF, 18, xmm18->as_VMReg()->next(13));
reg_def XMM18o( SOC, SOC, Op_RegF, 18, xmm18->as_VMReg()->next(14));
reg_def XMM18p( SOC, SOC, Op_RegF, 18, xmm18->as_VMReg()->next(15));

reg_def XMM19 ( SOC, SOC, Op_RegF, 19, xmm19->as_VMReg());
reg_def XMM19b( SOC, SOC, Op_RegF, 19, xmm19->as_VMReg()->next(1));
reg_def XMM19c( SOC, SOC, Op_RegF, 19, xmm19->as_VMReg()->next(2));
reg_def XMM19d( SOC, SOC, Op_RegF, 19, xmm19->as_VMReg()->next(3));
reg_def XMM19e( SOC, SOC, Op_RegF, 19, xmm19->as_VMReg()->next(4));
reg_def XMM19f( SOC, SOC, Op_RegF, 19, xmm19->as_VMReg()->next(5));
reg_def XMM19g( SOC, SOC, Op_RegF, 19, xmm19->as_VMReg()->next(6));
reg_def XMM19h( SOC, SOC, Op_RegF, 19, xmm19->as_VMReg()->next(7));
reg_def XMM19i( SOC, SOC, Op_RegF, 19, xmm19->as_VMReg()->next(8));
reg_def XMM19j( SOC, SOC, Op_RegF, 19, xmm19->as_VMReg()->next(9));
reg_def XMM19k( SOC, SOC, Op_RegF, 19, xmm19->as_VMReg()->next(10));
reg_def XMM19l( SOC, SOC, Op_RegF, 19, xmm19->as_VMReg()->next(11));
reg_def XMM19m( SOC, SOC, Op_RegF, 19, xmm19->as_VMReg()->next(12));
reg_def XMM19n( SOC, SOC, Op_RegF, 19, xmm19->as_VMReg()->next(13));
reg_def XMM19o( SOC, SOC, Op_RegF, 19, xmm19->as_VMReg()->next(14));
reg_def XMM19p( SOC, SOC, Op_RegF, 19, xmm19->as_VMReg()->next(15));

reg_def XMM20 ( SOC, SOC, Op_RegF, 20, xmm20->as_VMReg());
reg_def XMM20b( SOC, SOC, Op_RegF, 20, xmm20->as_VMReg()->next(1));
reg_def XMM20c( SOC, SOC, Op_RegF, 20, xmm20->as_VMReg()->next(2));
reg_def XMM20d( SOC, SOC, Op_RegF, 20, xmm20->as_VMReg()->next(3));
reg_def XMM20e( SOC, SOC, Op_RegF, 20, xmm20->as_VMReg()->next(4));
reg_def XMM20f( SOC, SOC, Op_RegF, 20, xmm20->as_VMReg()->next(5));
reg_def XMM20g( SOC, SOC, Op_RegF, 20, xmm20->as_VMReg()->next(6));
reg_def XMM20h( SOC, SOC, Op_RegF, 20, xmm20->as_VMReg()->next(7));
reg_def XMM20i( SOC, SOC, Op_RegF, 20, xmm20->as_VMReg()->next(8));
reg_def XMM20j( SOC, SOC, Op_RegF, 20, xmm20->as_VMReg()->next(9));
reg_def XMM20k( SOC, SOC, Op_RegF, 20, xmm20->as_VMReg()->next(10));
reg_def XMM20l( SOC, SOC, Op_RegF, 20, xmm20->as_VMReg()->next(11));
reg_def XMM20m( SOC, SOC, Op_RegF, 20, xmm20->as_VMReg()->next(12));
reg_def XMM20n( SOC, SOC, Op_RegF, 20, xmm20->as_VMReg()->next(13));
reg_def XMM20o( SOC, SOC, Op_RegF, 20, xmm20->as_VMReg()->next(14));
reg_def XMM20p( SOC, SOC, Op_RegF, 20, xmm20->as_VMReg()->next(15));

reg_def XMM21 ( SOC, SOC, Op_RegF, 21, xmm21->as_VMReg());
reg_def XMM21b( SOC, SOC, Op_RegF, 21, xmm21->as_VMReg()->next(1));
reg_def XMM21c( SOC, SOC, Op_RegF, 21, xmm21->as_VMReg()->next(2));
reg_def XMM21d( SOC, SOC, Op_RegF, 21, xmm21->as_VMReg()->next(3));
reg_def XMM21e( SOC, SOC, Op_RegF, 21, xmm21->as_VMReg()->next(4));
reg_def XMM21f( SOC, SOC, Op_RegF, 21, xmm21->as_VMReg()->next(5));
reg_def XMM21g( SOC, SOC, Op_RegF, 21, xmm21->as_VMReg()->next(6));
reg_def XMM21h( SOC, SOC, Op_RegF, 21, xmm21->as_VMReg()->next(7));
reg_def XMM21i( SOC, SOC, Op_RegF, 21, xmm21->as_VMReg()->next(8));
reg_def XMM21j( SOC, SOC, Op_RegF, 21, xmm21->as_VMReg()->next(9));
reg_def XMM21k( SOC, SOC, Op_RegF, 21, xmm21->as_VMReg()->next(10));
reg_def XMM21l( SOC, SOC, Op_RegF, 21, xmm21->as_VMReg()->next(11));
reg_def XMM21m( SOC, SOC, Op_RegF, 21, xmm21->as_VMReg()->next(12));
reg_def XMM21n( SOC, SOC, Op_RegF, 21, xmm21->as_VMReg()->next(13));
reg_def XMM21o( SOC, SOC, Op_RegF, 21, xmm21->as_VMReg()->next(14));
reg_def XMM21p( SOC, SOC, Op_RegF, 21, xmm21->as_VMReg()->next(15));

reg_def XMM22 ( SOC, SOC, Op_RegF, 22, xmm22->as_VMReg());
reg_def XMM22b( SOC, SOC, Op_RegF, 22, xmm22->as_VMReg()->next(1));
reg_def XMM22c( SOC, SOC, Op_RegF, 22, xmm22->as_VMReg()->next(2));
reg_def XMM22d( SOC, SOC, Op_RegF, 22, xmm22->as_VMReg()->next(3));
reg_def XMM22e( SOC, SOC, Op_RegF, 22, xmm22->as_VMReg()->next(4));
reg_def XMM22f( SOC, SOC, Op_RegF, 22, xmm22->as_VMReg()->next(5));
reg_def XMM22g( SOC, SOC, Op_RegF, 22, xmm22->as_VMReg()->next(6));
reg_def XMM22h( SOC, SOC, Op_RegF, 22, xmm22->as_VMReg()->next(7));
reg_def XMM22i( SOC, SOC, Op_RegF, 22, xmm22->as_VMReg()->next(8));
reg_def XMM22j( SOC, SOC, Op_RegF, 22, xmm22->as_VMReg()->next(9));
reg_def XMM22k( SOC, SOC, Op_RegF, 22, xmm22->as_VMReg()->next(10));
reg_def XMM22l( SOC, SOC, Op_RegF, 22, xmm22->as_VMReg()->next(11));
reg_def XMM22m( SOC, SOC, Op_RegF, 22, xmm22->as_VMReg()->next(12));
reg_def XMM22n( SOC, SOC, Op_RegF, 22, xmm22->as_VMReg()->next(13));
reg_def XMM22o( SOC, SOC, Op_RegF, 22, xmm22->as_VMReg()->next(14));
reg_def XMM22p( SOC, SOC, Op_RegF, 22, xmm22->as_VMReg()->next(15));

reg_def XMM23 ( SOC, SOC, Op_RegF, 23, xmm23->as_VMReg());
reg_def XMM23b( SOC, SOC, Op_RegF, 23, xmm23->as_VMReg()->next(1));
reg_def XMM23c( SOC, SOC, Op_RegF, 23, xmm23->as_VMReg()->next(2));
reg_def XMM23d( SOC, SOC, Op_RegF, 23, xmm23->as_VMReg()->next(3));
reg_def XMM23e( SOC, SOC, Op_RegF, 23, xmm23->as_VMReg()->next(4));
reg_def XMM23f( SOC, SOC, Op_RegF, 23, xmm23->as_VMReg()->next(5));
reg_def XMM23g( SOC, SOC, Op_RegF, 23, xmm23->as_VMReg()->next(6));
reg_def XMM23h( SOC, SOC, Op_RegF, 23, xmm23->as_VMReg()->next(7));
reg_def XMM23i( SOC, SOC, Op_RegF, 23, xmm23->as_VMReg()->next(8));
reg_def XMM23j( SOC, SOC, Op_RegF, 23, xmm23->as_VMReg()->next(9));
reg_def XMM23k( SOC, SOC, Op_RegF, 23, xmm23->as_VMReg()->next(10));
reg_def XMM23l( SOC, SOC, Op_RegF, 23, xmm23->as_VMReg()->next(11));
reg_def XMM23m( SOC, SOC, Op_RegF, 23, xmm23->as_VMReg()->next(12));
reg_def XMM23n( SOC, SOC, Op_RegF, 23, xmm23->as_VMReg()->next(13));
reg_def XMM23o( SOC, SOC, Op_RegF, 23, xmm23->as_VMReg()->next(14));
reg_def XMM23p( SOC, SOC, Op_RegF, 23, xmm23->as_VMReg()->next(15));

reg_def XMM24 ( SOC, SOC, Op_RegF, 24, xmm24->as_VMReg());
reg_def XMM24b( SOC, SOC, Op_RegF, 24, xmm24->as_VMReg()->next(1));
reg_def XMM24c( SOC, SOC, Op_RegF, 24, xmm24->as_VMReg()->next(2));
reg_def XMM24d( SOC, SOC, Op_RegF, 24, xmm24->as_VMReg()->next(3));
reg_def XMM24e( SOC, SOC, Op_RegF, 24, xmm24->as_VMReg()->next(4));
reg_def XMM24f( SOC, SOC, Op_RegF, 24, xmm24->as_VMReg()->next(5));
reg_def XMM24g( SOC, SOC, Op_RegF, 24, xmm24->as_VMReg()->next(6));
reg_def XMM24h( SOC, SOC, Op_RegF, 24, xmm24->as_VMReg()->next(7));
reg_def XMM24i( SOC, SOC, Op_RegF, 24, xmm24->as_VMReg()->next(8));
reg_def XMM24j( SOC, SOC, Op_RegF, 24, xmm24->as_VMReg()->next(9));
reg_def XMM24k( SOC, SOC, Op_RegF, 24, xmm24->as_VMReg()->next(10));
reg_def XMM24l( SOC, SOC, Op_RegF, 24, xmm24->as_VMReg()->next(11));
reg_def XMM24m( SOC, SOC, Op_RegF, 24, xmm24->as_VMReg()->next(12));
reg_def XMM24n( SOC, SOC, Op_RegF, 24, xmm24->as_VMReg()->next(13));
reg_def XMM24o( SOC, SOC, Op_RegF, 24, xmm24->as_VMReg()->next(14));
reg_def XMM24p( SOC, SOC, Op_RegF, 24, xmm24->as_VMReg()->next(15));

reg_def XMM25 ( SOC, SOC, Op_RegF, 25, xmm25->as_VMReg());
reg_def XMM25b( SOC, SOC, Op_RegF, 25, xmm25->as_VMReg()->next(1));
reg_def XMM25c( SOC, SOC, Op_RegF, 25, xmm25->as_VMReg()->next(2));
reg_def XMM25d( SOC, SOC, Op_RegF, 25, xmm25->as_VMReg()->next(3));
reg_def XMM25e( SOC, SOC, Op_RegF, 25, xmm25->as_VMReg()->next(4));
reg_def XMM25f( SOC, SOC, Op_RegF, 25, xmm25->as_VMReg()->next(5));
reg_def XMM25g( SOC, SOC, Op_RegF, 25, xmm25->as_VMReg()->next(6));
reg_def XMM25h( SOC, SOC, Op_RegF, 25, xmm25->as_VMReg()->next(7));
reg_def XMM25i( SOC, SOC, Op_RegF, 25, xmm25->as_VMReg()->next(8));
reg_def XMM25j( SOC, SOC, Op_RegF, 25, xmm25->as_VMReg()->next(9));
reg_def XMM25k( SOC, SOC, Op_RegF, 25, xmm25->as_VMReg()->next(10));
reg_def XMM25l( SOC, SOC, Op_RegF, 25, xmm25->as_VMReg()->next(11));
reg_def XMM25m( SOC, SOC, Op_RegF, 25, xmm25->as_VMReg()->next(12));
reg_def XMM25n( SOC, SOC, Op_RegF, 25, xmm25->as_VMReg()->next(13));
reg_def XMM25o( SOC, SOC, Op_RegF, 25, xmm25->as_VMReg()->next(14));
reg_def XMM25p( SOC, SOC, Op_RegF, 25, xmm25->as_VMReg()->next(15));

reg_def XMM26 ( SOC, SOC, Op_RegF, 26, xmm26->as_VMReg());
reg_def XMM26b( SOC, SOC, Op_RegF, 26, xmm26->as_VMReg()->next(1));
reg_def XMM26c( SOC, SOC, Op_RegF, 26, xmm26->as_VMReg()->next(2));
reg_def XMM26d( SOC, SOC, Op_RegF, 26, xmm26->as_VMReg()->next(3));
reg_def XMM26e( SOC, SOC, Op_RegF, 26, xmm26->as_VMReg()->next(4));
reg_def XMM26f( SOC, SOC, Op_RegF, 26, xmm26->as_VMReg()->next(5));
reg_def XMM26g( SOC, SOC, Op_RegF, 26, xmm26->as_VMReg()->next(6));
reg_def XMM26h( SOC, SOC, Op_RegF, 26, xmm26->as_VMReg()->next(7));
reg_def XMM26i( SOC, SOC, Op_RegF, 26, xmm26->as_VMReg()->next(8));
reg_def XMM26j( SOC, SOC, Op_RegF, 26, xmm26->as_VMReg()->next(9));
reg_def XMM26k( SOC, SOC, Op_RegF, 26, xmm26->as_VMReg()->next(10));
reg_def XMM26l( SOC, SOC, Op_RegF, 26, xmm26->as_VMReg()->next(11));
reg_def XMM26m( SOC, SOC, Op_RegF, 26, xmm26->as_VMReg()->next(12));
reg_def XMM26n( SOC, SOC, Op_RegF, 26, xmm26->as_VMReg()->next(13));
reg_def XMM26o( SOC, SOC, Op_RegF, 26, xmm26->as_VMReg()->next(14));
reg_def XMM26p( SOC, SOC, Op_RegF, 26, xmm26->as_VMReg()->next(15));

reg_def XMM27 ( SOC, SOC, Op_RegF, 27, xmm27->as_VMReg());
reg_def XMM27b( SOC, SOC, Op_RegF, 27, xmm27->as_VMReg()->next(1));
reg_def XMM27c( SOC, SOC, Op_RegF, 27, xmm27->as_VMReg()->next(2));
reg_def XMM27d( SOC, SOC, Op_RegF, 27, xmm27->as_VMReg()->next(3));
reg_def XMM27e( SOC, SOC, Op_RegF, 27, xmm27->as_VMReg()->next(4));
reg_def XMM27f( SOC, SOC, Op_RegF, 27, xmm27->as_VMReg()->next(5));
reg_def XMM27g( SOC, SOC, Op_RegF, 27, xmm27->as_VMReg()->next(6));
reg_def XMM27h( SOC, SOC, Op_RegF, 27, xmm27->as_VMReg()->next(7));
reg_def XMM27i( SOC, SOC, Op_RegF, 27, xmm27->as_VMReg()->next(8));
reg_def XMM27j( SOC, SOC, Op_RegF, 27, xmm27->as_VMReg()->next(9));
reg_def XMM27k( SOC, SOC, Op_RegF, 27, xmm27->as_VMReg()->next(10));
reg_def XMM27l( SOC, SOC, Op_RegF, 27, xmm27->as_VMReg()->next(11));
reg_def XMM27m( SOC, SOC, Op_RegF, 27, xmm27->as_VMReg()->next(12));
reg_def XMM27n( SOC, SOC, Op_RegF, 27, xmm27->as_VMReg()->next(13));
reg_def XMM27o( SOC, SOC, Op_RegF, 27, xmm27->as_VMReg()->next(14));
reg_def XMM27p( SOC, SOC, Op_RegF, 27, xmm27->as_VMReg()->next(15));

reg_def XMM28 ( SOC, SOC, Op_RegF, 28, xmm28->as_VMReg());
reg_def XMM28b( SOC, SOC, Op_RegF, 28, xmm28->as_VMReg()->next(1));
reg_def XMM28c( SOC, SOC, Op_RegF, 28, xmm28->as_VMReg()->next(2));
reg_def XMM28d( SOC, SOC, Op_RegF, 28, xmm28->as_VMReg()->next(3));
reg_def XMM28e( SOC, SOC, Op_RegF, 28, xmm28->as_VMReg()->next(4));
reg_def XMM28f( SOC, SOC, Op_RegF, 28, xmm28->as_VMReg()->next(5));
reg_def XMM28g( SOC, SOC, Op_RegF, 28, xmm28->as_VMReg()->next(6));
reg_def XMM28h( SOC, SOC, Op_RegF, 28, xmm28->as_VMReg()->next(7));
reg_def XMM28i( SOC, SOC, Op_RegF, 28, xmm28->as_VMReg()->next(8));
reg_def XMM28j( SOC, SOC, Op_RegF, 28, xmm28->as_VMReg()->next(9));
reg_def XMM28k( SOC, SOC, Op_RegF, 28, xmm28->as_VMReg()->next(10));
reg_def XMM28l( SOC, SOC, Op_RegF, 28, xmm28->as_VMReg()->next(11));
reg_def XMM28m( SOC, SOC, Op_RegF, 28, xmm28->as_VMReg()->next(12));
reg_def XMM28n( SOC, SOC, Op_RegF, 28, xmm28->as_VMReg()->next(13));
reg_def XMM28o( SOC, SOC, Op_RegF, 28, xmm28->as_VMReg()->next(14));
reg_def XMM28p( SOC, SOC, Op_RegF, 28, xmm28->as_VMReg()->next(15));

reg_def XMM29 ( SOC, SOC, Op_RegF, 29, xmm29->as_VMReg());
reg_def XMM29b( SOC, SOC, Op_RegF, 29, xmm29->as_VMReg()->next(1));
reg_def XMM29c( SOC, SOC, Op_RegF, 29, xmm29->as_VMReg()->next(2));
reg_def XMM29d( SOC, SOC, Op_RegF, 29, xmm29->as_VMReg()->next(3));
reg_def XMM29e( SOC, SOC, Op_RegF, 29, xmm29->as_VMReg()->next(4));
reg_def XMM29f( SOC, SOC, Op_RegF, 29, xmm29->as_VMReg()->next(5));
reg_def XMM29g( SOC, SOC, Op_RegF, 29, xmm29->as_VMReg()->next(6));
reg_def XMM29h( SOC, SOC, Op_RegF, 29, xmm29->as_VMReg()->next(7));
reg_def XMM29i( SOC, SOC, Op_RegF, 29, xmm29->as_VMReg()->next(8));
reg_def XMM29j( SOC, SOC, Op_RegF, 29, xmm29->as_VMReg()->next(9));
reg_def XMM29k( SOC, SOC, Op_RegF, 29, xmm29->as_VMReg()->next(10));
reg_def XMM29l( SOC, SOC, Op_RegF, 29, xmm29->as_VMReg()->next(11));
reg_def XMM29m( SOC, SOC, Op_RegF, 29, xmm29->as_VMReg()->next(12));
reg_def XMM29n( SOC, SOC, Op_RegF, 29, xmm29->as_VMReg()->next(13));
reg_def XMM29o( SOC, SOC, Op_RegF, 29, xmm29->as_VMReg()->next(14));
reg_def XMM29p( SOC, SOC, Op_RegF, 29, xmm29->as_VMReg()->next(15));

reg_def XMM30 ( SOC, SOC, Op_RegF, 30, xmm30->as_VMReg());
reg_def XMM30b( SOC, SOC, Op_RegF, 30, xmm30->as_VMReg()->next(1));
reg_def XMM30c( SOC, SOC, Op_RegF, 30, xmm30->as_VMReg()->next(2));
reg_def XMM30d( SOC, SOC, Op_RegF, 30, xmm30->as_VMReg()->next(3));
reg_def XMM30e( SOC, SOC, Op_RegF, 30, xmm30->as_VMReg()->next(4));
reg_def XMM30f( SOC, SOC, Op_RegF, 30, xmm30->as_VMReg()->next(5));
reg_def XMM30g( SOC, SOC, Op_RegF, 30, xmm30->as_VMReg()->next(6));
reg_def XMM30h( SOC, SOC, Op_RegF, 30, xmm30->as_VMReg()->next(7));
reg_def XMM30i( SOC, SOC, Op_RegF, 30, xmm30->as_VMReg()->next(8));
reg_def XMM30j( SOC, SOC, Op_RegF, 30, xmm30->as_VMReg()->next(9));
reg_def XMM30k( SOC, SOC, Op_RegF, 30, xmm30->as_VMReg()->next(10));
reg_def XMM30l( SOC, SOC, Op_RegF, 30, xmm30->as_VMReg()->next(11));
reg_def XMM30m( SOC, SOC, Op_RegF, 30, xmm30->as_VMReg()->next(12));
reg_def XMM30n( SOC, SOC, Op_RegF, 30, xmm30->as_VMReg()->next(13));
reg_def XMM30o( SOC, SOC, Op_RegF, 30, xmm30->as_VMReg()->next(14));
reg_def XMM30p( SOC, SOC, Op_RegF, 30, xmm30->as_VMReg()->next(15));

reg_def XMM31 ( SOC, SOC, Op_RegF, 31, xmm31->as_VMReg());
reg_def XMM31b( SOC, SOC, Op_RegF, 31, xmm31->as_VMReg()->next(1));
reg_def XMM31c( SOC, SOC, Op_RegF, 31, xmm31->as_VMReg()->next(2));
reg_def XMM31d( SOC, SOC, Op_RegF, 31, xmm31->as_VMReg()->next(3));
reg_def XMM31e( SOC, SOC, Op_RegF, 31, xmm31->as_VMReg()->next(4));
reg_def XMM31f( SOC, SOC, Op_RegF, 31, xmm31->as_VMReg()->next(5));
reg_def XMM31g( SOC, SOC, Op_RegF, 31, xmm31->as_VMReg()->next(6));
reg_def XMM31h( SOC, SOC, Op_RegF, 31, xmm31->as_VMReg()->next(7));
reg_def XMM31i( SOC, SOC, Op_RegF, 31, xmm31->as_VMReg()->next(8));
reg_def XMM31j( SOC, SOC, Op_RegF, 31, xmm31->as_VMReg()->next(9));
reg_def XMM31k( SOC, SOC, Op_RegF, 31, xmm31->as_VMReg()->next(10));
reg_def XMM31l( SOC, SOC, Op_RegF, 31, xmm31->as_VMReg()->next(11));
reg_def XMM31m( SOC, SOC, Op_RegF, 31, xmm31->as_VMReg()->next(12));
reg_def XMM31n( SOC, SOC, Op_RegF, 31, xmm31->as_VMReg()->next(13));
reg_def XMM31o( SOC, SOC, Op_RegF, 31, xmm31->as_VMReg()->next(14));
reg_def XMM31p( SOC, SOC, Op_RegF, 31, xmm31->as_VMReg()->next(15));

reg_def RFLAGS(SOC, SOC, 0, 16, VMRegImpl::Bad());

// AVX3 Mask Registers.
reg_def K1   (SOC, SOC, Op_RegI,  1, k1->as_VMReg());
reg_def K1_H (SOC, SOC, Op_RegI,  1, k1->as_VMReg()->next());

reg_def K2   (SOC, SOC, Op_RegI,  2, k2->as_VMReg());
reg_def K2_H (SOC, SOC, Op_RegI,  2, k2->as_VMReg()->next());

reg_def K3   (SOC, SOC, Op_RegI,  3, k3->as_VMReg());
reg_def K3_H (SOC, SOC, Op_RegI,  3, k3->as_VMReg()->next());

reg_def K4   (SOC, SOC, Op_RegI,  4, k4->as_VMReg());
reg_def K4_H (SOC, SOC, Op_RegI,  4, k4->as_VMReg()->next());

reg_def K5   (SOC, SOC, Op_RegI,  5, k5->as_VMReg());
reg_def K5_H (SOC, SOC, Op_RegI,  5, k5->as_VMReg()->next());

reg_def K6   (SOC, SOC, Op_RegI,  6, k6->as_VMReg());
reg_def K6_H (SOC, SOC, Op_RegI,  6, k6->as_VMReg()->next());

reg_def K7   (SOC, SOC, Op_RegI,  7, k7->as_VMReg());
reg_def K7_H (SOC, SOC, Op_RegI,  7, k7->as_VMReg()->next());


//----------Architecture Description Register Classes--------------------------
// Several register classes are automatically defined based upon information in
// this architecture description.
// 1) reg_class inline_cache_reg           ( /* as def'd in frame section */ )
// 2) reg_class stack_slots( /* one chunk of stack-based "registers" */ )
//

// Empty register class.
reg_class no_reg();

// Class for all pointer/long registers including APX extended GPRs.
reg_class all_reg(RAX, RAX_H,
                  RDX, RDX_H,
                  RBP, RBP_H,
                  RDI, RDI_H,
                  RSI, RSI_H,
                  RCX, RCX_H,
                  RBX, RBX_H,
                  RSP, RSP_H,
                  R8,  R8_H,
                  R9,  R9_H,
                  R10, R10_H,
                  R11, R11_H,
                  R12, R12_H,
                  R13, R13_H,
                  R14, R14_H,
                  R15, R15_H,
                  R16, R16_H,
                  R17, R17_H,
                  R18, R18_H,
                  R19, R19_H,
                  R20, R20_H,
                  R21, R21_H,
                  R22, R22_H,
                  R23, R23_H,
                  R24, R24_H,
                  R25, R25_H,
                  R26, R26_H,
                  R27, R27_H,
                  R28, R28_H,
                  R29, R29_H,
                  R30, R30_H,
                  R31, R31_H);

// Class for all int registers including APX extended GPRs.
reg_class all_int_reg(RAX
                      RDX,
                      RBP,
                      RDI,
                      RSI,
                      RCX,
                      RBX,
                      R8,
                      R9,
                      R10,
                      R11,
                      R12,
                      R13,
                      R14,
                      R16,
                      R17,
                      R18,
                      R19,
                      R20,
                      R21,
                      R22,
                      R23,
                      R24,
                      R25,
                      R26,
                      R27,
                      R28,
                      R29,
                      R30,
                      R31);

// Class for all pointer registers
reg_class any_reg %{
  return _ANY_REG_mask;
%}

// Class for all pointer registers (excluding RSP)
reg_class ptr_reg %{
  return _PTR_REG_mask;
%}

// Class for all pointer registers (excluding RSP and RBP)
reg_class ptr_reg_no_rbp %{
  return _PTR_REG_NO_RBP_mask;
%}

// Class for all pointer registers (excluding RAX and RSP)
reg_class ptr_no_rax_reg %{
  return _PTR_NO_RAX_REG_mask;
%}

// Class for all pointer registers (excluding RAX, RBX, and RSP)
reg_class ptr_no_rax_rbx_reg %{
  return _PTR_NO_RAX_RBX_REG_mask;
%}

// Class for all long registers (excluding RSP)
reg_class long_reg %{
  return _LONG_REG_mask;
%}

// Class for all long registers (excluding RAX, RDX and RSP)
reg_class long_no_rax_rdx_reg %{
  return _LONG_NO_RAX_RDX_REG_mask;
%}

// Class for all long registers (excluding RCX and RSP)
reg_class long_no_rcx_reg %{
  return _LONG_NO_RCX_REG_mask;
%}

// Class for all long registers (excluding RBP and R13)
reg_class long_no_rbp_r13_reg %{
  return _LONG_NO_RBP_R13_REG_mask;
%}

// Class for all int registers (excluding RSP)
reg_class int_reg %{
  return _INT_REG_mask;
%}

// Class for all int registers (excluding RAX, RDX, and RSP)
reg_class int_no_rax_rdx_reg %{
  return _INT_NO_RAX_RDX_REG_mask;
%}

// Class for all int registers (excluding RCX and RSP)
reg_class int_no_rcx_reg %{
  return _INT_NO_RCX_REG_mask;
%}

// Class for all int registers (excluding RBP and R13)
reg_class int_no_rbp_r13_reg %{
  return _INT_NO_RBP_R13_REG_mask;
%}

// Singleton class for RAX pointer register
reg_class ptr_rax_reg(RAX, RAX_H);

// Singleton class for RBX pointer register
reg_class ptr_rbx_reg(RBX, RBX_H);

// Singleton class for RSI pointer register
reg_class ptr_rsi_reg(RSI, RSI_H);

// Singleton class for RBP pointer register
reg_class ptr_rbp_reg(RBP, RBP_H);

// Singleton class for RDI pointer register
reg_class ptr_rdi_reg(RDI, RDI_H);

// Singleton class for stack pointer
reg_class ptr_rsp_reg(RSP, RSP_H);

// Singleton class for TLS pointer
reg_class ptr_r15_reg(R15, R15_H);

// Singleton class for RAX long register
reg_class long_rax_reg(RAX, RAX_H);

// Singleton class for RCX long register
reg_class long_rcx_reg(RCX, RCX_H);

// Singleton class for RDX long register
reg_class long_rdx_reg(RDX, RDX_H);

// Singleton class for R11 long register
reg_class long_r11_reg(R11, R11_H);

// Singleton class for RAX int register
reg_class int_rax_reg(RAX);

// Singleton class for RBX int register
reg_class int_rbx_reg(RBX);

// Singleton class for RCX int register
reg_class int_rcx_reg(RCX);

// Singleton class for RDX int register
reg_class int_rdx_reg(RDX);

// Singleton class for RDI int register
reg_class int_rdi_reg(RDI);

// Singleton class for instruction pointer
// reg_class ip_reg(RIP);

alloc_class chunk1(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
                   XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
                   XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
                   XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
                   XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
                   XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
                   XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
                   XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p,
                   XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
                   XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
                   XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
                   XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
                   XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
                   XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
                   XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
                   XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p,
                   XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,
                   XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h, XMM17i, XMM17j, XMM17k, XMM17l, XMM17m, XMM17n, XMM17o, XMM17p,
                   XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h, XMM18i, XMM18j, XMM18k, XMM18l, XMM18m, XMM18n, XMM18o, XMM18p,
                   XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h, XMM19i, XMM19j, XMM19k, XMM19l, XMM19m, XMM19n, XMM19o, XMM19p,
                   XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h, XMM20i, XMM20j, XMM20k, XMM20l, XMM20m, XMM20n, XMM20o, XMM20p,
                   XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h, XMM21i, XMM21j, XMM21k, XMM21l, XMM21m, XMM21n, XMM21o, XMM21p,
                   XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h, XMM22i, XMM22j, XMM22k, XMM22l, XMM22m, XMM22n, XMM22o, XMM22p,
                   XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h, XMM23i, XMM23j, XMM23k, XMM23l, XMM23m, XMM23n, XMM23o, XMM23p,
                   XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h, XMM24i, XMM24j, XMM24k, XMM24l, XMM24m, XMM24n, XMM24o, XMM24p,
                   XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h, XMM25i, XMM25j, XMM25k, XMM25l, XMM25m, XMM25n, XMM25o, XMM25p,
                   XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h, XMM26i, XMM26j, XMM26k, XMM26l, XMM26m, XMM26n, XMM26o, XMM26p,
                   XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h, XMM27i, XMM27j, XMM27k, XMM27l, XMM27m, XMM27n, XMM27o, XMM27p,
                   XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h, XMM28i, XMM28j, XMM28k, XMM28l, XMM28m, XMM28n, XMM28o, XMM28p,
                   XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h, XMM29i, XMM29j, XMM29k, XMM29l, XMM29m, XMM29n, XMM29o, XMM29p,
                   XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h, XMM30i, XMM30j, XMM30k, XMM30l, XMM30m, XMM30n, XMM30o, XMM30p,
                   XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p);

alloc_class chunk2(K7, K7_H,
                   K6, K6_H,
                   K5, K5_H,
                   K4, K4_H,
                   K3, K3_H,
                   K2, K2_H,
                   K1, K1_H);

reg_class  vectmask_reg(K1, K1_H,
                        K2, K2_H,
                        K3, K3_H,
                        K4, K4_H,
                        K5, K5_H,
                        K6, K6_H,
                        K7, K7_H);

reg_class vectmask_reg_K1(K1, K1_H);
reg_class vectmask_reg_K2(K2, K2_H);
reg_class vectmask_reg_K3(K3, K3_H);
reg_class vectmask_reg_K4(K4, K4_H);
reg_class vectmask_reg_K5(K5, K5_H);
reg_class vectmask_reg_K6(K6, K6_H);
reg_class vectmask_reg_K7(K7, K7_H);

// flags allocation class should be last.
alloc_class chunk3(RFLAGS);

// Singleton class for condition codes
reg_class int_flags(RFLAGS);

// Class for pre evex float registers
reg_class float_reg_legacy(XMM0,
                    XMM1,
                    XMM2,
                    XMM3,
                    XMM4,
                    XMM5,
                    XMM6,
                    XMM7,
                    XMM8,
                    XMM9,
                    XMM10,
                    XMM11,
                    XMM12,
                    XMM13,
                    XMM14,
                    XMM15);

// Class for evex float registers
reg_class float_reg_evex(XMM0,
                    XMM1,
                    XMM2,
                    XMM3,
                    XMM4,
                    XMM5,
                    XMM6,
                    XMM7,
                    XMM8,
                    XMM9,
                    XMM10,
                    XMM11,
                    XMM12,
                    XMM13,
                    XMM14,
                    XMM15,
                    XMM16,
                    XMM17,
                    XMM18,
                    XMM19,
                    XMM20,
                    XMM21,
                    XMM22,
                    XMM23,
                    XMM24,
                    XMM25,
                    XMM26,
                    XMM27,
                    XMM28,
                    XMM29,
                    XMM30,
                    XMM31);

reg_class_dynamic float_reg(float_reg_evex, float_reg_legacy, %{ VM_Version::supports_evex() %} );
reg_class_dynamic float_reg_vl(float_reg_evex, float_reg_legacy, %{ VM_Version::supports_evex() && VM_Version::supports_avx512vl() %} );

// Class for pre evex double registers
reg_class double_reg_legacy(XMM0,  XMM0b,
                     XMM1,  XMM1b,
                     XMM2,  XMM2b,
                     XMM3,  XMM3b,
                     XMM4,  XMM4b,
                     XMM5,  XMM5b,
                     XMM6,  XMM6b,
                     XMM7,  XMM7b,
                     XMM8,  XMM8b,
                     XMM9,  XMM9b,
                     XMM10, XMM10b,
                     XMM11, XMM11b,
                     XMM12, XMM12b,
                     XMM13, XMM13b,
                     XMM14, XMM14b,
                     XMM15, XMM15b);

// Class for evex double registers
reg_class double_reg_evex(XMM0,  XMM0b,
                     XMM1,  XMM1b,
                     XMM2,  XMM2b,
                     XMM3,  XMM3b,
                     XMM4,  XMM4b,
                     XMM5,  XMM5b,
                     XMM6,  XMM6b,
                     XMM7,  XMM7b,
                     XMM8,  XMM8b,
                     XMM9,  XMM9b,
                     XMM10, XMM10b,
                     XMM11, XMM11b,
                     XMM12, XMM12b,
                     XMM13, XMM13b,
                     XMM14, XMM14b,
                     XMM15, XMM15b,
                     XMM16, XMM16b,
                     XMM17, XMM17b,
                     XMM18, XMM18b,
                     XMM19, XMM19b,
                     XMM20, XMM20b,
                     XMM21, XMM21b,
                     XMM22, XMM22b,
                     XMM23, XMM23b,
                     XMM24, XMM24b,
                     XMM25, XMM25b,
                     XMM26, XMM26b,
                     XMM27, XMM27b,
                     XMM28, XMM28b,
                     XMM29, XMM29b,
                     XMM30, XMM30b,
                     XMM31, XMM31b);

reg_class_dynamic double_reg(double_reg_evex, double_reg_legacy, %{ VM_Version::supports_evex() %} );
reg_class_dynamic double_reg_vl(double_reg_evex, double_reg_legacy, %{ VM_Version::supports_evex() && VM_Version::supports_avx512vl() %} );

// Class for pre evex 32bit vector registers
reg_class vectors_reg_legacy(XMM0,
                      XMM1,
                      XMM2,
                      XMM3,
                      XMM4,
                      XMM5,
                      XMM6,
                      XMM7,
                      XMM8,
                      XMM9,
                      XMM10,
                      XMM11,
                      XMM12,
                      XMM13,
                      XMM14,
                      XMM15);

// Class for evex 32bit vector registers
reg_class vectors_reg_evex(XMM0,
                      XMM1,
                      XMM2,
                      XMM3,
                      XMM4,
                      XMM5,
                      XMM6,
                      XMM7,
                      XMM8,
                      XMM9,
                      XMM10,
                      XMM11,
                      XMM12,
                      XMM13,
                      XMM14,
                      XMM15,
                      XMM16,
                      XMM17,
                      XMM18,
                      XMM19,
                      XMM20,
                      XMM21,
                      XMM22,
                      XMM23,
                      XMM24,
                      XMM25,
                      XMM26,
                      XMM27,
                      XMM28,
                      XMM29,
                      XMM30,
                      XMM31);

reg_class_dynamic vectors_reg(vectors_reg_evex, vectors_reg_legacy, %{ VM_Version::supports_evex() %} );
reg_class_dynamic vectors_reg_vlbwdq(vectors_reg_evex, vectors_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );

// Class for all 64bit vector registers
reg_class vectord_reg_legacy(XMM0,  XMM0b,
                      XMM1,  XMM1b,
                      XMM2,  XMM2b,
                      XMM3,  XMM3b,
                      XMM4,  XMM4b,
                      XMM5,  XMM5b,
                      XMM6,  XMM6b,
                      XMM7,  XMM7b,
                      XMM8,  XMM8b,
                      XMM9,  XMM9b,
                      XMM10, XMM10b,
                      XMM11, XMM11b,
                      XMM12, XMM12b,
                      XMM13, XMM13b,
                      XMM14, XMM14b,
                      XMM15, XMM15b);

// Class for all 64bit vector registers
reg_class vectord_reg_evex(XMM0,  XMM0b,
                      XMM1,  XMM1b,
                      XMM2,  XMM2b,
                      XMM3,  XMM3b,
                      XMM4,  XMM4b,
                      XMM5,  XMM5b,
                      XMM6,  XMM6b,
                      XMM7,  XMM7b,
                      XMM8,  XMM8b,
                      XMM9,  XMM9b,
                      XMM10, XMM10b,
                      XMM11, XMM11b,
                      XMM12, XMM12b,
                      XMM13, XMM13b,
                      XMM14, XMM14b,
                      XMM15, XMM15b,
                      XMM16, XMM16b,
                      XMM17, XMM17b,
                      XMM18, XMM18b,
                      XMM19, XMM19b,
                      XMM20, XMM20b,
                      XMM21, XMM21b,
                      XMM22, XMM22b,
                      XMM23, XMM23b,
                      XMM24, XMM24b,
                      XMM25, XMM25b,
                      XMM26, XMM26b,
                      XMM27, XMM27b,
                      XMM28, XMM28b,
                      XMM29, XMM29b,
                      XMM30, XMM30b,
                      XMM31, XMM31b);

reg_class_dynamic vectord_reg(vectord_reg_evex, vectord_reg_legacy, %{ VM_Version::supports_evex() %} );
reg_class_dynamic vectord_reg_vlbwdq(vectord_reg_evex, vectord_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );

// Class for all 128bit vector registers
reg_class vectorx_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,
                      XMM1,  XMM1b,  XMM1c,  XMM1d,
                      XMM2,  XMM2b,  XMM2c,  XMM2d,
                      XMM3,  XMM3b,  XMM3c,  XMM3d,
                      XMM4,  XMM4b,  XMM4c,  XMM4d,
                      XMM5,  XMM5b,  XMM5c,  XMM5d,
                      XMM6,  XMM6b,  XMM6c,  XMM6d,
                      XMM7,  XMM7b,  XMM7c,  XMM7d,
                      XMM8,  XMM8b,  XMM8c,  XMM8d,
                      XMM9,  XMM9b,  XMM9c,  XMM9d,
                      XMM10, XMM10b, XMM10c, XMM10d,
                      XMM11, XMM11b, XMM11c, XMM11d,
                      XMM12, XMM12b, XMM12c, XMM12d,
                      XMM13, XMM13b, XMM13c, XMM13d,
                      XMM14, XMM14b, XMM14c, XMM14d,
                      XMM15, XMM15b, XMM15c, XMM15d);

// Class for all 128bit vector registers
reg_class vectorx_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,
                      XMM1,  XMM1b,  XMM1c,  XMM1d,
                      XMM2,  XMM2b,  XMM2c,  XMM2d,
                      XMM3,  XMM3b,  XMM3c,  XMM3d,
                      XMM4,  XMM4b,  XMM4c,  XMM4d,
                      XMM5,  XMM5b,  XMM5c,  XMM5d,
                      XMM6,  XMM6b,  XMM6c,  XMM6d,
                      XMM7,  XMM7b,  XMM7c,  XMM7d,
                      XMM8,  XMM8b,  XMM8c,  XMM8d,
                      XMM9,  XMM9b,  XMM9c,  XMM9d,
                      XMM10, XMM10b, XMM10c, XMM10d,
                      XMM11, XMM11b, XMM11c, XMM11d,
                      XMM12, XMM12b, XMM12c, XMM12d,
                      XMM13, XMM13b, XMM13c, XMM13d,
                      XMM14, XMM14b, XMM14c, XMM14d,
                      XMM15, XMM15b, XMM15c, XMM15d,
                      XMM16, XMM16b, XMM16c, XMM16d,
                      XMM17, XMM17b, XMM17c, XMM17d,
                      XMM18, XMM18b, XMM18c, XMM18d,
                      XMM19, XMM19b, XMM19c, XMM19d,
                      XMM20, XMM20b, XMM20c, XMM20d,
                      XMM21, XMM21b, XMM21c, XMM21d,
                      XMM22, XMM22b, XMM22c, XMM22d,
                      XMM23, XMM23b, XMM23c, XMM23d,
                      XMM24, XMM24b, XMM24c, XMM24d,
                      XMM25, XMM25b, XMM25c, XMM25d,
                      XMM26, XMM26b, XMM26c, XMM26d,
                      XMM27, XMM27b, XMM27c, XMM27d,
                      XMM28, XMM28b, XMM28c, XMM28d,
                      XMM29, XMM29b, XMM29c, XMM29d,
                      XMM30, XMM30b, XMM30c, XMM30d,
                      XMM31, XMM31b, XMM31c, XMM31d);

reg_class_dynamic vectorx_reg(vectorx_reg_evex, vectorx_reg_legacy, %{ VM_Version::supports_evex() %} );
reg_class_dynamic vectorx_reg_vlbwdq(vectorx_reg_evex, vectorx_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );

// Class for all 256bit vector registers
reg_class vectory_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,
                      XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,
                      XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,
                      XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,
                      XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,
                      XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,
                      XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,
                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,
                      XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,
                      XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,
                      XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h,
                      XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h,
                      XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h,
                      XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h,
                      XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h,
                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h);

// Class for all 256bit vector registers
reg_class vectory_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,
                      XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,
                      XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,
                      XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,
                      XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,
                      XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,
                      XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,
                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,
                      XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,
                      XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,
                      XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h,
                      XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h,
                      XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h,
                      XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h,
                      XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h,
                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h,
                      XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h,
                      XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h,
                      XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h,
                      XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h,
                      XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h,
                      XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h,
                      XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h,
                      XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h,
                      XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h,
                      XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h,
                      XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h,
                      XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h,
                      XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h,
                      XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h,
                      XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h,
                      XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h);

reg_class_dynamic vectory_reg(vectory_reg_evex, vectory_reg_legacy, %{ VM_Version::supports_evex() %} );
reg_class_dynamic vectory_reg_vlbwdq(vectory_reg_evex, vectory_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );

// Class for all 512bit vector registers
reg_class vectorz_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
                      XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
                      XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
                      XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
                      XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
                      XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
                      XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p,
                      XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
                      XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
                      XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
                      XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
                      XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
                      XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
                      XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p,
                      XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,
                      XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h, XMM17i, XMM17j, XMM17k, XMM17l, XMM17m, XMM17n, XMM17o, XMM17p,
                      XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h, XMM18i, XMM18j, XMM18k, XMM18l, XMM18m, XMM18n, XMM18o, XMM18p,
                      XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h, XMM19i, XMM19j, XMM19k, XMM19l, XMM19m, XMM19n, XMM19o, XMM19p,
                      XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h, XMM20i, XMM20j, XMM20k, XMM20l, XMM20m, XMM20n, XMM20o, XMM20p,
                      XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h, XMM21i, XMM21j, XMM21k, XMM21l, XMM21m, XMM21n, XMM21o, XMM21p,
                      XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h, XMM22i, XMM22j, XMM22k, XMM22l, XMM22m, XMM22n, XMM22o, XMM22p,
                      XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h, XMM23i, XMM23j, XMM23k, XMM23l, XMM23m, XMM23n, XMM23o, XMM23p,
                      XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h, XMM24i, XMM24j, XMM24k, XMM24l, XMM24m, XMM24n, XMM24o, XMM24p,
                      XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h, XMM25i, XMM25j, XMM25k, XMM25l, XMM25m, XMM25n, XMM25o, XMM25p,
                      XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h, XMM26i, XMM26j, XMM26k, XMM26l, XMM26m, XMM26n, XMM26o, XMM26p,
                      XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h, XMM27i, XMM27j, XMM27k, XMM27l, XMM27m, XMM27n, XMM27o, XMM27p,
                      XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h, XMM28i, XMM28j, XMM28k, XMM28l, XMM28m, XMM28n, XMM28o, XMM28p,
                      XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h, XMM29i, XMM29j, XMM29k, XMM29l, XMM29m, XMM29n, XMM29o, XMM29p,
                      XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h, XMM30i, XMM30j, XMM30k, XMM30l, XMM30m, XMM30n, XMM30o, XMM30p,
                      XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p);

// Class for restricted 512bit vector registers
reg_class vectorz_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
                      XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
                      XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
                      XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
                      XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
                      XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
                      XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p,
                      XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
                      XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
                      XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
                      XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
                      XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
                      XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
                      XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p);

reg_class_dynamic vectorz_reg   (vectorz_reg_evex, vectorz_reg_legacy, %{ VM_Version::supports_evex() %} );
reg_class_dynamic vectorz_reg_vl(vectorz_reg_evex, vectorz_reg_legacy, %{ VM_Version::supports_evex() && VM_Version::supports_avx512vl() %} );

reg_class xmm0_reg(XMM0, XMM0b, XMM0c, XMM0d);

%}


//----------SOURCE BLOCK-------------------------------------------------------
// This is a block of C++ code which provides values, functions, and
// definitions necessary in the rest of the architecture description

source_hpp %{

#include "peephole_x86_64.hpp"

bool castLL_is_imm32(const Node* n);

%}

source %{

bool castLL_is_imm32(const Node* n) {
  assert(n->is_CastLL(), "must be a CastLL");
  const TypeLong* t = n->bottom_type()->is_long();
  return (t->_lo == min_jlong || Assembler::is_simm32(t->_lo)) && (t->_hi == max_jlong || Assembler::is_simm32(t->_hi));
}

%}

// Register masks
source_hpp %{

extern RegMask _ANY_REG_mask;
extern RegMask _PTR_REG_mask;
extern RegMask _PTR_REG_NO_RBP_mask;
extern RegMask _PTR_NO_RAX_REG_mask;
extern RegMask _PTR_NO_RAX_RBX_REG_mask;
extern RegMask _LONG_REG_mask;
extern RegMask _LONG_NO_RAX_RDX_REG_mask;
extern RegMask _LONG_NO_RCX_REG_mask;
extern RegMask _LONG_NO_RBP_R13_REG_mask;
extern RegMask _INT_REG_mask;
extern RegMask _INT_NO_RAX_RDX_REG_mask;
extern RegMask _INT_NO_RCX_REG_mask;
extern RegMask _INT_NO_RBP_R13_REG_mask;
extern RegMask _FLOAT_REG_mask;

extern RegMask _STACK_OR_PTR_REG_mask;
extern RegMask _STACK_OR_LONG_REG_mask;
extern RegMask _STACK_OR_INT_REG_mask;

inline const RegMask& STACK_OR_PTR_REG_mask()  { return _STACK_OR_PTR_REG_mask;  }
inline const RegMask& STACK_OR_LONG_REG_mask() { return _STACK_OR_LONG_REG_mask; }
inline const RegMask& STACK_OR_INT_REG_mask()  { return _STACK_OR_INT_REG_mask;  }

%}

source %{
#define   RELOC_IMM64    Assembler::imm_operand
#define   RELOC_DISP32   Assembler::disp32_operand

#define __ masm->

RegMask _ANY_REG_mask;
RegMask _PTR_REG_mask;
RegMask _PTR_REG_NO_RBP_mask;
RegMask _PTR_NO_RAX_REG_mask;
RegMask _PTR_NO_RAX_RBX_REG_mask;
RegMask _LONG_REG_mask;
RegMask _LONG_NO_RAX_RDX_REG_mask;
RegMask _LONG_NO_RCX_REG_mask;
RegMask _LONG_NO_RBP_R13_REG_mask;
RegMask _INT_REG_mask;
RegMask _INT_NO_RAX_RDX_REG_mask;
RegMask _INT_NO_RCX_REG_mask;
RegMask _INT_NO_RBP_R13_REG_mask;
RegMask _FLOAT_REG_mask;
RegMask _STACK_OR_PTR_REG_mask;
RegMask _STACK_OR_LONG_REG_mask;
RegMask _STACK_OR_INT_REG_mask;

static bool need_r12_heapbase() {
  return UseCompressedOops;
}

void reg_mask_init() {
  constexpr Register egprs[] = {r16, r17, r18, r19, r20, r21, r22, r23, r24, r25, r26, r27, r28, r29, r30, r31};

  // _ALL_REG_mask is generated by adlc from the all_reg register class below.
  // We derive a number of subsets from it.
  _ANY_REG_mask.assignFrom(_ALL_REG_mask);

  if (PreserveFramePointer) {
    _ANY_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));
    _ANY_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()->next()));
  }
  if (need_r12_heapbase()) {
    _ANY_REG_mask.remove(OptoReg::as_OptoReg(r12->as_VMReg()));
    _ANY_REG_mask.remove(OptoReg::as_OptoReg(r12->as_VMReg()->next()));
  }

  _PTR_REG_mask.assignFrom(_ANY_REG_mask);
  _PTR_REG_mask.remove(OptoReg::as_OptoReg(rsp->as_VMReg()));
  _PTR_REG_mask.remove(OptoReg::as_OptoReg(rsp->as_VMReg()->next()));
  _PTR_REG_mask.remove(OptoReg::as_OptoReg(r15->as_VMReg()));
  _PTR_REG_mask.remove(OptoReg::as_OptoReg(r15->as_VMReg()->next()));
  if (!UseAPX) {
    for (uint i = 0; i < sizeof(egprs)/sizeof(Register); i++) {
      _PTR_REG_mask.remove(OptoReg::as_OptoReg(egprs[i]->as_VMReg()));
      _PTR_REG_mask.remove(OptoReg::as_OptoReg(egprs[i]->as_VMReg()->next()));
    }
  }

  _STACK_OR_PTR_REG_mask.assignFrom(_PTR_REG_mask);
  _STACK_OR_PTR_REG_mask.or_with(STACK_OR_STACK_SLOTS_mask());

  _PTR_REG_NO_RBP_mask.assignFrom(_PTR_REG_mask);
  _PTR_REG_NO_RBP_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));
  _PTR_REG_NO_RBP_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()->next()));

  _PTR_NO_RAX_REG_mask.assignFrom(_PTR_REG_mask);
  _PTR_NO_RAX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()));
  _PTR_NO_RAX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()->next()));

  _PTR_NO_RAX_RBX_REG_mask.assignFrom(_PTR_NO_RAX_REG_mask);
  _PTR_NO_RAX_RBX_REG_mask.remove(OptoReg::as_OptoReg(rbx->as_VMReg()));
  _PTR_NO_RAX_RBX_REG_mask.remove(OptoReg::as_OptoReg(rbx->as_VMReg()->next()));


  _LONG_REG_mask.assignFrom(_PTR_REG_mask);
  _STACK_OR_LONG_REG_mask.assignFrom(_LONG_REG_mask);
  _STACK_OR_LONG_REG_mask.or_with(STACK_OR_STACK_SLOTS_mask());

  _LONG_NO_RAX_RDX_REG_mask.assignFrom(_LONG_REG_mask);
  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()));
  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()->next()));
  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rdx->as_VMReg()));
  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rdx->as_VMReg()->next()));

  _LONG_NO_RCX_REG_mask.assignFrom(_LONG_REG_mask);
  _LONG_NO_RCX_REG_mask.remove(OptoReg::as_OptoReg(rcx->as_VMReg()));
  _LONG_NO_RCX_REG_mask.remove(OptoReg::as_OptoReg(rcx->as_VMReg()->next()));

  _LONG_NO_RBP_R13_REG_mask.assignFrom(_LONG_REG_mask);
  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));
  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()->next()));
  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(r13->as_VMReg()));
  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(r13->as_VMReg()->next()));

  _INT_REG_mask.assignFrom(_ALL_INT_REG_mask);
  if (!UseAPX) {
    for (uint i = 0; i < sizeof(egprs)/sizeof(Register); i++) {
      _INT_REG_mask.remove(OptoReg::as_OptoReg(egprs[i]->as_VMReg()));
    }
  }

  if (PreserveFramePointer) {
    _INT_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));
  }
  if (need_r12_heapbase()) {
    _INT_REG_mask.remove(OptoReg::as_OptoReg(r12->as_VMReg()));
  }

  _STACK_OR_INT_REG_mask.assignFrom(_INT_REG_mask);
  _STACK_OR_INT_REG_mask.or_with(STACK_OR_STACK_SLOTS_mask());

  _INT_NO_RAX_RDX_REG_mask.assignFrom(_INT_REG_mask);
  _INT_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()));
  _INT_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rdx->as_VMReg()));

  _INT_NO_RCX_REG_mask.assignFrom(_INT_REG_mask);
  _INT_NO_RCX_REG_mask.remove(OptoReg::as_OptoReg(rcx->as_VMReg()));

  _INT_NO_RBP_R13_REG_mask.assignFrom(_INT_REG_mask);
  _INT_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));
  _INT_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(r13->as_VMReg()));

  // _FLOAT_REG_LEGACY_mask/_FLOAT_REG_EVEX_mask is generated by adlc
  // from the float_reg_legacy/float_reg_evex register class.
  _FLOAT_REG_mask.assignFrom(VM_Version::supports_evex() ? _FLOAT_REG_EVEX_mask : _FLOAT_REG_LEGACY_mask);
}

static bool generate_vzeroupper(Compile* C) {
  return (VM_Version::supports_vzeroupper() && (C->max_vector_size() > 16 || C->clear_upper_avx() == true)) ? true: false;  // Generate vzeroupper
}

static int clear_avx_size() {
  return generate_vzeroupper(Compile::current()) ? 3: 0;  // vzeroupper
}

// !!!!! Special hack to get all types of calls to specify the byte offset
//       from the start of the call to the point where the return address
//       will point.
int MachCallStaticJavaNode::ret_addr_offset()
{
  int offset = 5; // 5 bytes from start of call to where return address points
  offset += clear_avx_size();
  return offset;
}

int MachCallDynamicJavaNode::ret_addr_offset()
{
  int offset = 15; // 15 bytes from start of call to where return address points
  offset += clear_avx_size();
  return offset;
}

int MachCallRuntimeNode::ret_addr_offset() {
  int offset = 13; // movq r10,#addr; callq (r10)
  if (this->ideal_Opcode() != Op_CallLeafVector) {
    offset += clear_avx_size();
  }
  return offset;
}
//
// Compute padding required for nodes which need alignment
//

// The address of the call instruction needs to be 4-byte aligned to
// ensure that it does not span a cache line so that it can be patched.
int CallStaticJavaDirectNode::compute_padding(int current_offset) const
{
  current_offset += clear_avx_size(); // skip vzeroupper
  current_offset += 1; // skip call opcode byte
  return align_up(current_offset, alignment_required()) - current_offset;
}

// The address of the call instruction needs to be 4-byte aligned to
// ensure that it does not span a cache line so that it can be patched.
int CallDynamicJavaDirectNode::compute_padding(int current_offset) const
{
  current_offset += clear_avx_size(); // skip vzeroupper
  current_offset += 11; // skip movq instruction + call opcode byte
  return align_up(current_offset, alignment_required()) - current_offset;
}

// This could be in MacroAssembler but it's fairly C2 specific
static void emit_cmpfp_fixup(MacroAssembler* masm) {
  Label exit;
  __ jccb(Assembler::noParity, exit);
  __ pushf();
  //
  // comiss/ucomiss instructions set ZF,PF,CF flags and
  // zero OF,AF,SF for NaN values.
  // Fixup flags by zeroing ZF,PF so that compare of NaN
  // values returns 'less than' result (CF is set).
  // Leave the rest of flags unchanged.
  //
  //    7 6 5 4 3 2 1 0
  //   |S|Z|r|A|r|P|r|C|  (r - reserved bit)
  //    0 0 1 0 1 0 1 1   (0x2B)
  //
  __ andq(Address(rsp, 0), 0xffffff2b);
  __ popf();
  __ bind(exit);
}

static void emit_cmpfp3(MacroAssembler* masm, Register dst) {
  Label done;
  __ movl(dst, -1);
  __ jcc(Assembler::parity, done);
  __ jcc(Assembler::below, done);
  __ setcc(Assembler::notEqual, dst);
  __ bind(done);
}

// Math.min()    # Math.max()
// --------------------------
// ucomis[s/d]   #
// ja   -> b     # a
// jp   -> NaN   # NaN
// jb   -> a     # b
// je            #
// |-jz -> a | b # a & b
// |    -> a     #
static void emit_fp_min_max(MacroAssembler* masm, XMMRegister dst,
                            XMMRegister a, XMMRegister b,
                            XMMRegister xmmt, Register rt,
                            bool min, bool single) {

  Label nan, zero, below, above, done;

  if (single)
    __ ucomiss(a, b);
  else
    __ ucomisd(a, b);

  if (dst->encoding() != (min ? b : a)->encoding())
    __ jccb(Assembler::above, above); // CF=0 & ZF=0
  else
    __ jccb(Assembler::above, done);

  __ jccb(Assembler::parity, nan);  // PF=1
  __ jccb(Assembler::below, below); // CF=1

  // equal
  __ vpxor(xmmt, xmmt, xmmt, Assembler::AVX_128bit);
  if (single) {
    __ ucomiss(a, xmmt);
    __ jccb(Assembler::equal, zero);

    __ movflt(dst, a);
    __ jmp(done);
  }
  else {
    __ ucomisd(a, xmmt);
    __ jccb(Assembler::equal, zero);

    __ movdbl(dst, a);
    __ jmp(done);
  }

  __ bind(zero);
  if (min)
    __ vpor(dst, a, b, Assembler::AVX_128bit);
  else
    __ vpand(dst, a, b, Assembler::AVX_128bit);

  __ jmp(done);

  __ bind(above);
  if (single)
    __ movflt(dst, min ? b : a);
  else
    __ movdbl(dst, min ? b : a);

  __ jmp(done);

  __ bind(nan);
  if (single) {
    __ movl(rt, 0x7fc00000); // Float.NaN
    __ movdl(dst, rt);
  }
  else {
    __ mov64(rt, 0x7ff8000000000000L); // Double.NaN
    __ movdq(dst, rt);
  }
  __ jmp(done);

  __ bind(below);
  if (single)
    __ movflt(dst, min ? a : b);
  else
    __ movdbl(dst, min ? a : b);

  __ bind(done);
}

//=============================================================================
const RegMask& MachConstantBaseNode::_out_RegMask = RegMask::EMPTY;

int ConstantTable::calculate_table_base_offset() const {
  return 0;  // absolute addressing, no offset
}

bool MachConstantBaseNode::requires_postalloc_expand() const { return false; }
void MachConstantBaseNode::postalloc_expand(GrowableArray <Node *> *nodes, PhaseRegAlloc *ra_) {
  ShouldNotReachHere();
}

void MachConstantBaseNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const {
  // Empty encoding
}

uint MachConstantBaseNode::size(PhaseRegAlloc* ra_) const {
  return 0;
}

#ifndef PRODUCT
void MachConstantBaseNode::format(PhaseRegAlloc* ra_, outputStream* st) const {
  st->print("# MachConstantBaseNode (empty encoding)");
}
#endif


//=============================================================================
#ifndef PRODUCT
void MachPrologNode::format(PhaseRegAlloc* ra_, outputStream* st) const {
  Compile* C = ra_->C;

  int framesize = C->output()->frame_size_in_bytes();
  int bangsize = C->output()->bang_size_in_bytes();
  assert((framesize & (StackAlignmentInBytes-1)) == 0, "frame size not aligned");
  // Remove wordSize for return addr which is already pushed.
  framesize -= wordSize;

  if (C->output()->need_stack_bang(bangsize)) {
    framesize -= wordSize;
    st->print("# stack bang (%d bytes)", bangsize);
    st->print("\n\t");
    st->print("pushq   rbp\t# Save rbp");
    if (PreserveFramePointer) {
        st->print("\n\t");
        st->print("movq    rbp, rsp\t# Save the caller's SP into rbp");
    }
    if (framesize) {
      st->print("\n\t");
      st->print("subq    rsp, #%d\t# Create frame",framesize);
    }
  } else {
    st->print("subq    rsp, #%d\t# Create frame",framesize);
    st->print("\n\t");
    framesize -= wordSize;
    st->print("movq    [rsp + #%d], rbp\t# Save rbp",framesize);
    if (PreserveFramePointer) {
      st->print("\n\t");
      st->print("movq    rbp, rsp\t# Save the caller's SP into rbp");
      if (framesize > 0) {
        st->print("\n\t");
        st->print("addq    rbp, #%d", framesize);
      }
    }
  }

  if (VerifyStackAtCalls) {
    st->print("\n\t");
    framesize -= wordSize;
    st->print("movq    [rsp + #%d], 0xbadb100d\t# Majik cookie for stack depth check",framesize);
#ifdef ASSERT
    st->print("\n\t");
    st->print("# stack alignment check");
#endif
  }
  if (C->stub_function() != nullptr) {
    st->print("\n\t");
    st->print("cmpl    [r15_thread + #disarmed_guard_value_offset], #disarmed_guard_value\t");
    st->print("\n\t");
    st->print("je      fast_entry\t");
    st->print("\n\t");
    st->print("call    #nmethod_entry_barrier_stub\t");
    st->print("\n\tfast_entry:");
  }
  st->cr();
}
#endif

void MachPrologNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {
  Compile* C = ra_->C;

  int framesize = C->output()->frame_size_in_bytes();
  int bangsize = C->output()->bang_size_in_bytes();

  if (C->clinit_barrier_on_entry()) {
    assert(VM_Version::supports_fast_class_init_checks(), "sanity");
    assert(!C->method()->holder()->is_not_initialized(), "initialization should have been started");

    Label L_skip_barrier;
    Register klass = rscratch1;

    __ mov_metadata(klass, C->method()->holder()->constant_encoding());
    __ clinit_barrier(klass, &L_skip_barrier /*L_fast_path*/);

    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); // slow path

    __ bind(L_skip_barrier);
  }

  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);

  C->output()->set_frame_complete(__ offset());

  if (C->has_mach_constant_base_node()) {
    // NOTE: We set the table base offset here because users might be
    // emitted before MachConstantBaseNode.
    ConstantTable& constant_table = C->output()->constant_table();
    constant_table.set_table_base_offset(constant_table.calculate_table_base_offset());
  }
}

uint MachPrologNode::size(PhaseRegAlloc* ra_) const
{
  return MachNode::size(ra_); // too many variables; just compute it
                              // the hard way
}

int MachPrologNode::reloc() const
{
  return 0; // a large enough number
}

//=============================================================================
#ifndef PRODUCT
void MachEpilogNode::format(PhaseRegAlloc* ra_, outputStream* st) const
{
  Compile* C = ra_->C;
  if (generate_vzeroupper(C)) {
    st->print("vzeroupper");
    st->cr(); st->print("\t");
  }

  int framesize = C->output()->frame_size_in_bytes();
  assert((framesize & (StackAlignmentInBytes-1)) == 0, "frame size not aligned");
  // Remove word for return adr already pushed
  // and RBP
  framesize -= 2*wordSize;

  if (framesize) {
    st->print_cr("addq    rsp, %d\t# Destroy frame", framesize);
    st->print("\t");
  }

  st->print_cr("popq    rbp");
  if (do_polling() && C->is_method_compilation()) {
    st->print("\t");
    st->print_cr("cmpq    rsp, poll_offset[r15_thread] \n\t"
                 "ja      #safepoint_stub\t"
                 "# Safepoint: poll for GC");
  }
}
#endif

void MachEpilogNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const
{
  Compile* C = ra_->C;

  if (generate_vzeroupper(C)) {
    // Clear upper bits of YMM registers when current compiled code uses
    // wide vectors to avoid AVX <-> SSE transition penalty during call.
    __ vzeroupper();
  }

  int framesize = C->output()->frame_size_in_bytes();
  assert((framesize & (StackAlignmentInBytes-1)) == 0, "frame size not aligned");
  // Remove word for return adr already pushed
  // and RBP
  framesize -= 2*wordSize;

  // Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here

  if (framesize) {
    __ addq(rsp, framesize);
  }

  __ popq(rbp);

  if (StackReservedPages > 0 && C->has_reserved_stack_access()) {
    __ reserved_stack_check();
  }

  if (do_polling() && C->is_method_compilation()) {
    Label dummy_label;
    Label* code_stub = &dummy_label;
    if (!C->output()->in_scratch_emit_size()) {
      C2SafepointPollStub* stub = new (C->comp_arena()) C2SafepointPollStub(__ offset());
      C->output()->add_stub(stub);
      code_stub = &stub->entry();
    }
    __ relocate(relocInfo::poll_return_type);
    __ safepoint_poll(*code_stub, true /* at_return */, true /* in_nmethod */);
  }
}

uint MachEpilogNode::size(PhaseRegAlloc* ra_) const
{
  return MachNode::size(ra_); // too many variables; just compute it
                              // the hard way
}

int MachEpilogNode::reloc() const
{
  return 2; // a large enough number
}

const Pipeline* MachEpilogNode::pipeline() const
{
  return MachNode::pipeline_class();
}

//=============================================================================

enum RC {
  rc_bad,
  rc_int,
  rc_kreg,
  rc_float,
  rc_stack
};

static enum RC rc_class(OptoReg::Name reg)
{
  if( !OptoReg::is_valid(reg)  ) return rc_bad;

  if (OptoReg::is_stack(reg)) return rc_stack;

  VMReg r = OptoReg::as_VMReg(reg);

  if (r->is_Register()) return rc_int;

  if (r->is_KRegister()) return rc_kreg;

  assert(r->is_XMMRegister(), "must be");
  return rc_float;
}

// Next two methods are shared by 32- and 64-bit VM. They are defined in x86.ad.
static void vec_mov_helper(C2_MacroAssembler *masm, int src_lo, int dst_lo,
                          int src_hi, int dst_hi, uint ireg, outputStream* st);

void vec_spill_helper(C2_MacroAssembler *masm, bool is_load,
                     int stack_offset, int reg, uint ireg, outputStream* st);

static void vec_stack_to_stack_helper(C2_MacroAssembler *masm, int src_offset,
                                      int dst_offset, uint ireg, outputStream* st) {
  if (masm) {
    switch (ireg) {
    case Op_VecS:
      __ movq(Address(rsp, -8), rax);
      __ movl(rax, Address(rsp, src_offset));
      __ movl(Address(rsp, dst_offset), rax);
      __ movq(rax, Address(rsp, -8));
      break;
    case Op_VecD:
      __ pushq(Address(rsp, src_offset));
      __ popq (Address(rsp, dst_offset));
      break;
    case Op_VecX:
      __ pushq(Address(rsp, src_offset));
      __ popq (Address(rsp, dst_offset));
      __ pushq(Address(rsp, src_offset+8));
      __ popq (Address(rsp, dst_offset+8));
      break;
    case Op_VecY:
      __ vmovdqu(Address(rsp, -32), xmm0);
      __ vmovdqu(xmm0, Address(rsp, src_offset));
      __ vmovdqu(Address(rsp, dst_offset), xmm0);
      __ vmovdqu(xmm0, Address(rsp, -32));
      break;
    case Op_VecZ:
      __ evmovdquq(Address(rsp, -64), xmm0, 2);
      __ evmovdquq(xmm0, Address(rsp, src_offset), 2);
      __ evmovdquq(Address(rsp, dst_offset), xmm0, 2);
      __ evmovdquq(xmm0, Address(rsp, -64), 2);
      break;
    default:
      ShouldNotReachHere();
    }
#ifndef PRODUCT
  } else {
    switch (ireg) {
    case Op_VecS:
      st->print("movq    [rsp - #8], rax\t# 32-bit mem-mem spill\n\t"
                "movl    rax, [rsp + #%d]\n\t"
                "movl    [rsp + #%d], rax\n\t"
                "movq    rax, [rsp - #8]",
                src_offset, dst_offset);
      break;
    case Op_VecD:
      st->print("pushq   [rsp + #%d]\t# 64-bit mem-mem spill\n\t"
                "popq    [rsp + #%d]",
                src_offset, dst_offset);
      break;
     case Op_VecX:
      st->print("pushq   [rsp + #%d]\t# 128-bit mem-mem spill\n\t"
                "popq    [rsp + #%d]\n\t"
                "pushq   [rsp + #%d]\n\t"
                "popq    [rsp + #%d]",
                src_offset, dst_offset, src_offset+8, dst_offset+8);
      break;
    case Op_VecY:
      st->print("vmovdqu [rsp - #32], xmm0\t# 256-bit mem-mem spill\n\t"
                "vmovdqu xmm0, [rsp + #%d]\n\t"
                "vmovdqu [rsp + #%d], xmm0\n\t"
                "vmovdqu xmm0, [rsp - #32]",
                src_offset, dst_offset);
      break;
    case Op_VecZ:
      st->print("vmovdqu [rsp - #64], xmm0\t# 512-bit mem-mem spill\n\t"
                "vmovdqu xmm0, [rsp + #%d]\n\t"
                "vmovdqu [rsp + #%d], xmm0\n\t"
                "vmovdqu xmm0, [rsp - #64]",
                src_offset, dst_offset);
      break;
    default:
      ShouldNotReachHere();
    }
#endif
  }
}

uint MachSpillCopyNode::implementation(C2_MacroAssembler* masm,
                                       PhaseRegAlloc* ra_,
                                       bool do_size,
                                       outputStream* st) const {
  assert(masm != nullptr || st  != nullptr, "sanity");
  // Get registers to move
  OptoReg::Name src_second = ra_->get_reg_second(in(1));
  OptoReg::Name src_first = ra_->get_reg_first(in(1));
  OptoReg::Name dst_second = ra_->get_reg_second(this);
  OptoReg::Name dst_first = ra_->get_reg_first(this);

  enum RC src_second_rc = rc_class(src_second);
  enum RC src_first_rc = rc_class(src_first);
  enum RC dst_second_rc = rc_class(dst_second);
  enum RC dst_first_rc = rc_class(dst_first);

  assert(OptoReg::is_valid(src_first) && OptoReg::is_valid(dst_first),
         "must move at least 1 register" );

  if (src_first == dst_first && src_second == dst_second) {
    // Self copy, no move
    return 0;
  }
  if (bottom_type()->isa_vect() != nullptr && bottom_type()->isa_vectmask() == nullptr) {
    uint ireg = ideal_reg();
    assert((src_first_rc != rc_int && dst_first_rc != rc_int), "sanity");
    assert((ireg == Op_VecS || ireg == Op_VecD || ireg == Op_VecX || ireg == Op_VecY || ireg == Op_VecZ ), "sanity");
    if( src_first_rc == rc_stack && dst_first_rc == rc_stack ) {
      // mem -> mem
      int src_offset = ra_->reg2offset(src_first);
      int dst_offset = ra_->reg2offset(dst_first);
      vec_stack_to_stack_helper(masm, src_offset, dst_offset, ireg, st);
    } else if (src_first_rc == rc_float && dst_first_rc == rc_float ) {
      vec_mov_helper(masm, src_first, dst_first, src_second, dst_second, ireg, st);
    } else if (src_first_rc == rc_float && dst_first_rc == rc_stack ) {
      int stack_offset = ra_->reg2offset(dst_first);
      vec_spill_helper(masm, false, stack_offset, src_first, ireg, st);
    } else if (src_first_rc == rc_stack && dst_first_rc == rc_float ) {
      int stack_offset = ra_->reg2offset(src_first);
      vec_spill_helper(masm, true,  stack_offset, dst_first, ireg, st);
    } else {
      ShouldNotReachHere();
    }
    return 0;
  }
  if (src_first_rc == rc_stack) {
    // mem ->
    if (dst_first_rc == rc_stack) {
      // mem -> mem
      assert(src_second != dst_first, "overlap");
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        int src_offset = ra_->reg2offset(src_first);
        int dst_offset = ra_->reg2offset(dst_first);
        if (masm) {
          __ pushq(Address(rsp, src_offset));
          __ popq (Address(rsp, dst_offset));
#ifndef PRODUCT
        } else {
          st->print("pushq   [rsp + #%d]\t# 64-bit mem-mem spill\n\t"
                    "popq    [rsp + #%d]",
                     src_offset, dst_offset);
#endif
        }
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        // No pushl/popl, so:
        int src_offset = ra_->reg2offset(src_first);
        int dst_offset = ra_->reg2offset(dst_first);
        if (masm) {
          __ movq(Address(rsp, -8), rax);
          __ movl(rax, Address(rsp, src_offset));
          __ movl(Address(rsp, dst_offset), rax);
          __ movq(rax, Address(rsp, -8));
#ifndef PRODUCT
        } else {
          st->print("movq    [rsp - #8], rax\t# 32-bit mem-mem spill\n\t"
                    "movl    rax, [rsp + #%d]\n\t"
                    "movl    [rsp + #%d], rax\n\t"
                    "movq    rax, [rsp - #8]",
                     src_offset, dst_offset);
#endif
        }
      }
      return 0;
    } else if (dst_first_rc == rc_int) {
      // mem -> gpr
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        int offset = ra_->reg2offset(src_first);
        if (masm) {
          __ movq(as_Register(Matcher::_regEncode[dst_first]), Address(rsp, offset));
#ifndef PRODUCT
        } else {
          st->print("movq    %s, [rsp + #%d]\t# spill",
                     Matcher::regName[dst_first],
                     offset);
#endif
        }
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        int offset = ra_->reg2offset(src_first);
        if (masm) {
          __ movl(as_Register(Matcher::_regEncode[dst_first]), Address(rsp, offset));
#ifndef PRODUCT
        } else {
          st->print("movl    %s, [rsp + #%d]\t# spill",
                     Matcher::regName[dst_first],
                     offset);
#endif
        }
      }
      return 0;
    } else if (dst_first_rc == rc_float) {
      // mem-> xmm
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        int offset = ra_->reg2offset(src_first);
        if (masm) {
          __ movdbl( as_XMMRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));
#ifndef PRODUCT
        } else {
          st->print("%s  %s, [rsp + #%d]\t# spill",
                     UseXmmLoadAndClearUpper ? "movsd " : "movlpd",
                     Matcher::regName[dst_first],
                     offset);
#endif
        }
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        int offset = ra_->reg2offset(src_first);
        if (masm) {
          __ movflt( as_XMMRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));
#ifndef PRODUCT
        } else {
          st->print("movss   %s, [rsp + #%d]\t# spill",
                     Matcher::regName[dst_first],
                     offset);
#endif
        }
      }
      return 0;
    } else if (dst_first_rc == rc_kreg) {
      // mem -> kreg
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        int offset = ra_->reg2offset(src_first);
        if (masm) {
          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));
#ifndef PRODUCT
        } else {
          st->print("kmovq   %s, [rsp + #%d]\t# spill",
                     Matcher::regName[dst_first],
                     offset);
#endif
        }
      }
      return 0;
    }
  } else if (src_first_rc == rc_int) {
    // gpr ->
    if (dst_first_rc == rc_stack) {
      // gpr -> mem
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        int offset = ra_->reg2offset(dst_first);
        if (masm) {
          __ movq(Address(rsp, offset), as_Register(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
          st->print("movq    [rsp + #%d], %s\t# spill",
                     offset,
                     Matcher::regName[src_first]);
#endif
        }
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        int offset = ra_->reg2offset(dst_first);
        if (masm) {
          __ movl(Address(rsp, offset), as_Register(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
          st->print("movl    [rsp + #%d], %s\t# spill",
                     offset,
                     Matcher::regName[src_first]);
#endif
        }
      }
      return 0;
    } else if (dst_first_rc == rc_int) {
      // gpr -> gpr
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        if (masm) {
          __ movq(as_Register(Matcher::_regEncode[dst_first]),
                  as_Register(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
          st->print("movq    %s, %s\t# spill",
                     Matcher::regName[dst_first],
                     Matcher::regName[src_first]);
#endif
        }
        return 0;
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        if (masm) {
          __ movl(as_Register(Matcher::_regEncode[dst_first]),
                  as_Register(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
          st->print("movl    %s, %s\t# spill",
                     Matcher::regName[dst_first],
                     Matcher::regName[src_first]);
#endif
        }
        return 0;
      }
    } else if (dst_first_rc == rc_float) {
      // gpr -> xmm
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        if (masm) {
          __ movdq( as_XMMRegister(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
          st->print("movdq   %s, %s\t# spill",
                     Matcher::regName[dst_first],
                     Matcher::regName[src_first]);
#endif
        }
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        if (masm) {
          __ movdl( as_XMMRegister(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
          st->print("movdl   %s, %s\t# spill",
                     Matcher::regName[dst_first],
                     Matcher::regName[src_first]);
#endif
        }
      }
      return 0;
    } else if (dst_first_rc == rc_kreg) {
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        if (masm) {
          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]));
  #ifndef PRODUCT
        } else {
           st->print("kmovq   %s, %s\t# spill",
                       Matcher::regName[dst_first],
                       Matcher::regName[src_first]);
  #endif
        }
      }
      Unimplemented();
      return 0;
    }
  } else if (src_first_rc == rc_float) {
    // xmm ->
    if (dst_first_rc == rc_stack) {
      // xmm -> mem
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        int offset = ra_->reg2offset(dst_first);
        if (masm) {
          __ movdbl( Address(rsp, offset), as_XMMRegister(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
          st->print("movsd   [rsp + #%d], %s\t# spill",
                     offset,
                     Matcher::regName[src_first]);
#endif
        }
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        int offset = ra_->reg2offset(dst_first);
        if (masm) {
          __ movflt(Address(rsp, offset), as_XMMRegister(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
          st->print("movss   [rsp + #%d], %s\t# spill",
                     offset,
                     Matcher::regName[src_first]);
#endif
        }
      }
      return 0;
    } else if (dst_first_rc == rc_int) {
      // xmm -> gpr
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        if (masm) {
          __ movdq( as_Register(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
          st->print("movdq   %s, %s\t# spill",
                     Matcher::regName[dst_first],
                     Matcher::regName[src_first]);
#endif
        }
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        if (masm) {
          __ movdl( as_Register(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
          st->print("movdl   %s, %s\t# spill",
                     Matcher::regName[dst_first],
                     Matcher::regName[src_first]);
#endif
        }
      }
      return 0;
    } else if (dst_first_rc == rc_float) {
      // xmm -> xmm
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        if (masm) {
          __ movdbl( as_XMMRegister(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
          st->print("%s  %s, %s\t# spill",
                     UseXmmRegToRegMoveAll ? "movapd" : "movsd ",
                     Matcher::regName[dst_first],
                     Matcher::regName[src_first]);
#endif
        }
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        if (masm) {
          __ movflt( as_XMMRegister(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
          st->print("%s  %s, %s\t# spill",
                     UseXmmRegToRegMoveAll ? "movaps" : "movss ",
                     Matcher::regName[dst_first],
                     Matcher::regName[src_first]);
#endif
        }
      }
      return 0;
    } else if (dst_first_rc == rc_kreg) {
      assert(false, "Illegal spilling");
      return 0;
    }
  } else if (src_first_rc == rc_kreg) {
    if (dst_first_rc == rc_stack) {
      // mem -> kreg
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        int offset = ra_->reg2offset(dst_first);
        if (masm) {
          __ kmov(Address(rsp, offset), as_KRegister(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
          st->print("kmovq   [rsp + #%d] , %s\t# spill",
                     offset,
                     Matcher::regName[src_first]);
#endif
        }
      }
      return 0;
    } else if (dst_first_rc == rc_int) {
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        if (masm) {
          __ kmov(as_Register(Matcher::_regEncode[dst_first]), as_KRegister(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
         st->print("kmovq   %s, %s\t# spill",
                     Matcher::regName[dst_first],
                     Matcher::regName[src_first]);
#endif
        }
      }
      Unimplemented();
      return 0;
    } else if (dst_first_rc == rc_kreg) {
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        if (masm) {
          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), as_KRegister(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
         st->print("kmovq   %s, %s\t# spill",
                     Matcher::regName[dst_first],
                     Matcher::regName[src_first]);
#endif
        }
      }
      return 0;
    } else if (dst_first_rc == rc_float) {
      assert(false, "Illegal spill");
      return 0;
    }
  }

  assert(0," foo ");
  Unimplemented();
  return 0;
}

#ifndef PRODUCT
void MachSpillCopyNode::format(PhaseRegAlloc *ra_, outputStream* st) const {
  implementation(nullptr, ra_, false, st);
}
#endif

void MachSpillCopyNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {
  implementation(masm, ra_, false, nullptr);
}

uint MachSpillCopyNode::size(PhaseRegAlloc *ra_) const {
  return MachNode::size(ra_);
}

//=============================================================================
#ifndef PRODUCT
void BoxLockNode::format(PhaseRegAlloc* ra_, outputStream* st) const
{
  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());
  int reg = ra_->get_reg_first(this);
  st->print("leaq    %s, [rsp + #%d]\t# box lock",
            Matcher::regName[reg], offset);
}
#endif

void BoxLockNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const
{
  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());
  int reg = ra_->get_encode(this);

  __ lea(as_Register(reg), Address(rsp, offset));
}

uint BoxLockNode::size(PhaseRegAlloc *ra_) const
{
  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());
  if (ra_->get_encode(this) > 15) {
    return (offset < 0x80) ? 6 : 9; // REX2
  } else {
    return (offset < 0x80) ? 5 : 8; // REX
  }
}

//=============================================================================
#ifndef PRODUCT
void MachUEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const
{
  if (UseCompressedClassPointers) {
    st->print_cr("movl    rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\t# compressed klass");
    st->print_cr("\tcmpl    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\t # Inline cache check");
  } else {
    st->print_cr("movq    rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\t# compressed klass");
    st->print_cr("\tcmpq    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\t # Inline cache check");
  }
  st->print_cr("\tjne     SharedRuntime::_ic_miss_stub");
}
#endif

void MachUEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const
{
  __ ic_check(InteriorEntryAlignment);
}

uint MachUEPNode::size(PhaseRegAlloc* ra_) const
{
  return MachNode::size(ra_); // too many variables; just compute it
                              // the hard way
}


//=============================================================================

bool Matcher::supports_vector_calling_convention(void) {
  return EnableVectorSupport;
}

OptoRegPair Matcher::vector_return_value(uint ideal_reg) {
  assert(EnableVectorSupport, "sanity");
  int lo = XMM0_num;
  int hi = XMM0b_num;
  if (ideal_reg == Op_VecX) hi = XMM0d_num;
  else if (ideal_reg == Op_VecY) hi = XMM0h_num;
  else if (ideal_reg == Op_VecZ) hi = XMM0p_num;
  return OptoRegPair(hi, lo);
}

// Is this branch offset short enough that a short branch can be used?
//
// NOTE: If the platform does not provide any short branch variants, then
//       this method should return false for offset 0.
bool Matcher::is_short_branch_offset(int rule, int br_size, int offset) {
  // The passed offset is relative to address of the branch.
  // On 86 a branch displacement is calculated relative to address
  // of a next instruction.
  offset -= br_size;

  // the short version of jmpConUCF2 contains multiple branches,
  // making the reach slightly less
  if (rule == jmpConUCF2_rule)
    return (-126 <= offset && offset <= 125);
  return (-128 <= offset && offset <= 127);
}

// Return whether or not this register is ever used as an argument.
// This function is used on startup to build the trampoline stubs in
// generateOptoStub.  Registers not mentioned will be killed by the VM
// call in the trampoline, and arguments in those registers not be
// available to the callee.
bool Matcher::can_be_java_arg(int reg)
{
  return
    reg ==  RDI_num || reg == RDI_H_num ||
    reg ==  RSI_num || reg == RSI_H_num ||
    reg ==  RDX_num || reg == RDX_H_num ||
    reg ==  RCX_num || reg == RCX_H_num ||
    reg ==   R8_num || reg ==  R8_H_num ||
    reg ==   R9_num || reg ==  R9_H_num ||
    reg ==  R12_num || reg == R12_H_num ||
    reg == XMM0_num || reg == XMM0b_num ||
    reg == XMM1_num || reg == XMM1b_num ||
    reg == XMM2_num || reg == XMM2b_num ||
    reg == XMM3_num || reg == XMM3b_num ||
    reg == XMM4_num || reg == XMM4b_num ||
    reg == XMM5_num || reg == XMM5b_num ||
    reg == XMM6_num || reg == XMM6b_num ||
    reg == XMM7_num || reg == XMM7b_num;
}

bool Matcher::is_spillable_arg(int reg)
{
  return can_be_java_arg(reg);
}

uint Matcher::int_pressure_limit()
{
  return (INTPRESSURE == -1) ? _INT_REG_mask.size() : INTPRESSURE;
}

uint Matcher::float_pressure_limit()
{
  // After experiment around with different values, the following default threshold
  // works best for LCM's register pressure scheduling on x64.
  uint dec_count  = VM_Version::supports_evex() ? 4 : 2;
  uint default_float_pressure_threshold = _FLOAT_REG_mask.size() - dec_count;
  return (FLOATPRESSURE == -1) ? default_float_pressure_threshold : FLOATPRESSURE;
}

bool Matcher::use_asm_for_ldiv_by_con( jlong divisor ) {
  // In 64 bit mode a code which use multiply when
  // devisor is constant is faster than hardware
  // DIV instruction (it uses MulHiL).
  return false;
}

// Register for DIVI projection of divmodI
const RegMask& Matcher::divI_proj_mask() {
  return INT_RAX_REG_mask();
}

// Register for MODI projection of divmodI
const RegMask& Matcher::modI_proj_mask() {
  return INT_RDX_REG_mask();
}

// Register for DIVL projection of divmodL
const RegMask& Matcher::divL_proj_mask() {
  return LONG_RAX_REG_mask();
}

// Register for MODL projection of divmodL
const RegMask& Matcher::modL_proj_mask() {
  return LONG_RDX_REG_mask();
}

%}

source_hpp %{
// Header information of the source block.
// Method declarations/definitions which are used outside
// the ad-scope can conveniently be defined here.
//
// To keep related declarations/definitions/uses close together,
// we switch between source %{ }% and source_hpp %{ }% freely as needed.

#include "runtime/vm_version.hpp"

class NativeJump;

class CallStubImpl {

  //--------------------------------------------------------------
  //---<  Used for optimization in Compile::shorten_branches  >---
  //--------------------------------------------------------------

 public:
  // Size of call trampoline stub.
  static uint size_call_trampoline() {
    return 0; // no call trampolines on this platform
  }

  // number of relocations needed by a call trampoline stub
  static uint reloc_call_trampoline() {
    return 0; // no call trampolines on this platform
  }
};

class HandlerImpl {

 public:

  static int emit_exception_handler(C2_MacroAssembler *masm);
  static int emit_deopt_handler(C2_MacroAssembler* masm);

  static uint size_exception_handler() {
    // NativeCall instruction size is the same as NativeJump.
    // exception handler starts out as jump and can be patched to
    // a call be deoptimization.  (4932387)
    // Note that this value is also credited (in output.cpp) to
    // the size of the code section.
    return NativeJump::instruction_size;
  }

  static uint size_deopt_handler() {
    // three 5 byte instructions plus one move for unreachable address.
    return 15+3;
  }
};

inline Assembler::AvxVectorLen vector_length_encoding(int bytes) {
  switch(bytes) {
    case  4: // fall-through
    case  8: // fall-through
    case 16: return Assembler::AVX_128bit;
    case 32: return Assembler::AVX_256bit;
    case 64: return Assembler::AVX_512bit;

    default: {
      ShouldNotReachHere();
      return Assembler::AVX_NoVec;
    }
  }
}

static inline Assembler::AvxVectorLen vector_length_encoding(const Node* n) {
  return vector_length_encoding(Matcher::vector_length_in_bytes(n));
}

static inline Assembler::AvxVectorLen vector_length_encoding(const MachNode* use, MachOper* opnd) {
  uint def_idx = use->operand_index(opnd);
  Node* def = use->in(def_idx);
  return vector_length_encoding(def);
}

static inline bool is_vector_popcount_predicate(BasicType bt) {
  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||
         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());
}

static inline bool is_clz_non_subword_predicate_evex(BasicType bt, int vlen_bytes) {
  return is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd() &&
           (VM_Version::supports_avx512vl() || vlen_bytes == 64);
}

class Node::PD {
public:
  enum NodeFlags {
    Flag_intel_jcc_erratum    = Node::_last_flag << 1,
    Flag_sets_carry_flag      = Node::_last_flag << 2,
    Flag_sets_parity_flag     = Node::_last_flag << 3,
    Flag_sets_zero_flag       = Node::_last_flag << 4,
    Flag_sets_overflow_flag   = Node::_last_flag << 5,
    Flag_sets_sign_flag       = Node::_last_flag << 6,
    Flag_clears_carry_flag    = Node::_last_flag << 7,
    Flag_clears_parity_flag   = Node::_last_flag << 8,
    Flag_clears_zero_flag     = Node::_last_flag << 9,
    Flag_clears_overflow_flag = Node::_last_flag << 10,
    Flag_clears_sign_flag     = Node::_last_flag << 11,
    _last_flag                = Flag_clears_sign_flag
  };
};

%} // end source_hpp

source %{

#include "opto/addnode.hpp"
#include "c2_intelJccErratum_x86.hpp"

void PhaseOutput::pd_perform_mach_node_analysis() {
  if (VM_Version::has_intel_jcc_erratum()) {
    int extra_padding = IntelJccErratum::tag_affected_machnodes(C, C->cfg(), C->regalloc());
    _buf_sizes._code += extra_padding;
  }
}

int MachNode::pd_alignment_required() const {
  if (VM_Version::has_intel_jcc_erratum() && IntelJccErratum::is_jcc_erratum_branch(this)) {
    // Conservatively add worst case padding. We assume that relocInfo::addr_unit() is 1 on x86.
    return IntelJccErratum::largest_jcc_size() + 1;
  } else {
    return 1;
  }
}

int MachNode::compute_padding(int current_offset) const {
  if (flags() & Node::PD::Flag_intel_jcc_erratum) {
    Compile* C = Compile::current();
    PhaseOutput* output = C->output();
    Block* block = output->block();
    int index = output->index();
    return IntelJccErratum::compute_padding(current_offset, this, block, index, C->regalloc());
  } else {
    return 0;
  }
}

// Emit exception handler code.
// Stuff framesize into a register and call a VM stub routine.
int HandlerImpl::emit_exception_handler(C2_MacroAssembler* masm) {

  // Note that the code buffer's insts_mark is always relative to insts.
  // That's why we must use the macroassembler to generate a handler.
  address base = __ start_a_stub(size_exception_handler());
  if (base == nullptr) {
    ciEnv::current()->record_failure("CodeCache is full");
    return 0;  // CodeBuffer::expand failed
  }
  int offset = __ offset();
  __ jump(RuntimeAddress(OptoRuntime::exception_blob()->entry_point()));
  assert(__ offset() - offset <= (int) size_exception_handler(), "overflow");
  __ end_a_stub();
  return offset;
}

// Emit deopt handler code.
int HandlerImpl::emit_deopt_handler(C2_MacroAssembler* masm) {

  // Note that the code buffer's insts_mark is always relative to insts.
  // That's why we must use the macroassembler to generate a handler.
  address base = __ start_a_stub(size_deopt_handler());
  if (base == nullptr) {
    ciEnv::current()->record_failure("CodeCache is full");
    return 0;  // CodeBuffer::expand failed
  }
  int offset = __ offset();

  address the_pc = (address) __ pc();
  Label next;
  // push a "the_pc" on the stack without destroying any registers
  // as they all may be live.

  // push address of "next"
  __ call(next, relocInfo::none); // reloc none is fine since it is a disp32
  __ bind(next);
  // adjust it so it matches "the_pc"
  __ subptr(Address(rsp, 0), __ offset() - offset);

  __ jump(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));
  assert(__ offset() - offset <= (int) size_deopt_handler(), "overflow %d", (__ offset() - offset));
  __ end_a_stub();
  return offset;
}

static Assembler::Width widthForType(BasicType bt) {
  if (bt == T_BYTE) {
    return Assembler::B;
  } else if (bt == T_SHORT) {
    return Assembler::W;
  } else if (bt == T_INT) {
    return Assembler::D;
  } else {
    assert(bt == T_LONG, "not a long: %s", type2name(bt));
    return Assembler::Q;
  }
}

//=============================================================================

  // Float masks come from different places depending on platform.
  static address float_signmask()  { return StubRoutines::x86::float_sign_mask(); }
  static address float_signflip()  { return StubRoutines::x86::float_sign_flip(); }
  static address double_signmask() { return StubRoutines::x86::double_sign_mask(); }
  static address double_signflip() { return StubRoutines::x86::double_sign_flip(); }
  static address vector_short_to_byte_mask() { return StubRoutines::x86::vector_short_to_byte_mask(); }
  static address vector_int_to_byte_mask() { return StubRoutines::x86::vector_int_to_byte_mask(); }
  static address vector_byte_perm_mask() { return StubRoutines::x86::vector_byte_perm_mask(); }
  static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }
  static address vector_all_bits_set() { return StubRoutines::x86::vector_all_bits_set(); }
  static address vector_int_mask_cmp_bits() { return StubRoutines::x86::vector_int_mask_cmp_bits(); }
  static address vector_int_to_short_mask() { return StubRoutines::x86::vector_int_to_short_mask(); }
  static address vector_byte_shufflemask() { return StubRoutines::x86::vector_byte_shuffle_mask(); }
  static address vector_short_shufflemask() { return StubRoutines::x86::vector_short_shuffle_mask(); }
  static address vector_int_shufflemask() { return StubRoutines::x86::vector_int_shuffle_mask(); }
  static address vector_long_shufflemask() { return StubRoutines::x86::vector_long_shuffle_mask(); }
  static address vector_32_bit_mask() { return StubRoutines::x86::vector_32_bit_mask(); }
  static address vector_64_bit_mask() { return StubRoutines::x86::vector_64_bit_mask(); }
  static address vector_float_signflip() { return StubRoutines::x86::vector_float_sign_flip();}
  static address vector_double_signflip() { return StubRoutines::x86::vector_double_sign_flip();}

//=============================================================================
bool Matcher::match_rule_supported(int opcode) {
  if (!has_match_rule(opcode)) {
    return false; // no match rule present
  }
  switch (opcode) {
    case Op_AbsVL:
    case Op_StoreVectorScatter:
      if (UseAVX < 3) {
        return false;
      }
      break;
    case Op_PopCountI:
    case Op_PopCountL:
      if (!UsePopCountInstruction) {
        return false;
      }
      break;
    case Op_PopCountVI:
      if (UseAVX < 2) {
        return false;
      }
      break;
    case Op_CompressV:
    case Op_ExpandV:
    case Op_PopCountVL:
      if (UseAVX < 2) {
        return false;
      }
      break;
    case Op_MulVI:
      if ((UseSSE < 4) && (UseAVX < 1)) { // only with SSE4_1 or AVX
        return false;
      }
      break;
    case Op_MulVL:
      if (UseSSE < 4) { // only with SSE4_1 or AVX
        return false;
      }
      break;
    case Op_MulReductionVL:
      if (VM_Version::supports_avx512dq() == false) {
        return false;
      }
      break;
    case Op_AbsVB:
    case Op_AbsVS:
    case Op_AbsVI:
    case Op_AddReductionVI:
    case Op_AndReductionV:
    case Op_OrReductionV:
    case Op_XorReductionV:
      if (UseSSE < 3) { // requires at least SSSE3
        return false;
      }
      break;
    case Op_MaxHF:
    case Op_MinHF:
      if (!VM_Version::supports_avx512vlbw()) {
        return false;
      }  // fallthrough
    case Op_AddHF:
    case Op_DivHF:
    case Op_FmaHF:
    case Op_MulHF:
    case Op_ReinterpretS2HF:
    case Op_ReinterpretHF2S:
    case Op_SubHF:
    case Op_SqrtHF:
      if (!VM_Version::supports_avx512_fp16()) {
        return false;
      }
      break;
    case Op_VectorLoadShuffle:
    case Op_VectorRearrange:
    case Op_MulReductionVI:
      if (UseSSE < 4) { // requires at least SSE4
        return false;
      }
      break;
    case Op_IsInfiniteF:
    case Op_IsInfiniteD:
      if (!VM_Version::supports_avx512dq()) {
        return false;
      }
      break;
    case Op_SqrtVD:
    case Op_SqrtVF:
    case Op_VectorMaskCmp:
    case Op_VectorCastB2X:
    case Op_VectorCastS2X:
    case Op_VectorCastI2X:
    case Op_VectorCastL2X:
    case Op_VectorCastF2X:
    case Op_VectorCastD2X:
    case Op_VectorUCastB2X:
    case Op_VectorUCastS2X:
    case Op_VectorUCastI2X:
    case Op_VectorMaskCast:
      if (UseAVX < 1) { // enabled for AVX only
        return false;
      }
      break;
    case Op_PopulateIndex:
      if (UseAVX < 2) {
        return false;
      }
      break;
    case Op_RoundVF:
      if (UseAVX < 2) { // enabled for AVX2 only
        return false;
      }
      break;
    case Op_RoundVD:
      if (UseAVX < 3) {
        return false;  // enabled for AVX3 only
      }
      break;
    case Op_CompareAndSwapL:
    case Op_CompareAndSwapP:
      break;
    case Op_StrIndexOf:
      if (!UseSSE42Intrinsics) {
        return false;
      }
      break;
    case Op_StrIndexOfChar:
      if (!UseSSE42Intrinsics) {
        return false;
      }
      break;
    case Op_OnSpinWait:
      if (VM_Version::supports_on_spin_wait() == false) {
        return false;
      }
      break;
    case Op_MulVB:
    case Op_LShiftVB:
    case Op_RShiftVB:
    case Op_URShiftVB:
    case Op_VectorInsert:
    case Op_VectorLoadMask:
    case Op_VectorStoreMask:
    case Op_VectorBlend:
      if (UseSSE < 4) {
        return false;
      }
      break;
    case Op_MaxD:
    case Op_MaxF:
    case Op_MinD:
    case Op_MinF:
      if (UseAVX < 1) { // enabled for AVX only
        return false;
      }
      break;
    case Op_CacheWB:
    case Op_CacheWBPreSync:
    case Op_CacheWBPostSync:
      if (!VM_Version::supports_data_cache_line_flush()) {
        return false;
      }
      break;
    case Op_ExtractB:
    case Op_ExtractL:
    case Op_ExtractI:
    case Op_RoundDoubleMode:
      if (UseSSE < 4) {
        return false;
      }
      break;
    case Op_RoundDoubleModeV:
      if (VM_Version::supports_avx() == false) {
        return false; // 128bit vroundpd is not available
      }
      break;
    case Op_LoadVectorGather:
    case Op_LoadVectorGatherMasked:
      if (UseAVX < 2) {
        return false;
      }
      break;
    case Op_FmaF:
    case Op_FmaD:
    case Op_FmaVD:
    case Op_FmaVF:
      if (!UseFMA) {
        return false;
      }
      break;
    case Op_MacroLogicV:
      if (UseAVX < 3 || !UseVectorMacroLogic) {
        return false;
      }
      break;

    case Op_VectorCmpMasked:
    case Op_VectorMaskGen:
      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {
        return false;
      }
      break;
    case Op_VectorMaskFirstTrue:
    case Op_VectorMaskLastTrue:
    case Op_VectorMaskTrueCount:
    case Op_VectorMaskToLong:
      if (UseAVX < 1) {
         return false;
      }
      break;
    case Op_RoundF:
    case Op_RoundD:
      break;
    case Op_CopySignD:
    case Op_CopySignF:
      if (UseAVX < 3)  {
        return false;
      }
      if (!VM_Version::supports_avx512vl()) {
        return false;
      }
      break;
    case Op_CompressBits:
    case Op_ExpandBits:
      if (!VM_Version::supports_bmi2()) {
        return false;
      }
      break;
    case Op_CompressM:
      if (!VM_Version::supports_avx512vl() || !VM_Version::supports_bmi2()) {
        return false;
      }
      break;
    case Op_ConvF2HF:
    case Op_ConvHF2F:
      if (!VM_Version::supports_float16()) {
        return false;
      }
      break;
    case Op_VectorCastF2HF:
    case Op_VectorCastHF2F:
      if (!VM_Version::supports_f16c() && !VM_Version::supports_evex()) {
        return false;
      }
      break;
  }
  return true;  // Match rules are supported by default.
}

//------------------------------------------------------------------------

static inline bool is_pop_count_instr_target(BasicType bt) {
  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||
         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());
}

bool Matcher::match_rule_supported_auto_vectorization(int opcode, int vlen, BasicType bt) {
  return match_rule_supported_vector(opcode, vlen, bt);
}

// Identify extra cases that we might want to provide match rules for vector nodes and
// other intrinsics guarded with vector length (vlen) and element type (bt).
bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {
  if (!match_rule_supported(opcode)) {
    return false;
  }
  // Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):
  //   * SSE2 supports 128bit vectors for all types;
  //   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;
  //   * AVX2 supports 256bit vectors for all types;
  //   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;
  //   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.
  // There's also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).
  // And MaxVectorSize is taken into account as well.
  if (!vector_size_supported(bt, vlen)) {
    return false;
  }
  // Special cases which require vector length follow:
  //   * implementation limitations
  //   * some 512bit vector operations on FLOAT and DOUBLE types require AVX512DQ
  //   * 128bit vroundpd instruction is present only in AVX1
  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;
  switch (opcode) {
    case Op_MaxVHF:
    case Op_MinVHF:
      if (!VM_Version::supports_avx512bw()) {
        return false;
      }
    case Op_AddVHF:
    case Op_DivVHF:
    case Op_FmaVHF:
    case Op_MulVHF:
    case Op_SubVHF:
    case Op_SqrtVHF:
      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {
        return false;
      }
      if (!VM_Version::supports_avx512_fp16()) {
        return false;
      }
      break;
    case Op_AbsVF:
    case Op_NegVF:
      if ((vlen == 16) && (VM_Version::supports_avx512dq() == false)) {
        return false; // 512bit vandps and vxorps are not available
      }
      break;
    case Op_AbsVD:
    case Op_NegVD:
      if ((vlen == 8) && (VM_Version::supports_avx512dq() == false)) {
        return false; // 512bit vpmullq, vandpd and vxorpd are not available
      }
      break;
    case Op_RotateRightV:
    case Op_RotateLeftV:
      if (bt != T_INT && bt != T_LONG) {
        return false;
      } // fallthrough
    case Op_MacroLogicV:
      if (!VM_Version::supports_evex() ||
          ((size_in_bits != 512) && !VM_Version::supports_avx512vl())) {
        return false;
      }
      break;
    case Op_ClearArray:
    case Op_VectorMaskGen:
    case Op_VectorCmpMasked:
      if (!VM_Version::supports_avx512bw()) {
        return false;
      }
      if ((size_in_bits != 512) && !VM_Version::supports_avx512vl()) {
        return false;
      }
      break;
    case Op_LoadVectorMasked:
    case Op_StoreVectorMasked:
      if (!VM_Version::supports_avx512bw() && (is_subword_type(bt) || UseAVX < 1)) {
        return false;
      }
      break;
    case Op_UMinV:
    case Op_UMaxV:
      if (UseAVX == 0) {
        return false;
      }
      break;
    case Op_MaxV:
    case Op_MinV:
      if (UseSSE < 4 && is_integral_type(bt)) {
        return false;
      }
      if ((bt == T_FLOAT || bt == T_DOUBLE)) {
          // Float/Double intrinsics are enabled for AVX family currently.
          if (UseAVX == 0) {
            return false;
          }
          if (UseAVX > 2 && (!VM_Version::supports_avx512dq() && size_in_bits == 512)) { // 512 bit Float/Double intrinsics need AVX512DQ
            return false;
          }
      }
      break;
    case Op_CallLeafVector:
      if (size_in_bits == 512 && !VM_Version::supports_avx512vlbwdq()) {
        return false;
      }
      break;
    case Op_AddReductionVI:
      if (bt == T_INT && (UseSSE < 3 || !VM_Version::supports_ssse3())) {
        return false;
      }
      // fallthrough
    case Op_AndReductionV:
    case Op_OrReductionV:
    case Op_XorReductionV:
      if (is_subword_type(bt) && (UseSSE < 4)) {
        return false;
      }
      break;
    case Op_MinReductionV:
    case Op_MaxReductionV:
      if ((bt == T_INT || is_subword_type(bt)) && UseSSE < 4) {
        return false;
      } else if (bt == T_LONG && (UseAVX < 3 || !VM_Version::supports_avx512vlbwdq())) {
        return false;
      }
      // Float/Double intrinsics enabled for AVX family.
      if (UseAVX == 0 && (bt == T_FLOAT || bt == T_DOUBLE)) {
        return false;
      }
      if (UseAVX > 2 && (!VM_Version::supports_avx512dq() && size_in_bits == 512)) {
        return false;
      }
      break;
    case Op_VectorTest:
      if (UseSSE < 4) {
        return false; // Implementation limitation
      } else if (size_in_bits < 32) {
        return false; // Implementation limitation
      }
      break;
    case Op_VectorLoadShuffle:
    case Op_VectorRearrange:
      if(vlen == 2) {
        return false; // Implementation limitation due to how shuffle is loaded
      } else if (size_in_bits == 256 && UseAVX < 2) {
        return false; // Implementation limitation
      }
      break;
    case Op_VectorLoadMask:
    case Op_VectorMaskCast:
      if (size_in_bits == 256 && UseAVX < 2) {
        return false; // Implementation limitation
      }
      // fallthrough
    case Op_VectorStoreMask:
      if (vlen == 2) {
        return false; // Implementation limitation
      }
      break;
    case Op_PopulateIndex:
      if (size_in_bits > 256 && !VM_Version::supports_avx512bw()) {
        return false;
      }
      break;
    case Op_VectorCastB2X:
    case Op_VectorCastS2X:
    case Op_VectorCastI2X:
      if (bt != T_DOUBLE && size_in_bits == 256 && UseAVX < 2) {
        return false;
      }
      break;
    case Op_VectorCastL2X:
      if (is_integral_type(bt) && size_in_bits == 256 && UseAVX < 2) {
        return false;
      } else if (!is_integral_type(bt) && !VM_Version::supports_avx512dq()) {
        return false;
      }
      break;
    case Op_VectorCastF2X: {
        // As per JLS section 5.1.3 narrowing conversion to sub-word types
        // happen after intermediate conversion to integer and special handling
        // code needs AVX2 vpcmpeqd instruction for 256 bit vectors.
        int src_size_in_bits = type2aelembytes(T_FLOAT) * vlen * BitsPerByte;
        if (is_integral_type(bt) && src_size_in_bits == 256 && UseAVX < 2) {
          return false;
        }
      }
      // fallthrough
    case Op_VectorCastD2X:
      if (bt == T_LONG && !VM_Version::supports_avx512dq()) {
        return false;
      }
      break;
    case Op_VectorCastF2HF:
    case Op_VectorCastHF2F:
      if (!VM_Version::supports_f16c() &&
         ((!VM_Version::supports_evex() ||
         ((size_in_bits != 512) && !VM_Version::supports_avx512vl())))) {
        return false;
      }
      break;
    case Op_RoundVD:
      if (!VM_Version::supports_avx512dq()) {
        return false;
      }
      break;
    case Op_MulReductionVI:
      if (bt == T_BYTE && size_in_bits == 512 && !VM_Version::supports_avx512bw()) {
        return false;
      }
      break;
    case Op_LoadVectorGatherMasked:
      if (!is_subword_type(bt) && size_in_bits < 512 && !VM_Version::supports_avx512vl()) {
        return false;
      }
      if (is_subword_type(bt) &&
         ((size_in_bits > 256 && !VM_Version::supports_avx512bw()) ||
          (size_in_bits < 64)                                      ||
          (bt == T_SHORT && !VM_Version::supports_bmi2()))) {
        return false;
      }
      break;
    case Op_StoreVectorScatterMasked:
    case Op_StoreVectorScatter:
      if (is_subword_type(bt)) {
        return false;
      } else if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {
        return false;
      }
      // fallthrough
    case Op_LoadVectorGather:
      if (!is_subword_type(bt) && size_in_bits == 64) {
        return false;
      }
      if (is_subword_type(bt) && size_in_bits < 64) {
        return false;
      }
      break;
    case Op_SaturatingAddV:
    case Op_SaturatingSubV:
      if (UseAVX < 1) {
        return false; // Implementation limitation
      }
      if (is_subword_type(bt) && size_in_bits == 512 && !VM_Version::supports_avx512bw()) {
        return false;
      }
      break;
    case Op_SelectFromTwoVector:
       if (size_in_bits < 128) {
         return false;
       }
       if ((size_in_bits < 512 && !VM_Version::supports_avx512vl())) {
         return false;
       }
       if (bt == T_SHORT && !VM_Version::supports_avx512bw()) {
         return false;
       }
       if (bt == T_BYTE && !VM_Version::supports_avx512_vbmi()) {
         return false;
       }
       if ((bt == T_INT || bt == T_FLOAT || bt == T_DOUBLE) && !VM_Version::supports_evex()) {
         return false;
       }
       break;
    case Op_MaskAll:
      if (!VM_Version::supports_evex()) {
        return false;
      }
      if ((vlen > 16 || is_subword_type(bt)) && !VM_Version::supports_avx512bw()) {
        return false;
      }
      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {
        return false;
      }
      break;
    case Op_VectorMaskCmp:
      if (vlen < 2 || size_in_bits < 32) {
        return false;
      }
      break;
    case Op_CompressM:
      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {
        return false;
      }
      break;
    case Op_CompressV:
    case Op_ExpandV:
      if (is_subword_type(bt) && !VM_Version::supports_avx512_vbmi2()) {
        return false;
      }
      if (size_in_bits < 128 ) {
        return false;
      }
    case Op_VectorLongToMask:
      if (UseAVX < 1) {
        return false;
      }
      if (UseAVX < 3 && !VM_Version::supports_bmi2()) {
        return false;
      }
      break;
    case Op_SignumVD:
    case Op_SignumVF:
      if (UseAVX < 1) {
        return false;
      }
      break;
    case Op_PopCountVI:
    case Op_PopCountVL: {
        if (!is_pop_count_instr_target(bt) &&
            (size_in_bits == 512) && !VM_Version::supports_avx512bw()) {
          return false;
        }
      }
      break;
    case Op_ReverseV:
    case Op_ReverseBytesV:
      if (UseAVX < 2) {
        return false;
      }
      break;
    case Op_CountTrailingZerosV:
    case Op_CountLeadingZerosV:
      if (UseAVX < 2) {
        return false;
      }
      break;
  }
  return true;  // Per default match rules are supported.
}

bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {
  // ADLC based match_rule_supported routine checks for the existence of pattern based
  // on IR opcode. Most of the unary/binary/ternary masked operation share the IR nodes
  // of their non-masked counterpart with mask edge being the differentiator.
  // This routine does a strict check on the existence of masked operation patterns
  // by returning a default false value for all the other opcodes apart from the
  // ones whose masked instruction patterns are defined in this file.
  if (!match_rule_supported_vector(opcode, vlen, bt)) {
    return false;
  }

  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;
  if (size_in_bits != 512 && !VM_Version::supports_avx512vl()) {
    return false;
  }
  switch(opcode) {
    // Unary masked operations
    case Op_AbsVB:
    case Op_AbsVS:
      if(!VM_Version::supports_avx512bw()) {
        return false;  // Implementation limitation
      }
    case Op_AbsVI:
    case Op_AbsVL:
      return true;

    // Ternary masked operations
    case Op_FmaVF:
    case Op_FmaVD:
      return true;

    case Op_MacroLogicV:
      if(bt != T_INT && bt != T_LONG) {
        return false;
      }
      return true;

    // Binary masked operations
    case Op_AddVB:
    case Op_AddVS:
    case Op_SubVB:
    case Op_SubVS:
    case Op_MulVS:
    case Op_LShiftVS:
    case Op_RShiftVS:
    case Op_URShiftVS:
      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), "");
      if (!VM_Version::supports_avx512bw()) {
        return false;  // Implementation limitation
      }
      return true;

    case Op_MulVL:
      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), "");
      if (!VM_Version::supports_avx512dq()) {
        return false;  // Implementation limitation
      }
      return true;

    case Op_AndV:
    case Op_OrV:
    case Op_XorV:
    case Op_RotateRightV:
    case Op_RotateLeftV:
      if (bt != T_INT && bt != T_LONG) {
        return false; // Implementation limitation
      }
      return true;

    case Op_VectorLoadMask:
      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), "");
      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {
        return false;
      }
      return true;

    case Op_AddVI:
    case Op_AddVL:
    case Op_AddVF:
    case Op_AddVD:
    case Op_SubVI:
    case Op_SubVL:
    case Op_SubVF:
    case Op_SubVD:
    case Op_MulVI:
    case Op_MulVF:
    case Op_MulVD:
    case Op_DivVF:
    case Op_DivVD:
    case Op_SqrtVF:
    case Op_SqrtVD:
    case Op_LShiftVI:
    case Op_LShiftVL:
    case Op_RShiftVI:
    case Op_RShiftVL:
    case Op_URShiftVI:
    case Op_URShiftVL:
    case Op_LoadVectorMasked:
    case Op_StoreVectorMasked:
    case Op_LoadVectorGatherMasked:
    case Op_StoreVectorScatterMasked:
      return true;

    case Op_UMinV:
    case Op_UMaxV:
      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {
        return false;
      } // fallthrough
    case Op_MaxV:
    case Op_MinV:
      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {
        return false; // Implementation limitation
      }
      if (is_floating_point_type(bt) && !VM_Version::supports_avx10_2()) {
        return false; // Implementation limitation
      }
      return true;
    case Op_SaturatingAddV:
    case Op_SaturatingSubV:
      if (!is_subword_type(bt)) {
        return false;
      }
      if (size_in_bits < 128 || !VM_Version::supports_avx512bw()) {
        return false; // Implementation limitation
      }
      return true;

    case Op_VectorMaskCmp:
      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {
        return false; // Implementation limitation
      }
      return true;

    case Op_VectorRearrange:
      if (bt == T_SHORT && !VM_Version::supports_avx512bw()) {
        return false; // Implementation limitation
      }
      if (bt == T_BYTE && !VM_Version::supports_avx512_vbmi()) {
        return false; // Implementation limitation
      } else if ((bt == T_INT || bt == T_FLOAT) && size_in_bits < 256) {
        return false; // Implementation limitation
      }
      return true;

    // Binary Logical operations
    case Op_AndVMask:
    case Op_OrVMask:
    case Op_XorVMask:
      if (vlen > 16 && !VM_Version::supports_avx512bw()) {
        return false; // Implementation limitation
      }
      return true;

    case Op_PopCountVI:
    case Op_PopCountVL:
      if (!is_pop_count_instr_target(bt)) {
        return false;
      }
      return true;

    case Op_MaskAll:
      return true;

    case Op_CountLeadingZerosV:
      if (is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd()) {
        return true;
      }
    default:
      return false;
  }
}

bool Matcher::vector_needs_partial_operations(Node* node, const TypeVect* vt) {
  return false;
}

// Return true if Vector::rearrange needs preparation of the shuffle argument
bool Matcher::vector_rearrange_requires_load_shuffle(BasicType elem_bt, int vlen) {
  switch (elem_bt) {
    case T_BYTE:  return false;
    case T_SHORT: return !VM_Version::supports_avx512bw();
    case T_INT:   return !VM_Version::supports_avx();
    case T_LONG:  return vlen < 8 && !VM_Version::supports_avx512vl();
    default:
      ShouldNotReachHere();
      return false;
  }
}

MachOper* Matcher::pd_specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp) {
  assert(Matcher::is_generic_vector(generic_opnd), "not generic");
  bool legacy = (generic_opnd->opcode() == LEGVEC);
  if (!VM_Version::supports_avx512vlbwdq() && // KNL
      is_temp && !legacy && (ideal_reg == Op_VecZ)) {
    // Conservatively specialize 512bit vec TEMP operands to legVecZ (zmm0-15) on KNL.
    return new legVecZOper();
  }
  if (legacy) {
    switch (ideal_reg) {
      case Op_VecS: return new legVecSOper();
      case Op_VecD: return new legVecDOper();
      case Op_VecX: return new legVecXOper();
      case Op_VecY: return new legVecYOper();
      case Op_VecZ: return new legVecZOper();
    }
  } else {
    switch (ideal_reg) {
      case Op_VecS: return new vecSOper();
      case Op_VecD: return new vecDOper();
      case Op_VecX: return new vecXOper();
      case Op_VecY: return new vecYOper();
      case Op_VecZ: return new vecZOper();
    }
  }
  ShouldNotReachHere();
  return nullptr;
}

bool Matcher::is_reg2reg_move(MachNode* m) {
  switch (m->rule()) {
    case MoveVec2Leg_rule:
    case MoveLeg2Vec_rule:
    case MoveF2VL_rule:
    case MoveF2LEG_rule:
    case MoveVL2F_rule:
    case MoveLEG2F_rule:
    case MoveD2VL_rule:
    case MoveD2LEG_rule:
    case MoveVL2D_rule:
    case MoveLEG2D_rule:
      return true;
    default:
      return false;
  }
}

bool Matcher::is_generic_vector(MachOper* opnd) {
  switch (opnd->opcode()) {
    case VEC:
    case LEGVEC:
      return true;
    default:
      return false;
  }
}

//------------------------------------------------------------------------

const RegMask* Matcher::predicate_reg_mask(void) {
  return &_VECTMASK_REG_mask;
}

// Max vector size in bytes. 0 if not supported.
int Matcher::vector_width_in_bytes(BasicType bt) {
  assert(is_java_primitive(bt), "only primitive type vectors");
  // SSE2 supports 128bit vectors for all types.
  // AVX2 supports 256bit vectors for all types.
  // AVX2/EVEX supports 512bit vectors for all types.
  int size = (UseAVX > 1) ? (1 << UseAVX) * 8 : 16;
  // AVX1 supports 256bit vectors only for FLOAT and DOUBLE.
  if (UseAVX > 0 && (bt == T_FLOAT || bt == T_DOUBLE))
    size = (UseAVX > 2) ? 64 : 32;
  if (UseAVX > 2 && (bt == T_BYTE || bt == T_SHORT || bt == T_CHAR))
    size = (VM_Version::supports_avx512bw()) ? 64 : 32;
  // Use flag to limit vector size.
  size = MIN2(size,(int)MaxVectorSize);
  // Minimum 2 values in vector (or 4 for bytes).
  switch (bt) {
  case T_DOUBLE:
  case T_LONG:
    if (size < 16) return 0;
    break;
  case T_FLOAT:
  case T_INT:
    if (size < 8) return 0;
    break;
  case T_BOOLEAN:
    if (size < 4) return 0;
    break;
  case T_CHAR:
    if (size < 4) return 0;
    break;
  case T_BYTE:
    if (size < 4) return 0;
    break;
  case T_SHORT:
    if (size < 4) return 0;
    break;
  default:
    ShouldNotReachHere();
  }
  return size;
}

// Limits on vector size (number of elements) loaded into vector.
int Matcher::max_vector_size(const BasicType bt) {
  return vector_width_in_bytes(bt)/type2aelembytes(bt);
}
int Matcher::min_vector_size(const BasicType bt) {
  int max_size = max_vector_size(bt);
  // Min size which can be loaded into vector is 4 bytes.
  int size = (type2aelembytes(bt) == 1) ? 4 : 2;
  // Support for calling svml double64 vectors
  if (bt == T_DOUBLE) {
    size = 1;
  }
  return MIN2(size,max_size);
}

int Matcher::max_vector_size_auto_vectorization(const BasicType bt) {
  // Limit the max vector size for auto vectorization to 256 bits (32 bytes)
  // by default on Cascade Lake
  if (VM_Version::is_default_intel_cascade_lake()) {
    return MIN2(Matcher::max_vector_size(bt), 32 / type2aelembytes(bt));
  }
  return Matcher::max_vector_size(bt);
}

int Matcher::scalable_vector_reg_size(const BasicType bt) {
  return -1;
}

// Vector ideal reg corresponding to specified size in bytes
uint Matcher::vector_ideal_reg(int size) {
  assert(MaxVectorSize >= size, "");
  switch(size) {
    case  4: return Op_VecS;
    case  8: return Op_VecD;
    case 16: return Op_VecX;
    case 32: return Op_VecY;
    case 64: return Op_VecZ;
  }
  ShouldNotReachHere();
  return 0;
}

// Check for shift by small constant as well
static bool clone_shift(Node* shift, Matcher* matcher, Matcher::MStack& mstack, VectorSet& address_visited) {
  if (shift->Opcode() == Op_LShiftX && shift->in(2)->is_Con() &&
      shift->in(2)->get_int() <= 3 &&
      // Are there other uses besides address expressions?
      !matcher->is_visited(shift)) {
    address_visited.set(shift->_idx); // Flag as address_visited
    mstack.push(shift->in(2), Matcher::Visit);
    Node *conv = shift->in(1);
    // Allow Matcher to match the rule which bypass
    // ConvI2L operation for an array index on LP64
    // if the index value is positive.
    if (conv->Opcode() == Op_ConvI2L &&
        conv->as_Type()->type()->is_long()->_lo >= 0 &&
        // Are there other uses besides address expressions?
        !matcher->is_visited(conv)) {
      address_visited.set(conv->_idx); // Flag as address_visited
      mstack.push(conv->in(1), Matcher::Pre_Visit);
    } else {
      mstack.push(conv, Matcher::Pre_Visit);
    }
    return true;
  }
  return false;
}

// This function identifies sub-graphs in which a 'load' node is
// input to two different nodes, and such that it can be matched
// with BMI instructions like blsi, blsr, etc.
// Example : for b = -a[i] & a[i] can be matched to blsi r32, m32.
// The graph is (AndL (SubL Con0 LoadL*) LoadL*), where LoadL*
// refers to the same node.
//
// Match the generic fused operations pattern (op1 (op2 Con{ConType} mop) mop)
// This is a temporary solution until we make DAGs expressible in ADL.
template<typename ConType>
class FusedPatternMatcher {
  Node* _op1_node;
  Node* _mop_node;
  int _con_op;

  static int match_next(Node* n, int next_op, int next_op_idx) {
    if (n->in(1) == nullptr || n->in(2) == nullptr) {
      return -1;
    }

    if (next_op_idx == -1) { // n is commutative, try rotations
      if (n->in(1)->Opcode() == next_op) {
        return 1;
      } else if (n->in(2)->Opcode() == next_op) {
        return 2;
      }
    } else {
      assert(next_op_idx > 0 && next_op_idx <= 2, "Bad argument index");
      if (n->in(next_op_idx)->Opcode() == next_op) {
        return next_op_idx;
      }
    }
    return -1;
  }

 public:
  FusedPatternMatcher(Node* op1_node, Node* mop_node, int con_op) :
    _op1_node(op1_node), _mop_node(mop_node), _con_op(con_op) { }

  bool match(int op1, int op1_op2_idx,  // op1 and the index of the op1->op2 edge, -1 if op1 is commutative
             int op2, int op2_con_idx,  // op2 and the index of the op2->con edge, -1 if op2 is commutative
             typename ConType::NativeType con_value) {
    if (_op1_node->Opcode() != op1) {
      return false;
    }
    if (_mop_node->outcnt() > 2) {
      return false;
    }
    op1_op2_idx = match_next(_op1_node, op2, op1_op2_idx);
    if (op1_op2_idx == -1) {
      return false;
    }
    // Memory operation must be the other edge
    int op1_mop_idx = (op1_op2_idx & 1) + 1;

    // Check that the mop node is really what we want
    if (_op1_node->in(op1_mop_idx) == _mop_node) {
      Node* op2_node = _op1_node->in(op1_op2_idx);
      if (op2_node->outcnt() > 1) {
        return false;
      }
      assert(op2_node->Opcode() == op2, "Should be");
      op2_con_idx = match_next(op2_node, _con_op, op2_con_idx);
      if (op2_con_idx == -1) {
        return false;
      }
      // Memory operation must be the other edge
      int op2_mop_idx = (op2_con_idx & 1) + 1;
      // Check that the memory operation is the same node
      if (op2_node->in(op2_mop_idx) == _mop_node) {
        // Now check the constant
        const Type* con_type = op2_node->in(op2_con_idx)->bottom_type();
        if (con_type != Type::TOP && ConType::as_self(con_type)->get_con() == con_value) {
          return true;
        }
      }
    }
    return false;
  }
};

static bool is_bmi_pattern(Node* n, Node* m) {
  assert(UseBMI1Instructions, "sanity");
  if (n != nullptr && m != nullptr) {
    if (m->Opcode() == Op_LoadI) {
      FusedPatternMatcher<TypeInt> bmii(n, m, Op_ConI);
      return bmii.match(Op_AndI, -1, Op_SubI,  1,  0)  ||
             bmii.match(Op_AndI, -1, Op_AddI, -1, -1)  ||
             bmii.match(Op_XorI, -1, Op_AddI, -1, -1);
    } else if (m->Opcode() == Op_LoadL) {
      FusedPatternMatcher<TypeLong> bmil(n, m, Op_ConL);
      return bmil.match(Op_AndL, -1, Op_SubL,  1,  0) ||
             bmil.match(Op_AndL, -1, Op_AddL, -1, -1) ||
             bmil.match(Op_XorL, -1, Op_AddL, -1, -1);
    }
  }
  return false;
}

// Should the matcher clone input 'm' of node 'n'?
bool Matcher::pd_clone_node(Node* n, Node* m, Matcher::MStack& mstack) {
  // If 'n' and 'm' are part of a graph for BMI instruction, clone the input 'm'.
  if (UseBMI1Instructions && is_bmi_pattern(n, m)) {
    mstack.push(m, Visit);
    return true;
  }
  if (is_vshift_con_pattern(n, m)) { // ShiftV src (ShiftCntV con)
    mstack.push(m, Visit);           // m = ShiftCntV
    return true;
  }
  if (is_encode_and_store_pattern(n, m)) {
    mstack.push(m, Visit);
    return true;
  }
  return false;
}

// Should the Matcher clone shifts on addressing modes, expecting them
// to be subsumed into complex addressing expressions or compute them
// into registers?
bool Matcher::pd_clone_address_expressions(AddPNode* m, Matcher::MStack& mstack, VectorSet& address_visited) {
  Node *off = m->in(AddPNode::Offset);
  if (off->is_Con()) {
    address_visited.test_set(m->_idx); // Flag as address_visited
    Node *adr = m->in(AddPNode::Address);

    // Intel can handle 2 adds in addressing mode, with one of them using an immediate offset.
    // AtomicAdd is not an addressing expression.
    // Cheap to find it by looking for screwy base.
    if (adr->is_AddP() &&
        !adr->in(AddPNode::Base)->is_top() &&
        !adr->in(AddPNode::Offset)->is_Con() &&
        off->get_long() == (int) (off->get_long()) && // immL32
        // Are there other uses besides address expressions?
        !is_visited(adr)) {
      address_visited.set(adr->_idx); // Flag as address_visited
      Node *shift = adr->in(AddPNode::Offset);
      if (!clone_shift(shift, this, mstack, address_visited)) {
        mstack.push(shift, Pre_Visit);
      }
      mstack.push(adr->in(AddPNode::Address), Pre_Visit);
      mstack.push(adr->in(AddPNode::Base), Pre_Visit);
    } else {
      mstack.push(adr, Pre_Visit);
    }

    // Clone X+offset as it also folds into most addressing expressions
    mstack.push(off, Visit);
    mstack.push(m->in(AddPNode::Base), Pre_Visit);
    return true;
  } else if (clone_shift(off, this, mstack, address_visited)) {
    address_visited.test_set(m->_idx); // Flag as address_visited
    mstack.push(m->in(AddPNode::Address), Pre_Visit);
    mstack.push(m->in(AddPNode::Base), Pre_Visit);
    return true;
  }
  return false;
}

static inline Assembler::ComparisonPredicate booltest_pred_to_comparison_pred(int bt) {
  switch (bt) {
    case BoolTest::eq:
      return Assembler::eq;
    case BoolTest::ne:
      return Assembler::neq;
    case BoolTest::le:
    case BoolTest::ule:
      return Assembler::le;
    case BoolTest::ge:
    case BoolTest::uge:
      return Assembler::nlt;
    case BoolTest::lt:
    case BoolTest::ult:
      return Assembler::lt;
    case BoolTest::gt:
    case BoolTest::ugt:
      return Assembler::nle;
    default : ShouldNotReachHere(); return Assembler::_false;
  }
}

static inline Assembler::ComparisonPredicateFP booltest_pred_to_comparison_pred_fp(int bt) {
  switch (bt) {
  case BoolTest::eq: return Assembler::EQ_OQ;  // ordered non-signaling
  // As per JLS 15.21.1, != of NaNs is true. Thus use unordered compare.
  case BoolTest::ne: return Assembler::NEQ_UQ; // unordered non-signaling
  case BoolTest::le: return Assembler::LE_OQ;  // ordered non-signaling
  case BoolTest::ge: return Assembler::GE_OQ;  // ordered non-signaling
  case BoolTest::lt: return Assembler::LT_OQ;  // ordered non-signaling
  case BoolTest::gt: return Assembler::GT_OQ;  // ordered non-signaling
  default: ShouldNotReachHere(); return Assembler::FALSE_OS;
  }
}

// Helper methods for MachSpillCopyNode::implementation().
static void vec_mov_helper(C2_MacroAssembler *masm, int src_lo, int dst_lo,
                          int src_hi, int dst_hi, uint ireg, outputStream* st) {
  assert(ireg == Op_VecS || // 32bit vector
         ((src_lo & 1) == 0 && (src_lo + 1) == src_hi &&
          (dst_lo & 1) == 0 && (dst_lo + 1) == dst_hi),
         "no non-adjacent vector moves" );
  if (masm) {
    switch (ireg) {
    case Op_VecS: // copy whole register
    case Op_VecD:
    case Op_VecX:
      if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {
        __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
      } else {
        __ vextractf32x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);
     }
      break;
    case Op_VecY:
      if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {
        __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
      } else {
        __ vextractf64x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);
     }
      break;
    case Op_VecZ:
      __ evmovdquq(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 2);
      break;
    default:
      ShouldNotReachHere();
    }
#ifndef PRODUCT
  } else {
    switch (ireg) {
    case Op_VecS:
    case Op_VecD:
    case Op_VecX:
      st->print("movdqu  %s,%s\t# spill",Matcher::regName[dst_lo],Matcher::regName[src_lo]);
      break;
    case Op_VecY:
    case Op_VecZ:
      st->print("vmovdqu %s,%s\t# spill",Matcher::regName[dst_lo],Matcher::regName[src_lo]);
      break;
    default:
      ShouldNotReachHere();
    }
#endif
  }
}

void vec_spill_helper(C2_MacroAssembler *masm, bool is_load,
                     int stack_offset, int reg, uint ireg, outputStream* st) {
  if (masm) {
    if (is_load) {
      switch (ireg) {
      case Op_VecS:
        __ movdl(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
        break;
      case Op_VecD:
        __ movq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
        break;
      case Op_VecX:
        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {
          __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
        } else {
          __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);
          __ vinsertf32x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);
        }
        break;
      case Op_VecY:
        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {
          __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
        } else {
          __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);
          __ vinsertf64x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);
        }
        break;
      case Op_VecZ:
        __ evmovdquq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset), 2);
        break;
      default:
        ShouldNotReachHere();
      }
    } else { // store
      switch (ireg) {
      case Op_VecS:
        __ movdl(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
        break;
      case Op_VecD:
        __ movq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
        break;
      case Op_VecX:
        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {
          __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
        }
        else {
          __ vextractf32x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);
        }
        break;
      case Op_VecY:
        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {
          __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
        }
        else {
          __ vextractf64x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);
        }
        break;
      case Op_VecZ:
        __ evmovdquq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 2);
        break;
      default:
        ShouldNotReachHere();
      }
    }
#ifndef PRODUCT
  } else {
    if (is_load) {
      switch (ireg) {
      case Op_VecS:
        st->print("movd    %s,[rsp + %d]\t# spill", Matcher::regName[reg], stack_offset);
        break;
      case Op_VecD:
        st->print("movq    %s,[rsp + %d]\t# spill", Matcher::regName[reg], stack_offset);
        break;
       case Op_VecX:
        st->print("movdqu  %s,[rsp + %d]\t# spill", Matcher::regName[reg], stack_offset);
        break;
      case Op_VecY:
      case Op_VecZ:
        st->print("vmovdqu %s,[rsp + %d]\t# spill", Matcher::regName[reg], stack_offset);
        break;
      default:
        ShouldNotReachHere();
      }
    } else { // store
      switch (ireg) {
      case Op_VecS:
        st->print("movd    [rsp + %d],%s\t# spill", stack_offset, Matcher::regName[reg]);
        break;
      case Op_VecD:
        st->print("movq    [rsp + %d],%s\t# spill", stack_offset, Matcher::regName[reg]);
        break;
       case Op_VecX:
        st->print("movdqu  [rsp + %d],%s\t# spill", stack_offset, Matcher::regName[reg]);
        break;
      case Op_VecY:
      case Op_VecZ:
        st->print("vmovdqu [rsp + %d],%s\t# spill", stack_offset, Matcher::regName[reg]);
        break;
      default:
        ShouldNotReachHere();
      }
    }
#endif
  }
}

template <class T>
static inline GrowableArray<jbyte>* vreplicate_imm(BasicType bt, T con, int len) {
  int size = type2aelembytes(bt) * len;
  GrowableArray<jbyte>* val = new GrowableArray<jbyte>(size, size, 0);
  for (int i = 0; i < len; i++) {
    int offset = i * type2aelembytes(bt);
    switch (bt) {
      case T_BYTE: val->at(i) = con; break;
      case T_SHORT: {
        jshort c = con;
        memcpy(val->adr_at(offset), &c, sizeof(jshort));
        break;
      }
      case T_INT: {
        jint c = con;
        memcpy(val->adr_at(offset), &c, sizeof(jint));
        break;
      }
      case T_LONG: {
        jlong c = con;
        memcpy(val->adr_at(offset), &c, sizeof(jlong));
        break;
      }
      case T_FLOAT: {
        jfloat c = con;
        memcpy(val->adr_at(offset), &c, sizeof(jfloat));
        break;
      }
      case T_DOUBLE: {
        jdouble c = con;
        memcpy(val->adr_at(offset), &c, sizeof(jdouble));
        break;
      }
      default: assert(false, "%s", type2name(bt));
    }
  }
  return val;
}

static inline jlong high_bit_set(BasicType bt) {
  switch (bt) {
    case T_BYTE:  return 0x8080808080808080;
    case T_SHORT: return 0x8000800080008000;
    case T_INT:   return 0x8000000080000000;
    case T_LONG:  return 0x8000000000000000;
    default:
      ShouldNotReachHere();
      return 0;
  }
}

#ifndef PRODUCT
  void MachNopNode::format(PhaseRegAlloc*, outputStream* st) const {
    st->print("nop \t# %d bytes pad for loops and calls", _count);
  }
#endif

  void MachNopNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc*) const {
    __ nop(_count);
  }

  uint MachNopNode::size(PhaseRegAlloc*) const {
    return _count;
  }

#ifndef PRODUCT
  void MachBreakpointNode::format(PhaseRegAlloc*, outputStream* st) const {
    st->print("# breakpoint");
  }
#endif

  void MachBreakpointNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const {
    __ int3();
  }

  uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {
    return MachNode::size(ra_);
  }

%}

//----------ENCODING BLOCK-----------------------------------------------------
// This block specifies the encoding classes used by the compiler to
// output byte streams.  Encoding classes are parameterized macros
// used by Machine Instruction Nodes in order to generate the bit
// encoding of the instruction.  Operands specify their base encoding
// interface with the interface keyword.  There are currently
// supported four interfaces, REG_INTER, CONST_INTER, MEMORY_INTER, &
// COND_INTER.  REG_INTER causes an operand to generate a function
// which returns its register number when queried.  CONST_INTER causes
// an operand to generate a function which returns the value of the
// constant when queried.  MEMORY_INTER causes an operand to generate
// four functions which return the Base Register, the Index Register,
// the Scale Value, and the Offset Value of the operand when queried.
// COND_INTER causes an operand to generate six functions which return
// the encoding code (ie - encoding bits for the instruction)
// associated with each basic boolean condition for a conditional
// instruction.
//
// Instructions specify two basic values for encoding.  Again, a
// function is available to check if the constant displacement is an
// oop. They use the ins_encode keyword to specify their encoding
// classes (which must be a sequence of enc_class names, and their
// parameters, specified in the encoding block), and they use the
// opcode keyword to specify, in order, their primary, secondary, and
// tertiary opcode.  Only the opcode sections which a particular
// instruction needs for encoding need to be specified.
encode %{
  enc_class cdql_enc(no_rax_rdx_RegI div)
  %{
    // Full implementation of Java idiv and irem; checks for
    // special case as described in JVM spec., p.243 & p.271.
    //
    //         normal case                           special case
    //
    // input : rax: dividend                         min_int
    //         reg: divisor                          -1
    //
    // output: rax: quotient  (= rax idiv reg)       min_int
    //         rdx: remainder (= rax irem reg)       0
    //
    //  Code sequnce:
    //
    //    0:   3d 00 00 00 80          cmp    $0x80000000,%eax
    //    5:   75 07/08                jne    e <normal>
    //    7:   33 d2                   xor    %edx,%edx
    //  [div >= 8 -> offset + 1]
    //  [REX_B]
    //    9:   83 f9 ff                cmp    $0xffffffffffffffff,$div
    //    c:   74 03/04                je     11 <done>
    // 000000000000000e <normal>:
    //    e:   99                      cltd
    //  [div >= 8 -> offset + 1]
    //  [REX_B]
    //    f:   f7 f9                   idiv   $div
    // 0000000000000011 <done>:
    Label normal;
    Label done;

    // cmp    $0x80000000,%eax
    __ cmpl(as_Register(RAX_enc), 0x80000000);

    // jne    e <normal>
    __ jccb(Assembler::notEqual, normal);

    // xor    %edx,%edx
    __ xorl(as_Register(RDX_enc), as_Register(RDX_enc));

    // cmp    $0xffffffffffffffff,%ecx
    __ cmpl($div$$Register, -1);

    // je     11 <done>
    __ jccb(Assembler::equal, done);

    // <normal>
    // cltd
    __ bind(normal);
    __ cdql();

    // idivl
    // <done>
    __ idivl($div$$Register);
    __ bind(done);
  %}

  enc_class cdqq_enc(no_rax_rdx_RegL div)
  %{
    // Full implementation of Java ldiv and lrem; checks for
    // special case as described in JVM spec., p.243 & p.271.
    //
    //         normal case                           special case
    //
    // input : rax: dividend                         min_long
    //         reg: divisor                          -1
    //
    // output: rax: quotient  (= rax idiv reg)       min_long
    //         rdx: remainder (= rax irem reg)       0
    //
    //  Code sequnce:
    //
    //    0:   48 ba 00 00 00 00 00    mov    $0x8000000000000000,%rdx
    //    7:   00 00 80
    //    a:   48 39 d0                cmp    %rdx,%rax
    //    d:   75 08                   jne    17 <normal>
    //    f:   33 d2                   xor    %edx,%edx
    //   11:   48 83 f9 ff             cmp    $0xffffffffffffffff,$div
    //   15:   74 05                   je     1c <done>
    // 0000000000000017 <normal>:
    //   17:   48 99                   cqto
    //   19:   48 f7 f9                idiv   $div
    // 000000000000001c <done>:
    Label normal;
    Label done;

    // mov    $0x8000000000000000,%rdx
    __ mov64(as_Register(RDX_enc), 0x8000000000000000);

    // cmp    %rdx,%rax
    __ cmpq(as_Register(RAX_enc), as_Register(RDX_enc));

    // jne    17 <normal>
    __ jccb(Assembler::notEqual, normal);

    // xor    %edx,%edx
    __ xorl(as_Register(RDX_enc), as_Register(RDX_enc));

    // cmp    $0xffffffffffffffff,$div
    __ cmpq($div$$Register, -1);

    // je     1e <done>
    __ jccb(Assembler::equal, done);

    // <normal>
    // cqto
    __ bind(normal);
    __ cdqq();

    // idivq (note: must be emitted by the user of this rule)
    // <done>
    __ idivq($div$$Register);
    __ bind(done);
  %}

  enc_class clear_avx %{
    DEBUG_ONLY(int off0 = __ offset());
    if (generate_vzeroupper(Compile::current())) {
      // Clear upper bits of YMM registers to avoid AVX <-> SSE transition penalty
      // Clear upper bits of YMM registers when current compiled code uses
      // wide vectors to avoid AVX <-> SSE transition penalty during call.
      __ vzeroupper();
    }
    DEBUG_ONLY(int off1 = __ offset());
    assert(off1 - off0 == clear_avx_size(), "correct size prediction");
  %}

  enc_class Java_To_Runtime(method meth) %{
    __ lea(r10, RuntimeAddress((address)$meth$$method));
    __ call(r10);
    __ post_call_nop();
  %}

  enc_class Java_Static_Call(method meth)
  %{
    // JAVA STATIC CALL
    // CALL to fixup routine.  Fixup routine uses ScopeDesc info to
    // determine who we intended to call.
    if (!_method) {
      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, $meth$$method)));
    } else if (_method->intrinsic_id() == vmIntrinsicID::_ensureMaterializedForStackWalk) {
      // The NOP here is purely to ensure that eliding a call to
      // JVM_EnsureMaterializedForStackWalk doesn't change the code size.
      __ addr_nop_5();
      __ block_comment("call JVM_EnsureMaterializedForStackWalk (elided)");
    } else {
      int method_index = resolved_method_index(masm);
      RelocationHolder rspec = _optimized_virtual ? opt_virtual_call_Relocation::spec(method_index)
                                                  : static_call_Relocation::spec(method_index);
      address mark = __ pc();
      int call_offset = __ offset();
      __ call(AddressLiteral(CAST_FROM_FN_PTR(address, $meth$$method), rspec));
      if (CodeBuffer::supports_shared_stubs() && _method->can_be_statically_bound()) {
        // Calls of the same statically bound method can share
        // a stub to the interpreter.
        __ code()->shared_stub_to_interp_for(_method, call_offset);
      } else {
        // Emit stubs for static call.
        address stub = CompiledDirectCall::emit_to_interp_stub(masm, mark);
        __ clear_inst_mark();
        if (stub == nullptr) {
          ciEnv::current()->record_failure("CodeCache is full");
          return;
        }
      }
    }
    __ post_call_nop();
  %}

  enc_class Java_Dynamic_Call(method meth) %{
    __ ic_call((address)$meth$$method, resolved_method_index(masm));
    __ post_call_nop();
  %}

  enc_class call_epilog %{
    if (VerifyStackAtCalls) {
      // Check that stack depth is unchanged: find majik cookie on stack
      int framesize = ra_->reg2offset_unchecked(OptoReg::add(ra_->_matcher._old_SP, -3*VMRegImpl::slots_per_word));
      Label L;
      __ cmpptr(Address(rsp, framesize), (int32_t)0xbadb100d);
      __ jccb(Assembler::equal, L);
      // Die if stack mismatch
      __ int3();
      __ bind(L);
    }
  %}

%}

//----------FRAME--------------------------------------------------------------
// Definition of frame structure and management information.
//
//  S T A C K   L A Y O U T    Allocators stack-slot number
//                             |   (to get allocators register number
//  G  Owned by    |        |  v    add OptoReg::stack0())
//  r   CALLER     |        |
//  o     |        +--------+      pad to even-align allocators stack-slot
//  w     V        |  pad0  |        numbers; owned by CALLER
//  t   -----------+--------+----> Matcher::_in_arg_limit, unaligned
//  h     ^        |   in   |  5
//        |        |  args  |  4   Holes in incoming args owned by SELF
//  |     |        |        |  3
//  |     |        +--------+
//  V     |        | old out|      Empty on Intel, window on Sparc
//        |    old |preserve|      Must be even aligned.
//        |     SP-+--------+----> Matcher::_old_SP, even aligned
//        |        |   in   |  3   area for Intel ret address
//     Owned by    |preserve|      Empty on Sparc.
//       SELF      +--------+
//        |        |  pad2  |  2   pad to align old SP
//        |        +--------+  1
//        |        | locks  |  0
//        |        +--------+----> OptoReg::stack0(), even aligned
//        |        |  pad1  | 11   pad to align new SP
//        |        +--------+
//        |        |        | 10
//        |        | spills |  9   spills
//        V        |        |  8   (pad0 slot for callee)
//      -----------+--------+----> Matcher::_out_arg_limit, unaligned
//        ^        |  out   |  7
//        |        |  args  |  6   Holes in outgoing args owned by CALLEE
//     Owned by    +--------+
//      CALLEE     | new out|  6   Empty on Intel, window on Sparc
//        |    new |preserve|      Must be even-aligned.
//        |     SP-+--------+----> Matcher::_new_SP, even aligned
//        |        |        |
//
// Note 1: Only region 8-11 is determined by the allocator.  Region 0-5 is
//         known from SELF's arguments and the Java calling convention.
//         Region 6-7 is determined per call site.
// Note 2: If the calling convention leaves holes in the incoming argument
//         area, those holes are owned by SELF.  Holes in the outgoing area
//         are owned by the CALLEE.  Holes should not be necessary in the
//         incoming area, as the Java calling convention is completely under
//         the control of the AD file.  Doubles can be sorted and packed to
//         avoid holes.  Holes in the outgoing arguments may be necessary for
//         varargs C calling conventions.
// Note 3: Region 0-3 is even aligned, with pad2 as needed.  Region 3-5 is
//         even aligned with pad0 as needed.
//         Region 6 is even aligned.  Region 6-7 is NOT even aligned;
//         region 6-11 is even aligned; it may be padded out more so that
//         the region from SP to FP meets the minimum stack alignment.
// Note 4: For I2C adapters, the incoming FP may not meet the minimum stack
//         alignment.  Region 11, pad1, may be dynamically extended so that
//         SP meets the minimum alignment.

frame
%{
  // These three registers define part of the calling convention
  // between compiled code and the interpreter.
  inline_cache_reg(RAX);                // Inline Cache Register

  // Optional: name the operand used by cisc-spilling to access
  // [stack_pointer + offset]
  cisc_spilling_operand_name(indOffset32);

  // Number of stack slots consumed by locking an object
  sync_stack_slots(2);

  // Compiled code's Frame Pointer
  frame_pointer(RSP);

  // Interpreter stores its frame pointer in a register which is
  // stored to the stack by I2CAdaptors.
  // I2CAdaptors convert from interpreted java to compiled java.
  interpreter_frame_pointer(RBP);

  // Stack alignment requirement
  stack_alignment(StackAlignmentInBytes); // Alignment size in bytes (128-bit -> 16 bytes)

  // Number of outgoing stack slots killed above the out_preserve_stack_slots
  // for calls to C.  Supports the var-args backing area for register parms.
  varargs_C_out_slots_killed(frame::arg_reg_save_area_bytes/BytesPerInt);

  // The after-PROLOG location of the return address.  Location of
  // return address specifies a type (REG or STACK) and a number
  // representing the register number (i.e. - use a register name) or
  // stack slot.
  // Ret Addr is on stack in slot 0 if no locks or verification or alignment.
  // Otherwise, it is above the locks and verification slot and alignment word
  return_addr(STACK - 2 +
              align_up((Compile::current()->in_preserve_stack_slots() +
                        Compile::current()->fixed_slots()),
                       stack_alignment_in_slots()));

  // Location of compiled Java return values.  Same as C for now.
  return_value
  %{
    assert(ideal_reg >= Op_RegI && ideal_reg <= Op_RegL,
           "only return normal values");

    static const int lo[Op_RegL + 1] = {
      0,
      0,
      RAX_num,  // Op_RegN
      RAX_num,  // Op_RegI
      RAX_num,  // Op_RegP
      XMM0_num, // Op_RegF
      XMM0_num, // Op_RegD
      RAX_num   // Op_RegL
    };
    static const int hi[Op_RegL + 1] = {
      0,
      0,
      OptoReg::Bad, // Op_RegN
      OptoReg::Bad, // Op_RegI
      RAX_H_num,    // Op_RegP
      OptoReg::Bad, // Op_RegF
      XMM0b_num,    // Op_RegD
      RAX_H_num     // Op_RegL
    };
    // Excluded flags and vector registers.
    assert(ARRAY_SIZE(hi) == _last_machine_leaf - 8, "missing type");
    return OptoRegPair(hi[ideal_reg], lo[ideal_reg]);
  %}
%}

//----------ATTRIBUTES---------------------------------------------------------
//----------Operand Attributes-------------------------------------------------
op_attrib op_cost(0);        // Required cost attribute

//----------Instruction Attributes---------------------------------------------
ins_attrib ins_cost(100);       // Required cost attribute
ins_attrib ins_size(8);         // Required size attribute (in bits)
ins_attrib ins_short_branch(0); // Required flag: is this instruction
                                // a non-matching short branch variant
                                // of some long branch?
ins_attrib ins_alignment(1);    // Required alignment attribute (must
                                // be a power of 2) specifies the
                                // alignment that some part of the
                                // instruction (not necessarily the
                                // start) requires.  If > 1, a
                                // compute_padding() function must be
                                // provided for the instruction

// Whether this node is expanded during code emission into a sequence of
// instructions and the first instruction can perform an implicit null check.
ins_attrib ins_is_late_expanded_null_check_candidate(false);

//----------OPERANDS-----------------------------------------------------------
// Operand definitions must precede instruction definitions for correct parsing
// in the ADLC because operands constitute user defined types which are used in
// instruction definitions.

//----------Simple Operands----------------------------------------------------
// Immediate Operands
// Integer Immediate
operand immI()
%{
  match(ConI);

  op_cost(10);
  format %{ %}
  interface(CONST_INTER);
%}

// Constant for test vs zero
operand immI_0()
%{
  predicate(n->get_int() == 0);
  match(ConI);

  op_cost(0);
  format %{ %}
  interface(CONST_INTER);
%}

// Constant for increment
operand immI_1()
%{
  predicate(n->get_int() == 1);
  match(ConI);

  op_cost(0);
  format %{ %}
  interface(CONST_INTER);
%}

// Constant for decrement
operand immI_M1()
%{
  predicate(n->get_int() == -1);
  match(ConI);

  op_cost(0);
  format %{ %}
  interface(CONST_INTER);
%}

operand immI_2()
%{
  predicate(n->get_int() == 2);
  match(ConI);

  op_cost(0);
  format %{ %}
  interface(CONST_INTER);
%}

operand immI_4()
%{
  predicate(n->get_int() == 4);
  match(ConI);

  op_cost(0);
  format %{ %}
  interface(CONST_INTER);
%}

operand immI_8()
%{
  predicate(n->get_int() == 8);
  match(ConI);

  op_cost(0);
  format %{ %}
  interface(CONST_INTER);
%}

// Valid scale values for addressing modes
operand immI2()
%{
  predicate(0 <= n->get_int() && (n->get_int() <= 3));
  match(ConI);

  format %{ %}
  interface(CONST_INTER);
%}

operand immU7()
%{
  predicate((0 <= n->get_int()) && (n->get_int() <= 0x7F));
  match(ConI);

  op_cost(5);
  format %{ %}
  interface(CONST_INTER);
%}

operand immI8()
%{
  predicate((-0x80 <= n->get_int()) && (n->get_int() < 0x80));
  match(ConI);

  op_cost(5);
  format %{ %}
  interface(CONST_INTER);
%}

operand immU8()
%{
  predicate((0 <= n->get_int()) && (n->get_int() <= 255));
  match(ConI);

  op_cost(5);
  format %{ %}
  interface(CONST_INTER);
%}

operand immI16()
%{
  predicate((-32768 <= n->get_int()) && (n->get_int() <= 32767));
  match(ConI);

  op_cost(10);
  format %{ %}
  interface(CONST_INTER);
%}

// Int Immediate non-negative
operand immU31()
%{
  predicate(n->get_int() >= 0);
  match(ConI);

  op_cost(0);
  format %{ %}
  interface(CONST_INTER);
%}

// Pointer Immediate
operand immP()
%{
  match(ConP);

  op_cost(10);
  format %{ %}
  interface(CONST_INTER);
%}

// Null Pointer Immediate
operand immP0()
%{
  predicate(n->get_ptr() == 0);
  match(ConP);

  op_cost(5);
  format %{ %}
  interface(CONST_INTER);
%}

// Pointer Immediate
operand immN() %{
  match(ConN);

  op_cost(10);
  format %{ %}
  interface(CONST_INTER);
%}

operand immNKlass() %{
  match(ConNKlass);

  op_cost(10);
  format %{ %}
  interface(CONST_INTER);
%}

// Null Pointer Immediate
operand immN0() %{
  predicate(n->get_narrowcon() == 0);
  match(ConN);

  op_cost(5);
  format %{ %}
  interface(CONST_INTER);
%}

operand immP31()
%{
  predicate(n->as_Type()->type()->reloc() == relocInfo::none
            && (n->get_ptr() >> 31) == 0);
  match(ConP);

  op_cost(5);
  format %{ %}
  interface(CONST_INTER);
%}


// Long Immediate
operand immL()
%{
  match(ConL);

  op_cost(20);
  format %{ %}
  interface(CONST_INTER);
%}

// Long Immediate 8-bit
operand immL8()
%{
  predicate(-0x80L <= n->get_long() && n->get_long() < 0x80L);
  match(ConL);

  op_cost(5);
  format %{ %}
  interface(CONST_INTER);
%}

// Long Immediate 32-bit unsigned
operand immUL32()
%{
  predicate(n->get_long() == (unsigned int) (n->get_long()));
  match(ConL);

  op_cost(10);
  format %{ %}
  interface(CONST_INTER);
%}

// Long Immediate 32-bit signed
operand immL32()
%{
  predicate(n->get_long() == (int) (n->get_long()));
  match(ConL);

  op_cost(15);
  format %{ %}
  interface(CONST_INTER);
%}

operand immL_Pow2()
%{
  predicate(is_power_of_2((julong)n->get_long()));
  match(ConL);

  op_cost(15);
  format %{ %}
  interface(CONST_INTER);
%}

operand immL_NotPow2()
%{
  predicate(is_power_of_2((julong)~n->get_long()));
  match(ConL);

  op_cost(15);
  format %{ %}
  interface(CONST_INTER);
%}

// Long Immediate zero
operand immL0()
%{
  predicate(n->get_long() == 0L);
  match(ConL);

  op_cost(10);
  format %{ %}
  interface(CONST_INTER);
%}

// Constant for increment
operand immL1()
%{
  predicate(n->get_long() == 1);
  match(ConL);

  format %{ %}
  interface(CONST_INTER);
%}

// Constant for decrement
operand immL_M1()
%{
  predicate(n->get_long() == -1);
  match(ConL);

  format %{ %}
  interface(CONST_INTER);
%}

// Long Immediate: low 32-bit mask
operand immL_32bits()
%{
  predicate(n->get_long() == 0xFFFFFFFFL);
  match(ConL);
  op_cost(20);

  format %{ %}
  interface(CONST_INTER);
%}

// Int Immediate: 2^n-1, positive
operand immI_Pow2M1()
%{
  predicate((n->get_int() > 0)
            && is_power_of_2((juint)n->get_int() + 1));
  match(ConI);

  op_cost(20);
  format %{ %}
  interface(CONST_INTER);
%}

// Float Immediate zero
operand immF0()
%{
  predicate(jint_cast(n->getf()) == 0);
  match(ConF);

  op_cost(5);
  format %{ %}
  interface(CONST_INTER);
%}

// Float Immediate
operand immF()
%{
  match(ConF);

  op_cost(15);
  format %{ %}
  interface(CONST_INTER);
%}

// Half Float Immediate
operand immH()
%{
  match(ConH);

  op_cost(15);
  format %{ %}
  interface(CONST_INTER);
%}

// Double Immediate zero
operand immD0()
%{
  predicate(jlong_cast(n->getd()) == 0);
  match(ConD);

  op_cost(5);
  format %{ %}
  interface(CONST_INTER);
%}

// Double Immediate
operand immD()
%{
  match(ConD);

  op_cost(15);
  format %{ %}
  interface(CONST_INTER);
%}

// Immediates for special shifts (sign extend)

// Constants for increment
operand immI_16()
%{
  predicate(n->get_int() == 16);
  match(ConI);

  format %{ %}
  interface(CONST_INTER);
%}

operand immI_24()
%{
  predicate(n->get_int() == 24);
  match(ConI);

  format %{ %}
  interface(CONST_INTER);
%}

// Constant for byte-wide masking
operand immI_255()
%{
  predicate(n->get_int() == 255);
  match(ConI);

  format %{ %}
  interface(CONST_INTER);
%}

// Constant for short-wide masking
operand immI_65535()
%{
  predicate(n->get_int() == 65535);
  match(ConI);

  format %{ %}
  interface(CONST_INTER);
%}

// Constant for byte-wide masking
operand immL_255()
%{
  predicate(n->get_long() == 255);
  match(ConL);

  format %{ %}
  interface(CONST_INTER);
%}

// Constant for short-wide masking
operand immL_65535()
%{
  predicate(n->get_long() == 65535);
  match(ConL);

  format %{ %}
  interface(CONST_INTER);
%}

operand kReg()
%{
  constraint(ALLOC_IN_RC(vectmask_reg));
  match(RegVectMask);
  format %{%}
  interface(REG_INTER);
%}

// Register Operands
// Integer Register
operand rRegI()
%{
  constraint(ALLOC_IN_RC(int_reg));
  match(RegI);

  match(rax_RegI);
  match(rbx_RegI);
  match(rcx_RegI);
  match(rdx_RegI);
  match(rdi_RegI);

  format %{ %}
  interface(REG_INTER);
%}

// Special Registers
operand rax_RegI()
%{
  constraint(ALLOC_IN_RC(int_rax_reg));
  match(RegI);
  match(rRegI);

  format %{ "RAX" %}
  interface(REG_INTER);
%}

// Special Registers
operand rbx_RegI()
%{
  constraint(ALLOC_IN_RC(int_rbx_reg));
  match(RegI);
  match(rRegI);

  format %{ "RBX" %}
  interface(REG_INTER);
%}

operand rcx_RegI()
%{
  constraint(ALLOC_IN_RC(int_rcx_reg));
  match(RegI);
  match(rRegI);

  format %{ "RCX" %}
  interface(REG_INTER);
%}

operand rdx_RegI()
%{
  constraint(ALLOC_IN_RC(int_rdx_reg));
  match(RegI);
  match(rRegI);

  format %{ "RDX" %}
  interface(REG_INTER);
%}

operand rdi_RegI()
%{
  constraint(ALLOC_IN_RC(int_rdi_reg));
  match(RegI);
  match(rRegI);

  format %{ "RDI" %}
  interface(REG_INTER);
%}

operand no_rax_rdx_RegI()
%{
  constraint(ALLOC_IN_RC(int_no_rax_rdx_reg));
  match(RegI);
  match(rbx_RegI);
  match(rcx_RegI);
  match(rdi_RegI);

  format %{ %}
  interface(REG_INTER);
%}

operand no_rbp_r13_RegI()
%{
  constraint(ALLOC_IN_RC(int_no_rbp_r13_reg));
  match(RegI);
  match(rRegI);
  match(rax_RegI);
  match(rbx_RegI);
  match(rcx_RegI);
  match(rdx_RegI);
  match(rdi_RegI);

  format %{ %}
  interface(REG_INTER);
%}

// Pointer Register
operand any_RegP()
%{
  constraint(ALLOC_IN_RC(any_reg));
  match(RegP);
  match(rax_RegP);
  match(rbx_RegP);
  match(rdi_RegP);
  match(rsi_RegP);
  match(rbp_RegP);
  match(r15_RegP);
  match(rRegP);

  format %{ %}
  interface(REG_INTER);
%}

operand rRegP()
%{
  constraint(ALLOC_IN_RC(ptr_reg));
  match(RegP);
  match(rax_RegP);
  match(rbx_RegP);
  match(rdi_RegP);
  match(rsi_RegP);
  match(rbp_RegP);  // See Q&A below about
  match(r15_RegP);  // r15_RegP and rbp_RegP.

  format %{ %}
  interface(REG_INTER);
%}

operand rRegN() %{
  constraint(ALLOC_IN_RC(int_reg));
  match(RegN);

  format %{ %}
  interface(REG_INTER);
%}

// Question: Why is r15_RegP (the read-only TLS register) a match for rRegP?
// Answer: Operand match rules govern the DFA as it processes instruction inputs.
// It's fine for an instruction input that expects rRegP to match a r15_RegP.
// The output of an instruction is controlled by the allocator, which respects
// register class masks, not match rules.  Unless an instruction mentions
// r15_RegP or any_RegP explicitly as its output, r15 will not be considered
// by the allocator as an input.
// The same logic applies to rbp_RegP being a match for rRegP: If PreserveFramePointer==true,
// the RBP is used as a proper frame pointer and is not included in ptr_reg. As a
// result, RBP is not included in the output of the instruction either.

// This operand is not allowed to use RBP even if
// RBP is not used to hold the frame pointer.
operand no_rbp_RegP()
%{
  constraint(ALLOC_IN_RC(ptr_reg_no_rbp));
  match(RegP);
  match(rbx_RegP);
  match(rsi_RegP);
  match(rdi_RegP);

  format %{ %}
  interface(REG_INTER);
%}

// Special Registers
// Return a pointer value
operand rax_RegP()
%{
  constraint(ALLOC_IN_RC(ptr_rax_reg));
  match(RegP);
  match(rRegP);

  format %{ %}
  interface(REG_INTER);
%}

// Special Registers
// Return a compressed pointer value
operand rax_RegN()
%{
  constraint(ALLOC_IN_RC(int_rax_reg));
  match(RegN);
  match(rRegN);

  format %{ %}
  interface(REG_INTER);
%}

// Used in AtomicAdd
operand rbx_RegP()
%{
  constraint(ALLOC_IN_RC(ptr_rbx_reg));
  match(RegP);
  match(rRegP);

  format %{ %}
  interface(REG_INTER);
%}

operand rsi_RegP()
%{
  constraint(ALLOC_IN_RC(ptr_rsi_reg));
  match(RegP);
  match(rRegP);

  format %{ %}
  interface(REG_INTER);
%}

operand rbp_RegP()
%{
  constraint(ALLOC_IN_RC(ptr_rbp_reg));
  match(RegP);
  match(rRegP);

  format %{ %}
  interface(REG_INTER);
%}

// Used in rep stosq
operand rdi_RegP()
%{
  constraint(ALLOC_IN_RC(ptr_rdi_reg));
  match(RegP);
  match(rRegP);

  format %{ %}
  interface(REG_INTER);
%}

operand r15_RegP()
%{
  constraint(ALLOC_IN_RC(ptr_r15_reg));
  match(RegP);
  match(rRegP);

  format %{ %}
  interface(REG_INTER);
%}

operand rRegL()
%{
  constraint(ALLOC_IN_RC(long_reg));
  match(RegL);
  match(rax_RegL);
  match(rdx_RegL);

  format %{ %}
  interface(REG_INTER);
%}

// Special Registers
operand no_rax_rdx_RegL()
%{
  constraint(ALLOC_IN_RC(long_no_rax_rdx_reg));
  match(RegL);
  match(rRegL);

  format %{ %}
  interface(REG_INTER);
%}

operand rax_RegL()
%{
  constraint(ALLOC_IN_RC(long_rax_reg));
  match(RegL);
  match(rRegL);

  format %{ "RAX" %}
  interface(REG_INTER);
%}

operand rcx_RegL()
%{
  constraint(ALLOC_IN_RC(long_rcx_reg));
  match(RegL);
  match(rRegL);

  format %{ %}
  interface(REG_INTER);
%}

operand rdx_RegL()
%{
  constraint(ALLOC_IN_RC(long_rdx_reg));
  match(RegL);
  match(rRegL);

  format %{ %}
  interface(REG_INTER);
%}

operand r11_RegL()
%{
  constraint(ALLOC_IN_RC(long_r11_reg));
  match(RegL);
  match(rRegL);

  format %{ %}
  interface(REG_INTER);
%}

operand no_rbp_r13_RegL()
%{
  constraint(ALLOC_IN_RC(long_no_rbp_r13_reg));
  match(RegL);
  match(rRegL);
  match(rax_RegL);
  match(rcx_RegL);
  match(rdx_RegL);

  format %{ %}
  interface(REG_INTER);
%}

// Flags register, used as output of compare instructions
operand rFlagsReg()
%{
  constraint(ALLOC_IN_RC(int_flags));
  match(RegFlags);

  format %{ "RFLAGS" %}
  interface(REG_INTER);
%}

// Flags register, used as output of FLOATING POINT compare instructions
operand rFlagsRegU()
%{
  constraint(ALLOC_IN_RC(int_flags));
  match(RegFlags);

  format %{ "RFLAGS_U" %}
  interface(REG_INTER);
%}

operand rFlagsRegUCF() %{
  constraint(ALLOC_IN_RC(int_flags));
  match(RegFlags);
  predicate(false);

  format %{ "RFLAGS_U_CF" %}
  interface(REG_INTER);
%}

// Float register operands
operand regF() %{
   constraint(ALLOC_IN_RC(float_reg));
   match(RegF);

   format %{ %}
   interface(REG_INTER);
%}

// Float register operands
operand legRegF() %{
   constraint(ALLOC_IN_RC(float_reg_legacy));
   match(RegF);

   format %{ %}
   interface(REG_INTER);
%}

// Float register operands
operand vlRegF() %{
   constraint(ALLOC_IN_RC(float_reg_vl));
   match(RegF);

   format %{ %}
   interface(REG_INTER);
%}

// Double register operands
operand regD() %{
   constraint(ALLOC_IN_RC(double_reg));
   match(RegD);

   format %{ %}
   interface(REG_INTER);
%}

// Double register operands
operand legRegD() %{
   constraint(ALLOC_IN_RC(double_reg_legacy));
   match(RegD);

   format %{ %}
   interface(REG_INTER);
%}

// Double register operands
operand vlRegD() %{
   constraint(ALLOC_IN_RC(double_reg_vl));
   match(RegD);

   format %{ %}
   interface(REG_INTER);
%}

//----------Memory Operands----------------------------------------------------
// Direct Memory Operand
// operand direct(immP addr)
// %{
//   match(addr);

//   format %{ "[$addr]" %}
//   interface(MEMORY_INTER) %{
//     base(0xFFFFFFFF);
//     index(0x4);
//     scale(0x0);
//     disp($addr);
//   %}
// %}

// Indirect Memory Operand
operand indirect(any_RegP reg)
%{
  constraint(ALLOC_IN_RC(ptr_reg));
  match(reg);

  format %{ "[$reg]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index(0x4);
    scale(0x0);
    disp(0x0);
  %}
%}

// Indirect Memory Plus Short Offset Operand
operand indOffset8(any_RegP reg, immL8 off)
%{
  constraint(ALLOC_IN_RC(ptr_reg));
  match(AddP reg off);

  format %{ "[$reg + $off (8-bit)]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index(0x4);
    scale(0x0);
    disp($off);
  %}
%}

// Indirect Memory Plus Long Offset Operand
operand indOffset32(any_RegP reg, immL32 off)
%{
  constraint(ALLOC_IN_RC(ptr_reg));
  match(AddP reg off);

  format %{ "[$reg + $off (32-bit)]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index(0x4);
    scale(0x0);
    disp($off);
  %}
%}

// Indirect Memory Plus Index Register Plus Offset Operand
operand indIndexOffset(any_RegP reg, rRegL lreg, immL32 off)
%{
  constraint(ALLOC_IN_RC(ptr_reg));
  match(AddP (AddP reg lreg) off);

  op_cost(10);
  format %{"[$reg + $off + $lreg]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index($lreg);
    scale(0x0);
    disp($off);
  %}
%}

// Indirect Memory Plus Index Register Plus Offset Operand
operand indIndex(any_RegP reg, rRegL lreg)
%{
  constraint(ALLOC_IN_RC(ptr_reg));
  match(AddP reg lreg);

  op_cost(10);
  format %{"[$reg + $lreg]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index($lreg);
    scale(0x0);
    disp(0x0);
  %}
%}

// Indirect Memory Times Scale Plus Index Register
operand indIndexScale(any_RegP reg, rRegL lreg, immI2 scale)
%{
  constraint(ALLOC_IN_RC(ptr_reg));
  match(AddP reg (LShiftL lreg scale));

  op_cost(10);
  format %{"[$reg + $lreg << $scale]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index($lreg);
    scale($scale);
    disp(0x0);
  %}
%}

operand indPosIndexScale(any_RegP reg, rRegI idx, immI2 scale)
%{
  constraint(ALLOC_IN_RC(ptr_reg));
  predicate(n->in(3)->in(1)->as_Type()->type()->is_long()->_lo >= 0);
  match(AddP reg (LShiftL (ConvI2L idx) scale));

  op_cost(10);
  format %{"[$reg + pos $idx << $scale]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index($idx);
    scale($scale);
    disp(0x0);
  %}
%}

// Indirect Memory Times Scale Plus Index Register Plus Offset Operand
operand indIndexScaleOffset(any_RegP reg, immL32 off, rRegL lreg, immI2 scale)
%{
  constraint(ALLOC_IN_RC(ptr_reg));
  match(AddP (AddP reg (LShiftL lreg scale)) off);

  op_cost(10);
  format %{"[$reg + $off + $lreg << $scale]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index($lreg);
    scale($scale);
    disp($off);
  %}
%}

// Indirect Memory Plus Positive Index Register Plus Offset Operand
operand indPosIndexOffset(any_RegP reg, immL32 off, rRegI idx)
%{
  constraint(ALLOC_IN_RC(ptr_reg));
  predicate(n->in(2)->in(3)->as_Type()->type()->is_long()->_lo >= 0);
  match(AddP (AddP reg (ConvI2L idx)) off);

  op_cost(10);
  format %{"[$reg + $off + $idx]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index($idx);
    scale(0x0);
    disp($off);
  %}
%}

// Indirect Memory Times Scale Plus Positive Index Register Plus Offset Operand
operand indPosIndexScaleOffset(any_RegP reg, immL32 off, rRegI idx, immI2 scale)
%{
  constraint(ALLOC_IN_RC(ptr_reg));
  predicate(n->in(2)->in(3)->in(1)->as_Type()->type()->is_long()->_lo >= 0);
  match(AddP (AddP reg (LShiftL (ConvI2L idx) scale)) off);

  op_cost(10);
  format %{"[$reg + $off + $idx << $scale]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index($idx);
    scale($scale);
    disp($off);
  %}
%}

// Indirect Narrow Oop Plus Offset Operand
// Note: x86 architecture doesn't support "scale * index + offset" without a base
// we can't free r12 even with CompressedOops::base() == nullptr.
operand indCompressedOopOffset(rRegN reg, immL32 off) %{
  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));
  constraint(ALLOC_IN_RC(ptr_reg));
  match(AddP (DecodeN reg) off);

  op_cost(10);
  format %{"[R12 + $reg << 3 + $off] (compressed oop addressing)" %}
  interface(MEMORY_INTER) %{
    base(0xc); // R12
    index($reg);
    scale(0x3);
    disp($off);
  %}
%}

// Indirect Memory Operand
operand indirectNarrow(rRegN reg)
%{
  predicate(CompressedOops::shift() == 0);
  constraint(ALLOC_IN_RC(ptr_reg));
  match(DecodeN reg);

  format %{ "[$reg]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index(0x4);
    scale(0x0);
    disp(0x0);
  %}
%}

// Indirect Memory Plus Short Offset Operand
operand indOffset8Narrow(rRegN reg, immL8 off)
%{
  predicate(CompressedOops::shift() == 0);
  constraint(ALLOC_IN_RC(ptr_reg));
  match(AddP (DecodeN reg) off);

  format %{ "[$reg + $off (8-bit)]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index(0x4);
    scale(0x0);
    disp($off);
  %}
%}

// Indirect Memory Plus Long Offset Operand
operand indOffset32Narrow(rRegN reg, immL32 off)
%{
  predicate(CompressedOops::shift() == 0);
  constraint(ALLOC_IN_RC(ptr_reg));
  match(AddP (DecodeN reg) off);

  format %{ "[$reg + $off (32-bit)]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index(0x4);
    scale(0x0);
    disp($off);
  %}
%}

// Indirect Memory Plus Index Register Plus Offset Operand
operand indIndexOffsetNarrow(rRegN reg, rRegL lreg, immL32 off)
%{
  predicate(CompressedOops::shift() == 0);
  constraint(ALLOC_IN_RC(ptr_reg));
  match(AddP (AddP (DecodeN reg) lreg) off);

  op_cost(10);
  format %{"[$reg + $off + $lreg]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index($lreg);
    scale(0x0);
    disp($off);
  %}
%}

// Indirect Memory Plus Index Register Plus Offset Operand
operand indIndexNarrow(rRegN reg, rRegL lreg)
%{
  predicate(CompressedOops::shift() == 0);
  constraint(ALLOC_IN_RC(ptr_reg));
  match(AddP (DecodeN reg) lreg);

  op_cost(10);
  format %{"[$reg + $lreg]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index($lreg);
    scale(0x0);
    disp(0x0);
  %}
%}

// Indirect Memory Times Scale Plus Index Register
operand indIndexScaleNarrow(rRegN reg, rRegL lreg, immI2 scale)
%{
  predicate(CompressedOops::shift() == 0);
  constraint(ALLOC_IN_RC(ptr_reg));
  match(AddP (DecodeN reg) (LShiftL lreg scale));

  op_cost(10);
  format %{"[$reg + $lreg << $scale]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index($lreg);
    scale($scale);
    disp(0x0);
  %}
%}

// Indirect Memory Times Scale Plus Index Register Plus Offset Operand
operand indIndexScaleOffsetNarrow(rRegN reg, immL32 off, rRegL lreg, immI2 scale)
%{
  predicate(CompressedOops::shift() == 0);
  constraint(ALLOC_IN_RC(ptr_reg));
  match(AddP (AddP (DecodeN reg) (LShiftL lreg scale)) off);

  op_cost(10);
  format %{"[$reg + $off + $lreg << $scale]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index($lreg);
    scale($scale);
    disp($off);
  %}
%}

// Indirect Memory Times Plus Positive Index Register Plus Offset Operand
operand indPosIndexOffsetNarrow(rRegN reg, immL32 off, rRegI idx)
%{
  constraint(ALLOC_IN_RC(ptr_reg));
  predicate(CompressedOops::shift() == 0 && n->in(2)->in(3)->as_Type()->type()->is_long()->_lo >= 0);
  match(AddP (AddP (DecodeN reg) (ConvI2L idx)) off);

  op_cost(10);
  format %{"[$reg + $off + $idx]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index($idx);
    scale(0x0);
    disp($off);
  %}
%}

// Indirect Memory Times Scale Plus Positive Index Register Plus Offset Operand
operand indPosIndexScaleOffsetNarrow(rRegN reg, immL32 off, rRegI idx, immI2 scale)
%{
  constraint(ALLOC_IN_RC(ptr_reg));
  predicate(CompressedOops::shift() == 0 && n->in(2)->in(3)->in(1)->as_Type()->type()->is_long()->_lo >= 0);
  match(AddP (AddP (DecodeN reg) (LShiftL (ConvI2L idx) scale)) off);

  op_cost(10);
  format %{"[$reg + $off + $idx << $scale]" %}
  interface(MEMORY_INTER) %{
    base($reg);
    index($idx);
    scale($scale);
    disp($off);
  %}
%}

//----------Special Memory Operands--------------------------------------------
// Stack Slot Operand - This operand is used for loading and storing temporary
//                      values on the stack where a match requires a value to
//                      flow through memory.
operand stackSlotP(sRegP reg)
%{
  constraint(ALLOC_IN_RC(stack_slots));
  // No match rule because this operand is only generated in matching

  format %{ "[$reg]" %}
  interface(MEMORY_INTER) %{
    base(0x4);   // RSP
    index(0x4);  // No Index
    scale(0x0);  // No Scale
    disp($reg);  // Stack Offset
  %}
%}

operand stackSlotI(sRegI reg)
%{
  constraint(ALLOC_IN_RC(stack_slots));
  // No match rule because this operand is only generated in matching

  format %{ "[$reg]" %}
  interface(MEMORY_INTER) %{
    base(0x4);   // RSP
    index(0x4);  // No Index
    scale(0x0);  // No Scale
    disp($reg);  // Stack Offset
  %}
%}

operand stackSlotF(sRegF reg)
%{
  constraint(ALLOC_IN_RC(stack_slots));
  // No match rule because this operand is only generated in matching

  format %{ "[$reg]" %}
  interface(MEMORY_INTER) %{
    base(0x4);   // RSP
    index(0x4);  // No Index
    scale(0x0);  // No Scale
    disp($reg);  // Stack Offset
  %}
%}

operand stackSlotD(sRegD reg)
%{
  constraint(ALLOC_IN_RC(stack_slots));
  // No match rule because this operand is only generated in matching

  format %{ "[$reg]" %}
  interface(MEMORY_INTER) %{
    base(0x4);   // RSP
    index(0x4);  // No Index
    scale(0x0);  // No Scale
    disp($reg);  // Stack Offset
  %}
%}
operand stackSlotL(sRegL reg)
%{
  constraint(ALLOC_IN_RC(stack_slots));
  // No match rule because this operand is only generated in matching

  format %{ "[$reg]" %}
  interface(MEMORY_INTER) %{
    base(0x4);   // RSP
    index(0x4);  // No Index
    scale(0x0);  // No Scale
    disp($reg);  // Stack Offset
  %}
%}

//----------Conditional Branch Operands----------------------------------------
// Comparison Op  - This is the operation of the comparison, and is limited to
//                  the following set of codes:
//                  L (<), LE (<=), G (>), GE (>=), E (==), NE (!=)
//
// Other attributes of the comparison, such as unsignedness, are specified
// by the comparison instruction that sets a condition code flags register.
// That result is represented by a flags operand whose subtype is appropriate
// to the unsignedness (etc.) of the comparison.
//
// Later, the instruction which matches both the Comparison Op (a Bool) and
// the flags (produced by the Cmp) specifies the coding of the comparison op
// by matching a specific subtype of Bool operand below, such as cmpOpU.

// Comparison Code
operand cmpOp()
%{
  match(Bool);

  format %{ "" %}
  interface(COND_INTER) %{
    equal(0x4, "e");
    not_equal(0x5, "ne");
    less(0xC, "l");
    greater_equal(0xD, "ge");
    less_equal(0xE, "le");
    greater(0xF, "g");
    overflow(0x0, "o");
    no_overflow(0x1, "no");
  %}
%}

// Comparison Code, unsigned compare.  Used by FP also, with
// C2 (unordered) turned into GT or LT already.  The other bits
// C0 and C3 are turned into Carry & Zero flags.
operand cmpOpU()
%{
  match(Bool);

  format %{ "" %}
  interface(COND_INTER) %{
    equal(0x4, "e");
    not_equal(0x5, "ne");
    less(0x2, "b");
    greater_equal(0x3, "ae");
    less_equal(0x6, "be");
    greater(0x7, "a");
    overflow(0x0, "o");
    no_overflow(0x1, "no");
  %}
%}


// Floating comparisons that don't require any fixup for the unordered case,
// If both inputs of the comparison are the same, ZF is always set so we
// don't need to use cmpOpUCF2 for eq/ne
operand cmpOpUCF() %{
  match(Bool);
  predicate(n->as_Bool()->_test._test == BoolTest::lt ||
            n->as_Bool()->_test._test == BoolTest::ge ||
            n->as_Bool()->_test._test == BoolTest::le ||
            n->as_Bool()->_test._test == BoolTest::gt ||
            n->in(1)->in(1) == n->in(1)->in(2));
  format %{ "" %}
  interface(COND_INTER) %{
    equal(0xb, "np");
    not_equal(0xa, "p");
    less(0x2, "b");
    greater_equal(0x3, "ae");
    less_equal(0x6, "be");
    greater(0x7, "a");
    overflow(0x0, "o");
    no_overflow(0x1, "no");
  %}
%}


// Floating comparisons that can be fixed up with extra conditional jumps
operand cmpOpUCF2() %{
  match(Bool);
  predicate((n->as_Bool()->_test._test == BoolTest::ne ||
             n->as_Bool()->_test._test == BoolTest::eq) &&
            n->in(1)->in(1) != n->in(1)->in(2));
  format %{ "" %}
  interface(COND_INTER) %{
    equal(0x4, "e");
    not_equal(0x5, "ne");
    less(0x2, "b");
    greater_equal(0x3, "ae");
    less_equal(0x6, "be");
    greater(0x7, "a");
    overflow(0x0, "o");
    no_overflow(0x1, "no");
  %}
%}

// Operands for bound floating pointer register arguments
operand rxmm0() %{
  constraint(ALLOC_IN_RC(xmm0_reg));
  match(VecX);
  format%{%}
  interface(REG_INTER);
%}

// Vectors

// Dummy generic vector class. Should be used for all vector operands.
// Replaced with vec[SDXYZ] during post-selection pass.
operand vec() %{
  constraint(ALLOC_IN_RC(dynamic));
  match(VecX);
  match(VecY);
  match(VecZ);
  match(VecS);
  match(VecD);

  format %{ %}
  interface(REG_INTER);
%}

// Dummy generic legacy vector class. Should be used for all legacy vector operands.
// Replaced with legVec[SDXYZ] during post-selection cleanup.
// Note: legacy register class is used to avoid extra (unneeded in 32-bit VM)
// runtime code generation via reg_class_dynamic.
operand legVec() %{
  constraint(ALLOC_IN_RC(dynamic));
  match(VecX);
  match(VecY);
  match(VecZ);
  match(VecS);
  match(VecD);

  format %{ %}
  interface(REG_INTER);
%}

// Replaces vec during post-selection cleanup. See above.
operand vecS() %{
  constraint(ALLOC_IN_RC(vectors_reg_vlbwdq));
  match(VecS);

  format %{ %}
  interface(REG_INTER);
%}

// Replaces legVec during post-selection cleanup. See above.
operand legVecS() %{
  constraint(ALLOC_IN_RC(vectors_reg_legacy));
  match(VecS);

  format %{ %}
  interface(REG_INTER);
%}

// Replaces vec during post-selection cleanup. See above.
operand vecD() %{
  constraint(ALLOC_IN_RC(vectord_reg_vlbwdq));
  match(VecD);

  format %{ %}
  interface(REG_INTER);
%}

// Replaces legVec during post-selection cleanup. See above.
operand legVecD() %{
  constraint(ALLOC_IN_RC(vectord_reg_legacy));
  match(VecD);

  format %{ %}
  interface(REG_INTER);
%}

// Replaces vec during post-selection cleanup. See above.
operand vecX() %{
  constraint(ALLOC_IN_RC(vectorx_reg_vlbwdq));
  match(VecX);

  format %{ %}
  interface(REG_INTER);
%}

// Replaces legVec during post-selection cleanup. See above.
operand legVecX() %{
  constraint(ALLOC_IN_RC(vectorx_reg_legacy));
  match(VecX);

  format %{ %}
  interface(REG_INTER);
%}

// Replaces vec during post-selection cleanup. See above.
operand vecY() %{
  constraint(ALLOC_IN_RC(vectory_reg_vlbwdq));
  match(VecY);

  format %{ %}
  interface(REG_INTER);
%}

// Replaces legVec during post-selection cleanup. See above.
operand legVecY() %{
  constraint(ALLOC_IN_RC(vectory_reg_legacy));
  match(VecY);

  format %{ %}
  interface(REG_INTER);
%}

// Replaces vec during post-selection cleanup. See above.
operand vecZ() %{
  constraint(ALLOC_IN_RC(vectorz_reg));
  match(VecZ);

  format %{ %}
  interface(REG_INTER);
%}

// Replaces legVec during post-selection cleanup. See above.
operand legVecZ() %{
  constraint(ALLOC_IN_RC(vectorz_reg_legacy));
  match(VecZ);

  format %{ %}
  interface(REG_INTER);
%}

//----------OPERAND CLASSES----------------------------------------------------
// Operand Classes are groups of operands that are used as to simplify
// instruction definitions by not requiring the AD writer to specify separate
// instructions for every form of operand when the instruction accepts
// multiple operand types with the same basic encoding and format.  The classic
// case of this is memory operands.

opclass memory(indirect, indOffset8, indOffset32, indIndexOffset, indIndex,
               indIndexScale, indPosIndexScale, indIndexScaleOffset, indPosIndexOffset, indPosIndexScaleOffset,
               indCompressedOopOffset,
               indirectNarrow, indOffset8Narrow, indOffset32Narrow,
               indIndexOffsetNarrow, indIndexNarrow, indIndexScaleNarrow,
               indIndexScaleOffsetNarrow, indPosIndexOffsetNarrow, indPosIndexScaleOffsetNarrow);

//----------PIPELINE-----------------------------------------------------------
// Rules which define the behavior of the target architectures pipeline.
pipeline %{

//----------ATTRIBUTES---------------------------------------------------------
attributes %{
  variable_size_instructions;        // Fixed size instructions
  max_instructions_per_bundle = 3;   // Up to 3 instructions per bundle
  instruction_unit_size = 1;         // An instruction is 1 bytes long
  instruction_fetch_unit_size = 16;  // The processor fetches one line
  instruction_fetch_units = 1;       // of 16 bytes
%}

//----------RESOURCES----------------------------------------------------------
// Resources are the functional units available to the machine

// Generic P2/P3 pipeline
// 3 decoders, only D0 handles big operands; a "bundle" is the limit of
// 3 instructions decoded per cycle.
// 2 load/store ops per cycle, 1 branch, 1 FPU,
// 3 ALU op, only ALU0 handles mul instructions.
resources( D0, D1, D2, DECODE = D0 | D1 | D2,
           MS0, MS1, MS2, MEM = MS0 | MS1 | MS2,
           BR, FPU,
           ALU0, ALU1, ALU2, ALU = ALU0 | ALU1 | ALU2);

//----------PIPELINE DESCRIPTION-----------------------------------------------
// Pipeline Description specifies the stages in the machine's pipeline

// Generic P2/P3 pipeline
pipe_desc(S0, S1, S2, S3, S4, S5);

//----------PIPELINE CLASSES---------------------------------------------------
// Pipeline Classes describe the stages in which input and output are
// referenced by the hardware pipeline.

// Naming convention: ialu or fpu
// Then: _reg
// Then: _reg if there is a 2nd register
// Then: _long if it's a pair of instructions implementing a long
// Then: _fat if it requires the big decoder
//   Or: _mem if it requires the big decoder and a memory unit.

// Integer ALU reg operation
pipe_class ialu_reg(rRegI dst)
%{
    single_instruction;
    dst    : S4(write);
    dst    : S3(read);
    DECODE : S0;        // any decoder
    ALU    : S3;        // any alu
%}

// Long ALU reg operation
pipe_class ialu_reg_long(rRegL dst)
%{
    instruction_count(2);
    dst    : S4(write);
    dst    : S3(read);
    DECODE : S0(2);     // any 2 decoders
    ALU    : S3(2);     // both alus
%}

// Integer ALU reg operation using big decoder
pipe_class ialu_reg_fat(rRegI dst)
%{
    single_instruction;
    dst    : S4(write);
    dst    : S3(read);
    D0     : S0;        // big decoder only
    ALU    : S3;        // any alu
%}

// Integer ALU reg-reg operation
pipe_class ialu_reg_reg(rRegI dst, rRegI src)
%{
    single_instruction;
    dst    : S4(write);
    src    : S3(read);
    DECODE : S0;        // any decoder
    ALU    : S3;        // any alu
%}

// Integer ALU reg-reg operation
pipe_class ialu_reg_reg_fat(rRegI dst, memory src)
%{
    single_instruction;
    dst    : S4(write);
    src    : S3(read);
    D0     : S0;        // big decoder only
    ALU    : S3;        // any alu
%}

// Integer ALU reg-mem operation
pipe_class ialu_reg_mem(rRegI dst, memory mem)
%{
    single_instruction;
    dst    : S5(write);
    mem    : S3(read);
    D0     : S0;        // big decoder only
    ALU    : S4;        // any alu
    MEM    : S3;        // any mem
%}

// Integer mem operation (prefetch)
pipe_class ialu_mem(memory mem)
%{
    single_instruction;
    mem    : S3(read);
    D0     : S0;        // big decoder only
    MEM    : S3;        // any mem
%}

// Integer Store to Memory
pipe_class ialu_mem_reg(memory mem, rRegI src)
%{
    single_instruction;
    mem    : S3(read);
    src    : S5(read);
    D0     : S0;        // big decoder only
    ALU    : S4;        // any alu
    MEM    : S3;
%}

// // Long Store to Memory
// pipe_class ialu_mem_long_reg(memory mem, rRegL src)
// %{
//     instruction_count(2);
//     mem    : S3(read);
//     src    : S5(read);
//     D0     : S0(2);          // big decoder only; twice
//     ALU    : S4(2);     // any 2 alus
//     MEM    : S3(2);  // Both mems
// %}

// Integer Store to Memory
pipe_class ialu_mem_imm(memory mem)
%{
    single_instruction;
    mem    : S3(read);
    D0     : S0;        // big decoder only
    ALU    : S4;        // any alu
    MEM    : S3;
%}

// Integer ALU0 reg-reg operation
pipe_class ialu_reg_reg_alu0(rRegI dst, rRegI src)
%{
    single_instruction;
    dst    : S4(write);
    src    : S3(read);
    D0     : S0;        // Big decoder only
    ALU0   : S3;        // only alu0
%}

// Integer ALU0 reg-mem operation
pipe_class ialu_reg_mem_alu0(rRegI dst, memory mem)
%{
    single_instruction;
    dst    : S5(write);
    mem    : S3(read);
    D0     : S0;        // big decoder only
    ALU0   : S4;        // ALU0 only
    MEM    : S3;        // any mem
%}

// Integer ALU reg-reg operation
pipe_class ialu_cr_reg_reg(rFlagsReg cr, rRegI src1, rRegI src2)
%{
    single_instruction;
    cr     : S4(write);
    src1   : S3(read);
    src2   : S3(read);
    DECODE : S0;        // any decoder
    ALU    : S3;        // any alu
%}

// Integer ALU reg-imm operation
pipe_class ialu_cr_reg_imm(rFlagsReg cr, rRegI src1)
%{
    single_instruction;
    cr     : S4(write);
    src1   : S3(read);
    DECODE : S0;        // any decoder
    ALU    : S3;        // any alu
%}

// Integer ALU reg-mem operation
pipe_class ialu_cr_reg_mem(rFlagsReg cr, rRegI src1, memory src2)
%{
    single_instruction;
    cr     : S4(write);
    src1   : S3(read);
    src2   : S3(read);
    D0     : S0;        // big decoder only
    ALU    : S4;        // any alu
    MEM    : S3;
%}

// Conditional move reg-reg
pipe_class pipe_cmplt( rRegI p, rRegI q, rRegI y)
%{
    instruction_count(4);
    y      : S4(read);
    q      : S3(read);
    p      : S3(read);
    DECODE : S0(4);     // any decoder
%}

// Conditional move reg-reg
pipe_class pipe_cmov_reg( rRegI dst, rRegI src, rFlagsReg cr)
%{
    single_instruction;
    dst    : S4(write);
    src    : S3(read);
    cr     : S3(read);
    DECODE : S0;        // any decoder
%}

// Conditional move reg-mem
pipe_class pipe_cmov_mem( rFlagsReg cr, rRegI dst, memory src)
%{
    single_instruction;
    dst    : S4(write);
    src    : S3(read);
    cr     : S3(read);
    DECODE : S0;        // any decoder
    MEM    : S3;
%}

// Conditional move reg-reg long
pipe_class pipe_cmov_reg_long( rFlagsReg cr, rRegL dst, rRegL src)
%{
    single_instruction;
    dst    : S4(write);
    src    : S3(read);
    cr     : S3(read);
    DECODE : S0(2);     // any 2 decoders
%}

// Float reg-reg operation
pipe_class fpu_reg(regD dst)
%{
    instruction_count(2);
    dst    : S3(read);
    DECODE : S0(2);     // any 2 decoders
    FPU    : S3;
%}

// Float reg-reg operation
pipe_class fpu_reg_reg(regD dst, regD src)
%{
    instruction_count(2);
    dst    : S4(write);
    src    : S3(read);
    DECODE : S0(2);     // any 2 decoders
    FPU    : S3;
%}

// Float reg-reg operation
pipe_class fpu_reg_reg_reg(regD dst, regD src1, regD src2)
%{
    instruction_count(3);
    dst    : S4(write);
    src1   : S3(read);
    src2   : S3(read);
    DECODE : S0(3);     // any 3 decoders
    FPU    : S3(2);
%}

// Float reg-reg operation
pipe_class fpu_reg_reg_reg_reg(regD dst, regD src1, regD src2, regD src3)
%{
    instruction_count(4);
    dst    : S4(write);
    src1   : S3(read);
    src2   : S3(read);
    src3   : S3(read);
    DECODE : S0(4);     // any 3 decoders
    FPU    : S3(2);
%}

// Float reg-reg operation
pipe_class fpu_reg_mem_reg_reg(regD dst, memory src1, regD src2, regD src3)
%{
    instruction_count(4);
    dst    : S4(write);
    src1   : S3(read);
    src2   : S3(read);
    src3   : S3(read);
    DECODE : S1(3);     // any 3 decoders
    D0     : S0;        // Big decoder only
    FPU    : S3(2);
    MEM    : S3;
%}

// Float reg-mem operation
pipe_class fpu_reg_mem(regD dst, memory mem)
%{
    instruction_count(2);
    dst    : S5(write);
    mem    : S3(read);
    D0     : S0;        // big decoder only
    DECODE : S1;        // any decoder for FPU POP
    FPU    : S4;
    MEM    : S3;        // any mem
%}

// Float reg-mem operation
pipe_class fpu_reg_reg_mem(regD dst, regD src1, memory mem)
%{
    instruction_count(3);
    dst    : S5(write);
    src1   : S3(read);
    mem    : S3(read);
    D0     : S0;        // big decoder only
    DECODE : S1(2);     // any decoder for FPU POP
    FPU    : S4;
    MEM    : S3;        // any mem
%}

// Float mem-reg operation
pipe_class fpu_mem_reg(memory mem, regD src)
%{
    instruction_count(2);
    src    : S5(read);
    mem    : S3(read);
    DECODE : S0;        // any decoder for FPU PUSH
    D0     : S1;        // big decoder only
    FPU    : S4;
    MEM    : S3;        // any mem
%}

pipe_class fpu_mem_reg_reg(memory mem, regD src1, regD src2)
%{
    instruction_count(3);
    src1   : S3(read);
    src2   : S3(read);
    mem    : S3(read);
    DECODE : S0(2);     // any decoder for FPU PUSH
    D0     : S1;        // big decoder only
    FPU    : S4;
    MEM    : S3;        // any mem
%}

pipe_class fpu_mem_reg_mem(memory mem, regD src1, memory src2)
%{
    instruction_count(3);
    src1   : S3(read);
    src2   : S3(read);
    mem    : S4(read);
    DECODE : S0;        // any decoder for FPU PUSH
    D0     : S0(2);     // big decoder only
    FPU    : S4;
    MEM    : S3(2);     // any mem
%}

pipe_class fpu_mem_mem(memory dst, memory src1)
%{
    instruction_count(2);
    src1   : S3(read);
    dst    : S4(read);
    D0     : S0(2);     // big decoder only
    MEM    : S3(2);     // any mem
%}

pipe_class fpu_mem_mem_mem(memory dst, memory src1, memory src2)
%{
    instruction_count(3);
    src1   : S3(read);
    src2   : S3(read);
    dst    : S4(read);
    D0     : S0(3);     // big decoder only
    FPU    : S4;
    MEM    : S3(3);     // any mem
%}

pipe_class fpu_mem_reg_con(memory mem, regD src1)
%{
    instruction_count(3);
    src1   : S4(read);
    mem    : S4(read);
    DECODE : S0;        // any decoder for FPU PUSH
    D0     : S0(2);     // big decoder only
    FPU    : S4;
    MEM    : S3(2);     // any mem
%}

// Float load constant
pipe_class fpu_reg_con(regD dst)
%{
    instruction_count(2);
    dst    : S5(write);
    D0     : S0;        // big decoder only for the load
    DECODE : S1;        // any decoder for FPU POP
    FPU    : S4;
    MEM    : S3;        // any mem
%}

// Float load constant
pipe_class fpu_reg_reg_con(regD dst, regD src)
%{
    instruction_count(3);
    dst    : S5(write);
    src    : S3(read);
    D0     : S0;        // big decoder only for the load
    DECODE : S1(2);     // any decoder for FPU POP
    FPU    : S4;
    MEM    : S3;        // any mem
%}

// UnConditional branch
pipe_class pipe_jmp(label labl)
%{
    single_instruction;
    BR   : S3;
%}

// Conditional branch
pipe_class pipe_jcc(cmpOp cmp, rFlagsReg cr, label labl)
%{
    single_instruction;
    cr    : S1(read);
    BR    : S3;
%}

// Allocation idiom
pipe_class pipe_cmpxchg(rRegP dst, rRegP heap_ptr)
%{
    instruction_count(1); force_serialization;
    fixed_latency(6);
    heap_ptr : S3(read);
    DECODE   : S0(3);
    D0       : S2;
    MEM      : S3;
    ALU      : S3(2);
    dst      : S5(write);
    BR       : S5;
%}

// Generic big/slow expanded idiom
pipe_class pipe_slow()
%{
    instruction_count(10); multiple_bundles; force_serialization;
    fixed_latency(100);
    D0  : S0(2);
    MEM : S3(2);
%}

// The real do-nothing guy
pipe_class empty()
%{
    instruction_count(0);
%}

// Define the class for the Nop node
define
%{
   MachNop = empty;
%}

%}

//----------INSTRUCTIONS-------------------------------------------------------
//
// match      -- States which machine-independent subtree may be replaced
//               by this instruction.
// ins_cost   -- The estimated cost of this instruction is used by instruction
//               selection to identify a minimum cost tree of machine
//               instructions that matches a tree of machine-independent
//               instructions.
// format     -- A string providing the disassembly for this instruction.
//               The value of an instruction's operand may be inserted
//               by referring to it with a '$' prefix.
// opcode     -- Three instruction opcodes may be provided.  These are referred
//               to within an encode class as $primary, $secondary, and $tertiary
//               rrspectively.  The primary opcode is commonly used to
//               indicate the type of machine instruction, while secondary
//               and tertiary are often used for prefix options or addressing
//               modes.
// ins_encode -- A list of encode classes with parameters. The encode class
//               name must have been defined in an 'enc_class' specification
//               in the encode section of the architecture description.

// ============================================================================

instruct ShouldNotReachHere() %{
  match(Halt);
  format %{ "stop\t# ShouldNotReachHere" %}
  ins_encode %{
    if (is_reachable()) {
      const char* str = __ code_string(_halt_reason);
      __ stop(str);
    }
  %}
  ins_pipe(pipe_slow);
%}

// ============================================================================

// Dummy reg-to-reg vector moves. Removed during post-selection cleanup.
// Load Float
instruct MoveF2VL(vlRegF dst, regF src) %{
  match(Set dst src);
  format %{ "movss $dst,$src\t! load float (4 bytes)" %}
  ins_encode %{
    ShouldNotReachHere();
  %}
  ins_pipe( fpu_reg_reg );
%}

// Load Float
instruct MoveF2LEG(legRegF dst, regF src) %{
  match(Set dst src);
  format %{ "movss $dst,$src\t# if src != dst load float (4 bytes)" %}
  ins_encode %{
    ShouldNotReachHere();
  %}
  ins_pipe( fpu_reg_reg );
%}

// Load Float
instruct MoveVL2F(regF dst, vlRegF src) %{
  match(Set dst src);
  format %{ "movss $dst,$src\t! load float (4 bytes)" %}
  ins_encode %{
    ShouldNotReachHere();
  %}
  ins_pipe( fpu_reg_reg );
%}

// Load Float
instruct MoveLEG2F(regF dst, legRegF src) %{
  match(Set dst src);
  format %{ "movss $dst,$src\t# if src != dst load float (4 bytes)" %}
  ins_encode %{
    ShouldNotReachHere();
  %}
  ins_pipe( fpu_reg_reg );
%}

// Load Double
instruct MoveD2VL(vlRegD dst, regD src) %{
  match(Set dst src);
  format %{ "movsd $dst,$src\t! load double (8 bytes)" %}
  ins_encode %{
    ShouldNotReachHere();
  %}
  ins_pipe( fpu_reg_reg );
%}

// Load Double
instruct MoveD2LEG(legRegD dst, regD src) %{
  match(Set dst src);
  format %{ "movsd $dst,$src\t# if src != dst load double (8 bytes)" %}
  ins_encode %{
    ShouldNotReachHere();
  %}
  ins_pipe( fpu_reg_reg );
%}

// Load Double
instruct MoveVL2D(regD dst, vlRegD src) %{
  match(Set dst src);
  format %{ "movsd $dst,$src\t! load double (8 bytes)" %}
  ins_encode %{
    ShouldNotReachHere();
  %}
  ins_pipe( fpu_reg_reg );
%}

// Load Double
instruct MoveLEG2D(regD dst, legRegD src) %{
  match(Set dst src);
  format %{ "movsd $dst,$src\t# if src != dst load double (8 bytes)" %}
  ins_encode %{
    ShouldNotReachHere();
  %}
  ins_pipe( fpu_reg_reg );
%}

//----------Load/Store/Move Instructions---------------------------------------
//----------Load Instructions--------------------------------------------------

// Load Byte (8 bit signed)
instruct loadB(rRegI dst, memory mem)
%{
  match(Set dst (LoadB mem));

  ins_cost(125);
  format %{ "movsbl  $dst, $mem\t# byte" %}

  ins_encode %{
    __ movsbl($dst$$Register, $mem$$Address);
  %}

  ins_pipe(ialu_reg_mem);
%}

// Load Byte (8 bit signed) into Long Register
instruct loadB2L(rRegL dst, memory mem)
%{
  match(Set dst (ConvI2L (LoadB mem)));

  ins_cost(125);
  format %{ "movsbq  $dst, $mem\t# byte -> long" %}

  ins_encode %{
    __ movsbq($dst$$Register, $mem$$Address);
  %}

  ins_pipe(ialu_reg_mem);
%}

// Load Unsigned Byte (8 bit UNsigned)
instruct loadUB(rRegI dst, memory mem)
%{
  match(Set dst (LoadUB mem));

  ins_cost(125);
  format %{ "movzbl  $dst, $mem\t# ubyte" %}

  ins_encode %{
    __ movzbl($dst$$Register, $mem$$Address);
  %}

  ins_pipe(ialu_reg_mem);
%}

// Load Unsigned Byte (8 bit UNsigned) into Long Register
instruct loadUB2L(rRegL dst, memory mem)
%{
  match(Set dst (ConvI2L (LoadUB mem)));

  ins_cost(125);
  format %{ "movzbq  $dst, $mem\t# ubyte -> long" %}

  ins_encode %{
    __ movzbq($dst$$Register, $mem$$Address);
  %}

  ins_pipe(ialu_reg_mem);
%}

// Load Unsigned Byte (8 bit UNsigned) with 32-bit mask into Long Register
instruct loadUB2L_immI(rRegL dst, memory mem, immI mask, rFlagsReg cr) %{
  match(Set dst (ConvI2L (AndI (LoadUB mem) mask)));
  effect(KILL cr);

  format %{ "movzbq  $dst, $mem\t# ubyte & 32-bit mask -> long\n\t"
            "andl    $dst, right_n_bits($mask, 8)" %}
  ins_encode %{
    Register Rdst = $dst$$Register;
    __ movzbq(Rdst, $mem$$Address);
    __ andl(Rdst, $mask$$constant & right_n_bits(8));
  %}
  ins_pipe(ialu_reg_mem);
%}

// Load Short (16 bit signed)
instruct loadS(rRegI dst, memory mem)
%{
  match(Set dst (LoadS mem));

  ins_cost(125);
  format %{ "movswl $dst, $mem\t# short" %}

  ins_encode %{
    __ movswl($dst$$Register, $mem$$Address);
  %}

  ins_pipe(ialu_reg_mem);
%}

// Load Short (16 bit signed) to Byte (8 bit signed)
instruct loadS2B(rRegI dst, memory mem, immI_24 twentyfour) %{
  match(Set dst (RShiftI (LShiftI (LoadS mem) twentyfour) twentyfour));

  ins_cost(125);
  format %{ "movsbl $dst, $mem\t# short -> byte" %}
  ins_encode %{
    __ movsbl($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Load Short (16 bit signed) into Long Register
instruct loadS2L(rRegL dst, memory mem)
%{
  match(Set dst (ConvI2L (LoadS mem)));

  ins_cost(125);
  format %{ "movswq $dst, $mem\t# short -> long" %}

  ins_encode %{
    __ movswq($dst$$Register, $mem$$Address);
  %}

  ins_pipe(ialu_reg_mem);
%}

// Load Unsigned Short/Char (16 bit UNsigned)
instruct loadUS(rRegI dst, memory mem)
%{
  match(Set dst (LoadUS mem));

  ins_cost(125);
  format %{ "movzwl  $dst, $mem\t# ushort/char" %}

  ins_encode %{
    __ movzwl($dst$$Register, $mem$$Address);
  %}

  ins_pipe(ialu_reg_mem);
%}

// Load Unsigned Short/Char (16 bit UNsigned) to Byte (8 bit signed)
instruct loadUS2B(rRegI dst, memory mem, immI_24 twentyfour) %{
  match(Set dst (RShiftI (LShiftI (LoadUS mem) twentyfour) twentyfour));

  ins_cost(125);
  format %{ "movsbl $dst, $mem\t# ushort -> byte" %}
  ins_encode %{
    __ movsbl($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Load Unsigned Short/Char (16 bit UNsigned) into Long Register
instruct loadUS2L(rRegL dst, memory mem)
%{
  match(Set dst (ConvI2L (LoadUS mem)));

  ins_cost(125);
  format %{ "movzwq  $dst, $mem\t# ushort/char -> long" %}

  ins_encode %{
    __ movzwq($dst$$Register, $mem$$Address);
  %}

  ins_pipe(ialu_reg_mem);
%}

// Load Unsigned Short/Char (16 bit UNsigned) with mask 0xFF into Long Register
instruct loadUS2L_immI_255(rRegL dst, memory mem, immI_255 mask) %{
  match(Set dst (ConvI2L (AndI (LoadUS mem) mask)));

  format %{ "movzbq  $dst, $mem\t# ushort/char & 0xFF -> long" %}
  ins_encode %{
    __ movzbq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Load Unsigned Short/Char (16 bit UNsigned) with 32-bit mask into Long Register
instruct loadUS2L_immI(rRegL dst, memory mem, immI mask, rFlagsReg cr) %{
  match(Set dst (ConvI2L (AndI (LoadUS mem) mask)));
  effect(KILL cr);

  format %{ "movzwq  $dst, $mem\t# ushort/char & 32-bit mask -> long\n\t"
            "andl    $dst, right_n_bits($mask, 16)" %}
  ins_encode %{
    Register Rdst = $dst$$Register;
    __ movzwq(Rdst, $mem$$Address);
    __ andl(Rdst, $mask$$constant & right_n_bits(16));
  %}
  ins_pipe(ialu_reg_mem);
%}

// Load Integer
instruct loadI(rRegI dst, memory mem)
%{
  match(Set dst (LoadI mem));

  ins_cost(125);
  format %{ "movl    $dst, $mem\t# int" %}

  ins_encode %{
    __ movl($dst$$Register, $mem$$Address);
  %}

  ins_pipe(ialu_reg_mem);
%}

// Load Integer (32 bit signed) to Byte (8 bit signed)
instruct loadI2B(rRegI dst, memory mem, immI_24 twentyfour) %{
  match(Set dst (RShiftI (LShiftI (LoadI mem) twentyfour) twentyfour));

  ins_cost(125);
  format %{ "movsbl  $dst, $mem\t# int -> byte" %}
  ins_encode %{
    __ movsbl($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Load Integer (32 bit signed) to Unsigned Byte (8 bit UNsigned)
instruct loadI2UB(rRegI dst, memory mem, immI_255 mask) %{
  match(Set dst (AndI (LoadI mem) mask));

  ins_cost(125);
  format %{ "movzbl  $dst, $mem\t# int -> ubyte" %}
  ins_encode %{
    __ movzbl($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Load Integer (32 bit signed) to Short (16 bit signed)
instruct loadI2S(rRegI dst, memory mem, immI_16 sixteen) %{
  match(Set dst (RShiftI (LShiftI (LoadI mem) sixteen) sixteen));

  ins_cost(125);
  format %{ "movswl  $dst, $mem\t# int -> short" %}
  ins_encode %{
    __ movswl($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Load Integer (32 bit signed) to Unsigned Short/Char (16 bit UNsigned)
instruct loadI2US(rRegI dst, memory mem, immI_65535 mask) %{
  match(Set dst (AndI (LoadI mem) mask));

  ins_cost(125);
  format %{ "movzwl  $dst, $mem\t# int -> ushort/char" %}
  ins_encode %{
    __ movzwl($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Load Integer into Long Register
instruct loadI2L(rRegL dst, memory mem)
%{
  match(Set dst (ConvI2L (LoadI mem)));

  ins_cost(125);
  format %{ "movslq  $dst, $mem\t# int -> long" %}

  ins_encode %{
    __ movslq($dst$$Register, $mem$$Address);
  %}

  ins_pipe(ialu_reg_mem);
%}

// Load Integer with mask 0xFF into Long Register
instruct loadI2L_immI_255(rRegL dst, memory mem, immI_255 mask) %{
  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));

  format %{ "movzbq  $dst, $mem\t# int & 0xFF -> long" %}
  ins_encode %{
    __ movzbq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Load Integer with mask 0xFFFF into Long Register
instruct loadI2L_immI_65535(rRegL dst, memory mem, immI_65535 mask) %{
  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));

  format %{ "movzwq  $dst, $mem\t# int & 0xFFFF -> long" %}
  ins_encode %{
    __ movzwq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Load Integer with a 31-bit mask into Long Register
instruct loadI2L_immU31(rRegL dst, memory mem, immU31 mask, rFlagsReg cr) %{
  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));
  effect(KILL cr);

  format %{ "movl    $dst, $mem\t# int & 31-bit mask -> long\n\t"
            "andl    $dst, $mask" %}
  ins_encode %{
    Register Rdst = $dst$$Register;
    __ movl(Rdst, $mem$$Address);
    __ andl(Rdst, $mask$$constant);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Load Unsigned Integer into Long Register
instruct loadUI2L(rRegL dst, memory mem, immL_32bits mask)
%{
  match(Set dst (AndL (ConvI2L (LoadI mem)) mask));

  ins_cost(125);
  format %{ "movl    $dst, $mem\t# uint -> long" %}

  ins_encode %{
    __ movl($dst$$Register, $mem$$Address);
  %}

  ins_pipe(ialu_reg_mem);
%}

// Load Long
instruct loadL(rRegL dst, memory mem)
%{
  match(Set dst (LoadL mem));

  ins_cost(125);
  format %{ "movq    $dst, $mem\t# long" %}

  ins_encode %{
    __ movq($dst$$Register, $mem$$Address);
  %}

  ins_pipe(ialu_reg_mem); // XXX
%}

// Load Range
instruct loadRange(rRegI dst, memory mem)
%{
  match(Set dst (LoadRange mem));

  ins_cost(125); // XXX
  format %{ "movl    $dst, $mem\t# range" %}
  ins_encode %{
    __ movl($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Load Pointer
instruct loadP(rRegP dst, memory mem)
%{
  match(Set dst (LoadP mem));
  predicate(n->as_Load()->barrier_data() == 0);

  ins_cost(125); // XXX
  format %{ "movq    $dst, $mem\t# ptr" %}
  ins_encode %{
    __ movq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_mem); // XXX
%}

// Load Compressed Pointer
instruct loadN(rRegN dst, memory mem)
%{
   predicate(n->as_Load()->barrier_data() == 0);
   match(Set dst (LoadN mem));

   ins_cost(125); // XXX
   format %{ "movl    $dst, $mem\t# compressed ptr" %}
   ins_encode %{
     __ movl($dst$$Register, $mem$$Address);
   %}
   ins_pipe(ialu_reg_mem); // XXX
%}


// Load Klass Pointer
instruct loadKlass(rRegP dst, memory mem)
%{
  match(Set dst (LoadKlass mem));

  ins_cost(125); // XXX
  format %{ "movq    $dst, $mem\t# class" %}
  ins_encode %{
    __ movq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_mem); // XXX
%}

// Load narrow Klass Pointer
instruct loadNKlass(rRegN dst, memory mem)
%{
  predicate(!UseCompactObjectHeaders);
  match(Set dst (LoadNKlass mem));

  ins_cost(125); // XXX
  format %{ "movl    $dst, $mem\t# compressed klass ptr" %}
  ins_encode %{
    __ movl($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_mem); // XXX
%}

instruct loadNKlassCompactHeaders(rRegN dst, memory mem, rFlagsReg cr)
%{
  predicate(UseCompactObjectHeaders);
  match(Set dst (LoadNKlass mem));
  effect(KILL cr);
  ins_cost(125);
  format %{
    "movl    $dst, $mem\t# compressed klass ptr, shifted\n\t"
    "shrl    $dst, markWord::klass_shift_at_offset"
  %}
  ins_encode %{
    if (UseAPX) {
      __ eshrl($dst$$Register, $mem$$Address, markWord::klass_shift_at_offset, false);
    }
    else {
      __ movl($dst$$Register, $mem$$Address);
      __ shrl($dst$$Register, markWord::klass_shift_at_offset);
    }
  %}
  ins_pipe(ialu_reg_mem);
%}

// Load Float
instruct loadF(regF dst, memory mem)
%{
  match(Set dst (LoadF mem));

  ins_cost(145); // XXX
  format %{ "movss   $dst, $mem\t# float" %}
  ins_encode %{
    __ movflt($dst$$XMMRegister, $mem$$Address);
  %}
  ins_pipe(pipe_slow); // XXX
%}

// Load Double
instruct loadD_partial(regD dst, memory mem)
%{
  predicate(!UseXmmLoadAndClearUpper);
  match(Set dst (LoadD mem));

  ins_cost(145); // XXX
  format %{ "movlpd  $dst, $mem\t# double" %}
  ins_encode %{
    __ movdbl($dst$$XMMRegister, $mem$$Address);
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct loadD(regD dst, memory mem)
%{
  predicate(UseXmmLoadAndClearUpper);
  match(Set dst (LoadD mem));

  ins_cost(145); // XXX
  format %{ "movsd   $dst, $mem\t# double" %}
  ins_encode %{
    __ movdbl($dst$$XMMRegister, $mem$$Address);
  %}
  ins_pipe(pipe_slow); // XXX
%}

// max = java.lang.Math.max(float a, float b)
instruct maxF_avx10_reg(regF dst, regF a, regF b) %{
  predicate(VM_Version::supports_avx10_2());
  match(Set dst (MaxF a b));
  format %{ "maxF $dst, $a, $b" %}
  ins_encode %{
    __ eminmaxss($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MAX_COMPARE_SIGN);
  %}
  ins_pipe( pipe_slow );
%}

// max = java.lang.Math.max(float a, float b)
instruct maxF_reg(legRegF dst, legRegF a, legRegF b, legRegF tmp, legRegF atmp, legRegF btmp) %{
  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));
  match(Set dst (MaxF a b));
  effect(USE a, USE b, TEMP tmp, TEMP atmp, TEMP btmp);
  format %{ "maxF $dst, $a, $b \t! using $tmp, $atmp and $btmp as TEMP" %}
  ins_encode %{
    __ vminmax_fp(Op_MaxV, T_FLOAT, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);
  %}
  ins_pipe( pipe_slow );
%}

instruct maxF_reduction_reg(legRegF dst, legRegF a, legRegF b, legRegF xtmp, rRegI rtmp, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));
  match(Set dst (MaxF a b));
  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);

  format %{ "maxF_reduction $dst, $a, $b \t!using $xtmp and $rtmp as TEMP" %}
  ins_encode %{
    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,
                    false /*min*/, true /*single*/);
  %}
  ins_pipe( pipe_slow );
%}

// max = java.lang.Math.max(double a, double b)
instruct maxD_avx10_reg(regD dst, regD a, regD b) %{
  predicate(VM_Version::supports_avx10_2());
  match(Set dst (MaxD a b));
  format %{ "maxD $dst, $a, $b" %}
  ins_encode %{
    __ eminmaxsd($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MAX_COMPARE_SIGN);
  %}
  ins_pipe( pipe_slow );
%}

// max = java.lang.Math.max(double a, double b)
instruct maxD_reg(legRegD dst, legRegD a, legRegD b, legRegD tmp, legRegD atmp, legRegD btmp) %{
  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));
  match(Set dst (MaxD a b));
  effect(USE a, USE b, TEMP atmp, TEMP btmp, TEMP tmp);
  format %{ "maxD $dst, $a, $b \t! using $tmp, $atmp and $btmp as TEMP" %}
  ins_encode %{
    __ vminmax_fp(Op_MaxV, T_DOUBLE, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);
  %}
  ins_pipe( pipe_slow );
%}

instruct maxD_reduction_reg(legRegD dst, legRegD a, legRegD b, legRegD xtmp, rRegL rtmp, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));
  match(Set dst (MaxD a b));
  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);

  format %{ "maxD_reduction $dst, $a, $b \t! using $xtmp and $rtmp as TEMP" %}
  ins_encode %{
    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,
                    false /*min*/, false /*single*/);
  %}
  ins_pipe( pipe_slow );
%}

// max = java.lang.Math.min(float a, float b)
instruct minF_avx10_reg(regF dst, regF a, regF b) %{
  predicate(VM_Version::supports_avx10_2());
  match(Set dst (MinF a b));
  format %{ "minF $dst, $a, $b" %}
  ins_encode %{
    __ eminmaxss($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MIN_COMPARE_SIGN);
  %}
  ins_pipe( pipe_slow );
%}

// min = java.lang.Math.min(float a, float b)
instruct minF_reg(legRegF dst, legRegF a, legRegF b, legRegF tmp, legRegF atmp, legRegF btmp) %{
  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));
  match(Set dst (MinF a b));
  effect(USE a, USE b, TEMP tmp, TEMP atmp, TEMP btmp);
  format %{ "minF $dst, $a, $b \t! using $tmp, $atmp and $btmp as TEMP" %}
  ins_encode %{
    __ vminmax_fp(Op_MinV, T_FLOAT, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);
  %}
  ins_pipe( pipe_slow );
%}

instruct minF_reduction_reg(legRegF dst, legRegF a, legRegF b, legRegF xtmp, rRegI rtmp, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));
  match(Set dst (MinF a b));
  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);

  format %{ "minF_reduction $dst, $a, $b \t! using $xtmp and $rtmp as TEMP" %}
  ins_encode %{
    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,
                    true /*min*/, true /*single*/);
  %}
  ins_pipe( pipe_slow );
%}

// max = java.lang.Math.min(double a, double b)
instruct minD_avx10_reg(regD dst, regD a, regD b) %{
  predicate(VM_Version::supports_avx10_2());
  match(Set dst (MinD a b));
  format %{ "minD $dst, $a, $b" %}
  ins_encode %{
    __ eminmaxsd($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MIN_COMPARE_SIGN);
  %}
  ins_pipe( pipe_slow );
%}

// min = java.lang.Math.min(double a, double b)
instruct minD_reg(legRegD dst, legRegD a, legRegD b, legRegD tmp, legRegD atmp, legRegD btmp) %{
  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));
  match(Set dst (MinD a b));
  effect(USE a, USE b, TEMP tmp, TEMP atmp, TEMP btmp);
    format %{ "minD $dst, $a, $b \t! using $tmp, $atmp and $btmp as TEMP" %}
  ins_encode %{
    __ vminmax_fp(Op_MinV, T_DOUBLE, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);
  %}
  ins_pipe( pipe_slow );
%}

instruct minD_reduction_reg(legRegD dst, legRegD a, legRegD b, legRegD xtmp, rRegL rtmp, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));
  match(Set dst (MinD a b));
  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);

  format %{ "maxD_reduction $dst, $a, $b \t! using $xtmp and $rtmp as TEMP" %}
  ins_encode %{
    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,
                    true /*min*/, false /*single*/);
  %}
  ins_pipe( pipe_slow );
%}

// Load Effective Address
instruct leaP8(rRegP dst, indOffset8 mem)
%{
  match(Set dst mem);

  ins_cost(110); // XXX
  format %{ "leaq    $dst, $mem\t# ptr 8" %}
  ins_encode %{
    __ leaq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_reg_fat);
%}

instruct leaP32(rRegP dst, indOffset32 mem)
%{
  match(Set dst mem);

  ins_cost(110);
  format %{ "leaq    $dst, $mem\t# ptr 32" %}
  ins_encode %{
    __ leaq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_reg_fat);
%}

instruct leaPIdxOff(rRegP dst, indIndexOffset mem)
%{
  match(Set dst mem);

  ins_cost(110);
  format %{ "leaq    $dst, $mem\t# ptr idxoff" %}
  ins_encode %{
    __ leaq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_reg_fat);
%}

instruct leaPIdxScale(rRegP dst, indIndexScale mem)
%{
  match(Set dst mem);

  ins_cost(110);
  format %{ "leaq    $dst, $mem\t# ptr idxscale" %}
  ins_encode %{
    __ leaq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_reg_fat);
%}

instruct leaPPosIdxScale(rRegP dst, indPosIndexScale mem)
%{
  match(Set dst mem);

  ins_cost(110);
  format %{ "leaq    $dst, $mem\t# ptr idxscale" %}
  ins_encode %{
    __ leaq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_reg_fat);
%}

instruct leaPIdxScaleOff(rRegP dst, indIndexScaleOffset mem)
%{
  match(Set dst mem);

  ins_cost(110);
  format %{ "leaq    $dst, $mem\t# ptr idxscaleoff" %}
  ins_encode %{
    __ leaq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_reg_fat);
%}

instruct leaPPosIdxOff(rRegP dst, indPosIndexOffset mem)
%{
  match(Set dst mem);

  ins_cost(110);
  format %{ "leaq    $dst, $mem\t# ptr posidxoff" %}
  ins_encode %{
    __ leaq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_reg_fat);
%}

instruct leaPPosIdxScaleOff(rRegP dst, indPosIndexScaleOffset mem)
%{
  match(Set dst mem);

  ins_cost(110);
  format %{ "leaq    $dst, $mem\t# ptr posidxscaleoff" %}
  ins_encode %{
    __ leaq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_reg_fat);
%}

// Load Effective Address which uses Narrow (32-bits) oop
instruct leaPCompressedOopOffset(rRegP dst, indCompressedOopOffset mem)
%{
  predicate(UseCompressedOops && (CompressedOops::shift() != 0));
  match(Set dst mem);

  ins_cost(110);
  format %{ "leaq    $dst, $mem\t# ptr compressedoopoff32" %}
  ins_encode %{
    __ leaq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_reg_fat);
%}

instruct leaP8Narrow(rRegP dst, indOffset8Narrow mem)
%{
  predicate(CompressedOops::shift() == 0);
  match(Set dst mem);

  ins_cost(110); // XXX
  format %{ "leaq    $dst, $mem\t# ptr off8narrow" %}
  ins_encode %{
    __ leaq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_reg_fat);
%}

instruct leaP32Narrow(rRegP dst, indOffset32Narrow mem)
%{
  predicate(CompressedOops::shift() == 0);
  match(Set dst mem);

  ins_cost(110);
  format %{ "leaq    $dst, $mem\t# ptr off32narrow" %}
  ins_encode %{
    __ leaq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_reg_fat);
%}

instruct leaPIdxOffNarrow(rRegP dst, indIndexOffsetNarrow mem)
%{
  predicate(CompressedOops::shift() == 0);
  match(Set dst mem);

  ins_cost(110);
  format %{ "leaq    $dst, $mem\t# ptr idxoffnarrow" %}
  ins_encode %{
    __ leaq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_reg_fat);
%}

instruct leaPIdxScaleNarrow(rRegP dst, indIndexScaleNarrow mem)
%{
  predicate(CompressedOops::shift() == 0);
  match(Set dst mem);

  ins_cost(110);
  format %{ "leaq    $dst, $mem\t# ptr idxscalenarrow" %}
  ins_encode %{
    __ leaq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_reg_fat);
%}

instruct leaPIdxScaleOffNarrow(rRegP dst, indIndexScaleOffsetNarrow mem)
%{
  predicate(CompressedOops::shift() == 0);
  match(Set dst mem);

  ins_cost(110);
  format %{ "leaq    $dst, $mem\t# ptr idxscaleoffnarrow" %}
  ins_encode %{
    __ leaq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_reg_fat);
%}

instruct leaPPosIdxOffNarrow(rRegP dst, indPosIndexOffsetNarrow mem)
%{
  predicate(CompressedOops::shift() == 0);
  match(Set dst mem);

  ins_cost(110);
  format %{ "leaq    $dst, $mem\t# ptr posidxoffnarrow" %}
  ins_encode %{
    __ leaq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_reg_fat);
%}

instruct leaPPosIdxScaleOffNarrow(rRegP dst, indPosIndexScaleOffsetNarrow mem)
%{
  predicate(CompressedOops::shift() == 0);
  match(Set dst mem);

  ins_cost(110);
  format %{ "leaq    $dst, $mem\t# ptr posidxscaleoffnarrow" %}
  ins_encode %{
    __ leaq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg_reg_fat);
%}

instruct loadConI(rRegI dst, immI src)
%{
  match(Set dst src);

  format %{ "movl    $dst, $src\t# int" %}
  ins_encode %{
    __ movl($dst$$Register, $src$$constant);
  %}
  ins_pipe(ialu_reg_fat); // XXX
%}

instruct loadConI0(rRegI dst, immI_0 src, rFlagsReg cr)
%{
  match(Set dst src);
  effect(KILL cr);

  ins_cost(50);
  format %{ "xorl    $dst, $dst\t# int" %}
  ins_encode %{
    __ xorl($dst$$Register, $dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct loadConL(rRegL dst, immL src)
%{
  match(Set dst src);

  ins_cost(150);
  format %{ "movq    $dst, $src\t# long" %}
  ins_encode %{
    __ mov64($dst$$Register, $src$$constant);
  %}
  ins_pipe(ialu_reg);
%}

instruct loadConL0(rRegL dst, immL0 src, rFlagsReg cr)
%{
  match(Set dst src);
  effect(KILL cr);

  ins_cost(50);
  format %{ "xorl    $dst, $dst\t# long" %}
  ins_encode %{
    __ xorl($dst$$Register, $dst$$Register);
  %}
  ins_pipe(ialu_reg); // XXX
%}

instruct loadConUL32(rRegL dst, immUL32 src)
%{
  match(Set dst src);

  ins_cost(60);
  format %{ "movl    $dst, $src\t# long (unsigned 32-bit)" %}
  ins_encode %{
    __ movl($dst$$Register, $src$$constant);
  %}
  ins_pipe(ialu_reg);
%}

instruct loadConL32(rRegL dst, immL32 src)
%{
  match(Set dst src);

  ins_cost(70);
  format %{ "movq    $dst, $src\t# long (32-bit)" %}
  ins_encode %{
    __ movq($dst$$Register, $src$$constant);
  %}
  ins_pipe(ialu_reg);
%}

instruct loadConP(rRegP dst, immP con) %{
  match(Set dst con);

  format %{ "movq    $dst, $con\t# ptr" %}
  ins_encode %{
    __ mov64($dst$$Register, $con$$constant, $con->constant_reloc(), RELOC_IMM64);
  %}
  ins_pipe(ialu_reg_fat); // XXX
%}

instruct loadConP0(rRegP dst, immP0 src, rFlagsReg cr)
%{
  match(Set dst src);
  effect(KILL cr);

  ins_cost(50);
  format %{ "xorl    $dst, $dst\t# ptr" %}
  ins_encode %{
    __ xorl($dst$$Register, $dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct loadConP31(rRegP dst, immP31 src, rFlagsReg cr)
%{
  match(Set dst src);
  effect(KILL cr);

  ins_cost(60);
  format %{ "movl    $dst, $src\t# ptr (positive 32-bit)" %}
  ins_encode %{
    __ movl($dst$$Register, $src$$constant);
  %}
  ins_pipe(ialu_reg);
%}

instruct loadConF(regF dst, immF con) %{
  match(Set dst con);
  ins_cost(125);
  format %{ "movss   $dst, [$constantaddress]\t# load from constant table: float=$con" %}
  ins_encode %{
    __ movflt($dst$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct loadConH(regF dst, immH con) %{
  match(Set dst con);
  ins_cost(125);
  format %{ "movss   $dst, [$constantaddress]\t# load from constant table: halffloat=$con" %}
  ins_encode %{
    __ movflt($dst$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct loadConN0(rRegN dst, immN0 src, rFlagsReg cr) %{
  match(Set dst src);
  effect(KILL cr);
  format %{ "xorq    $dst, $src\t# compressed null pointer" %}
  ins_encode %{
    __ xorq($dst$$Register, $dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct loadConN(rRegN dst, immN src) %{
  match(Set dst src);

  ins_cost(125);
  format %{ "movl    $dst, $src\t# compressed ptr" %}
  ins_encode %{
    address con = (address)$src$$constant;
    if (con == nullptr) {
      ShouldNotReachHere();
    } else {
      __ set_narrow_oop($dst$$Register, (jobject)$src$$constant);
    }
  %}
  ins_pipe(ialu_reg_fat); // XXX
%}

instruct loadConNKlass(rRegN dst, immNKlass src) %{
  match(Set dst src);

  ins_cost(125);
  format %{ "movl    $dst, $src\t# compressed klass ptr" %}
  ins_encode %{
    address con = (address)$src$$constant;
    if (con == nullptr) {
      ShouldNotReachHere();
    } else {
      __ set_narrow_klass($dst$$Register, (Klass*)$src$$constant);
    }
  %}
  ins_pipe(ialu_reg_fat); // XXX
%}

instruct loadConF0(regF dst, immF0 src)
%{
  match(Set dst src);
  ins_cost(100);

  format %{ "xorps   $dst, $dst\t# float 0.0" %}
  ins_encode %{
    __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

// Use the same format since predicate() can not be used here.
instruct loadConD(regD dst, immD con) %{
  match(Set dst con);
  ins_cost(125);
  format %{ "movsd   $dst, [$constantaddress]\t# load from constant table: double=$con" %}
  ins_encode %{
    __ movdbl($dst$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct loadConD0(regD dst, immD0 src)
%{
  match(Set dst src);
  ins_cost(100);

  format %{ "xorpd   $dst, $dst\t# double 0.0" %}
  ins_encode %{
    __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct loadSSI(rRegI dst, stackSlotI src)
%{
  match(Set dst src);

  ins_cost(125);
  format %{ "movl    $dst, $src\t# int stk" %}
  ins_encode %{
    __ movl($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct loadSSL(rRegL dst, stackSlotL src)
%{
  match(Set dst src);

  ins_cost(125);
  format %{ "movq    $dst, $src\t# long stk" %}
  ins_encode %{
    __ movq($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct loadSSP(rRegP dst, stackSlotP src)
%{
  match(Set dst src);

  ins_cost(125);
  format %{ "movq    $dst, $src\t# ptr stk" %}
  ins_encode %{
    __ movq($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct loadSSF(regF dst, stackSlotF src)
%{
  match(Set dst src);

  ins_cost(125);
  format %{ "movss   $dst, $src\t# float stk" %}
  ins_encode %{
    __ movflt($dst$$XMMRegister, Address(rsp, $src$$disp));
  %}
  ins_pipe(pipe_slow); // XXX
%}

// Use the same format since predicate() can not be used here.
instruct loadSSD(regD dst, stackSlotD src)
%{
  match(Set dst src);

  ins_cost(125);
  format %{ "movsd   $dst, $src\t# double stk" %}
  ins_encode  %{
    __ movdbl($dst$$XMMRegister, Address(rsp, $src$$disp));
  %}
  ins_pipe(pipe_slow); // XXX
%}

// Prefetch instructions for allocation.
// Must be safe to execute with invalid address (cannot fault).

instruct prefetchAlloc( memory mem ) %{
  predicate(AllocatePrefetchInstr==3);
  match(PrefetchAllocation mem);
  ins_cost(125);

  format %{ "PREFETCHW $mem\t# Prefetch allocation into level 1 cache and mark modified" %}
  ins_encode %{
    __ prefetchw($mem$$Address);
  %}
  ins_pipe(ialu_mem);
%}

instruct prefetchAllocNTA( memory mem ) %{
  predicate(AllocatePrefetchInstr==0);
  match(PrefetchAllocation mem);
  ins_cost(125);

  format %{ "PREFETCHNTA $mem\t# Prefetch allocation to non-temporal cache for write" %}
  ins_encode %{
    __ prefetchnta($mem$$Address);
  %}
  ins_pipe(ialu_mem);
%}

instruct prefetchAllocT0( memory mem ) %{
  predicate(AllocatePrefetchInstr==1);
  match(PrefetchAllocation mem);
  ins_cost(125);

  format %{ "PREFETCHT0 $mem\t# Prefetch allocation to level 1 and 2 caches for write" %}
  ins_encode %{
    __ prefetcht0($mem$$Address);
  %}
  ins_pipe(ialu_mem);
%}

instruct prefetchAllocT2( memory mem ) %{
  predicate(AllocatePrefetchInstr==2);
  match(PrefetchAllocation mem);
  ins_cost(125);

  format %{ "PREFETCHT2 $mem\t# Prefetch allocation to level 2 cache for write" %}
  ins_encode %{
    __ prefetcht2($mem$$Address);
  %}
  ins_pipe(ialu_mem);
%}

//----------Store Instructions-------------------------------------------------

// Store Byte
instruct storeB(memory mem, rRegI src)
%{
  match(Set mem (StoreB mem src));

  ins_cost(125); // XXX
  format %{ "movb    $mem, $src\t# byte" %}
  ins_encode %{
    __ movb($mem$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

// Store Char/Short
instruct storeC(memory mem, rRegI src)
%{
  match(Set mem (StoreC mem src));

  ins_cost(125); // XXX
  format %{ "movw    $mem, $src\t# char/short" %}
  ins_encode %{
    __ movw($mem$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

// Store Integer
instruct storeI(memory mem, rRegI src)
%{
  match(Set mem (StoreI mem src));

  ins_cost(125); // XXX
  format %{ "movl    $mem, $src\t# int" %}
  ins_encode %{
    __ movl($mem$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

// Store Long
instruct storeL(memory mem, rRegL src)
%{
  match(Set mem (StoreL mem src));

  ins_cost(125); // XXX
  format %{ "movq    $mem, $src\t# long" %}
  ins_encode %{
    __ movq($mem$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg); // XXX
%}

// Store Pointer
instruct storeP(memory mem, any_RegP src)
%{
  predicate(n->as_Store()->barrier_data() == 0);
  match(Set mem (StoreP mem src));

  ins_cost(125); // XXX
  format %{ "movq    $mem, $src\t# ptr" %}
  ins_encode %{
    __ movq($mem$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct storeImmP0(memory mem, immP0 zero)
%{
  predicate(UseCompressedOops && (CompressedOops::base() == nullptr) && n->as_Store()->barrier_data() == 0);
  match(Set mem (StoreP mem zero));

  ins_cost(125); // XXX
  format %{ "movq    $mem, R12\t# ptr (R12_heapbase==0)" %}
  ins_encode %{
    __ movq($mem$$Address, r12);
  %}
  ins_pipe(ialu_mem_reg);
%}

// Store Null Pointer, mark word, or other simple pointer constant.
instruct storeImmP(memory mem, immP31 src)
%{
  predicate(n->as_Store()->barrier_data() == 0);
  match(Set mem (StoreP mem src));

  ins_cost(150); // XXX
  format %{ "movq    $mem, $src\t# ptr" %}
  ins_encode %{
    __ movq($mem$$Address, $src$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

// Store Compressed Pointer
instruct storeN(memory mem, rRegN src)
%{
  predicate(n->as_Store()->barrier_data() == 0);
  match(Set mem (StoreN mem src));

  ins_cost(125); // XXX
  format %{ "movl    $mem, $src\t# compressed ptr" %}
  ins_encode %{
    __ movl($mem$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct storeNKlass(memory mem, rRegN src)
%{
  match(Set mem (StoreNKlass mem src));

  ins_cost(125); // XXX
  format %{ "movl    $mem, $src\t# compressed klass ptr" %}
  ins_encode %{
    __ movl($mem$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct storeImmN0(memory mem, immN0 zero)
%{
  predicate(CompressedOops::base() == nullptr && n->as_Store()->barrier_data() == 0);
  match(Set mem (StoreN mem zero));

  ins_cost(125); // XXX
  format %{ "movl    $mem, R12\t# compressed ptr (R12_heapbase==0)" %}
  ins_encode %{
    __ movl($mem$$Address, r12);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct storeImmN(memory mem, immN src)
%{
  predicate(n->as_Store()->barrier_data() == 0);
  match(Set mem (StoreN mem src));

  ins_cost(150); // XXX
  format %{ "movl    $mem, $src\t# compressed ptr" %}
  ins_encode %{
    address con = (address)$src$$constant;
    if (con == nullptr) {
      __ movl($mem$$Address, 0);
    } else {
      __ set_narrow_oop($mem$$Address, (jobject)$src$$constant);
    }
  %}
  ins_pipe(ialu_mem_imm);
%}

instruct storeImmNKlass(memory mem, immNKlass src)
%{
  match(Set mem (StoreNKlass mem src));

  ins_cost(150); // XXX
  format %{ "movl    $mem, $src\t# compressed klass ptr" %}
  ins_encode %{
    __ set_narrow_klass($mem$$Address, (Klass*)$src$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

// Store Integer Immediate
instruct storeImmI0(memory mem, immI_0 zero)
%{
  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));
  match(Set mem (StoreI mem zero));

  ins_cost(125); // XXX
  format %{ "movl    $mem, R12\t# int (R12_heapbase==0)" %}
  ins_encode %{
    __ movl($mem$$Address, r12);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct storeImmI(memory mem, immI src)
%{
  match(Set mem (StoreI mem src));

  ins_cost(150);
  format %{ "movl    $mem, $src\t# int" %}
  ins_encode %{
    __ movl($mem$$Address, $src$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

// Store Long Immediate
instruct storeImmL0(memory mem, immL0 zero)
%{
  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));
  match(Set mem (StoreL mem zero));

  ins_cost(125); // XXX
  format %{ "movq    $mem, R12\t# long (R12_heapbase==0)" %}
  ins_encode %{
    __ movq($mem$$Address, r12);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct storeImmL(memory mem, immL32 src)
%{
  match(Set mem (StoreL mem src));

  ins_cost(150);
  format %{ "movq    $mem, $src\t# long" %}
  ins_encode %{
    __ movq($mem$$Address, $src$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

// Store Short/Char Immediate
instruct storeImmC0(memory mem, immI_0 zero)
%{
  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));
  match(Set mem (StoreC mem zero));

  ins_cost(125); // XXX
  format %{ "movw    $mem, R12\t# short/char (R12_heapbase==0)" %}
  ins_encode %{
    __ movw($mem$$Address, r12);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct storeImmI16(memory mem, immI16 src)
%{
  predicate(UseStoreImmI16);
  match(Set mem (StoreC mem src));

  ins_cost(150);
  format %{ "movw    $mem, $src\t# short/char" %}
  ins_encode %{
    __ movw($mem$$Address, $src$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

// Store Byte Immediate
instruct storeImmB0(memory mem, immI_0 zero)
%{
  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));
  match(Set mem (StoreB mem zero));

  ins_cost(125); // XXX
  format %{ "movb    $mem, R12\t# short/char (R12_heapbase==0)" %}
  ins_encode %{
    __ movb($mem$$Address, r12);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct storeImmB(memory mem, immI8 src)
%{
  match(Set mem (StoreB mem src));

  ins_cost(150); // XXX
  format %{ "movb    $mem, $src\t# byte" %}
  ins_encode %{
    __ movb($mem$$Address, $src$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

// Store Float
instruct storeF(memory mem, regF src)
%{
  match(Set mem (StoreF mem src));

  ins_cost(95); // XXX
  format %{ "movss   $mem, $src\t# float" %}
  ins_encode %{
    __ movflt($mem$$Address, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow); // XXX
%}

// Store immediate Float value (it is faster than store from XMM register)
instruct storeF0(memory mem, immF0 zero)
%{
  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));
  match(Set mem (StoreF mem zero));

  ins_cost(25); // XXX
  format %{ "movl    $mem, R12\t# float 0. (R12_heapbase==0)" %}
  ins_encode %{
    __ movl($mem$$Address, r12);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct storeF_imm(memory mem, immF src)
%{
  match(Set mem (StoreF mem src));

  ins_cost(50);
  format %{ "movl    $mem, $src\t# float" %}
  ins_encode %{
    __ movl($mem$$Address, jint_cast($src$$constant));
  %}
  ins_pipe(ialu_mem_imm);
%}

// Store Double
instruct storeD(memory mem, regD src)
%{
  match(Set mem (StoreD mem src));

  ins_cost(95); // XXX
  format %{ "movsd   $mem, $src\t# double" %}
  ins_encode %{
    __ movdbl($mem$$Address, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow); // XXX
%}

// Store immediate double 0.0 (it is faster than store from XMM register)
instruct storeD0_imm(memory mem, immD0 src)
%{
  predicate(!UseCompressedOops || (CompressedOops::base() != nullptr));
  match(Set mem (StoreD mem src));

  ins_cost(50);
  format %{ "movq    $mem, $src\t# double 0." %}
  ins_encode %{
    __ movq($mem$$Address, $src$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

instruct storeD0(memory mem, immD0 zero)
%{
  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));
  match(Set mem (StoreD mem zero));

  ins_cost(25); // XXX
  format %{ "movq    $mem, R12\t# double 0. (R12_heapbase==0)" %}
  ins_encode %{
    __ movq($mem$$Address, r12);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct storeSSI(stackSlotI dst, rRegI src)
%{
  match(Set dst src);

  ins_cost(100);
  format %{ "movl    $dst, $src\t# int stk" %}
  ins_encode %{
    __ movl($dst$$Address, $src$$Register);
  %}
  ins_pipe( ialu_mem_reg );
%}

instruct storeSSL(stackSlotL dst, rRegL src)
%{
  match(Set dst src);

  ins_cost(100);
  format %{ "movq    $dst, $src\t# long stk" %}
  ins_encode %{
    __ movq($dst$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct storeSSP(stackSlotP dst, rRegP src)
%{
  match(Set dst src);

  ins_cost(100);
  format %{ "movq    $dst, $src\t# ptr stk" %}
  ins_encode %{
    __ movq($dst$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct storeSSF(stackSlotF dst, regF src)
%{
  match(Set dst src);

  ins_cost(95); // XXX
  format %{ "movss   $dst, $src\t# float stk" %}
  ins_encode %{
    __ movflt(Address(rsp, $dst$$disp), $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct storeSSD(stackSlotD dst, regD src)
%{
  match(Set dst src);

  ins_cost(95); // XXX
  format %{ "movsd   $dst, $src\t# double stk" %}
  ins_encode %{
    __ movdbl(Address(rsp, $dst$$disp), $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct cacheWB(indirect addr)
%{
  predicate(VM_Version::supports_data_cache_line_flush());
  match(CacheWB addr);

  ins_cost(100);
  format %{"cache wb $addr" %}
  ins_encode %{
    assert($addr->index_position() < 0, "should be");
    assert($addr$$disp == 0, "should be");
    __ cache_wb(Address($addr$$base$$Register, 0));
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct cacheWBPreSync()
%{
  predicate(VM_Version::supports_data_cache_line_flush());
  match(CacheWBPreSync);

  ins_cost(100);
  format %{"cache wb presync" %}
  ins_encode %{
    __ cache_wbsync(true);
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct cacheWBPostSync()
%{
  predicate(VM_Version::supports_data_cache_line_flush());
  match(CacheWBPostSync);

  ins_cost(100);
  format %{"cache wb postsync" %}
  ins_encode %{
    __ cache_wbsync(false);
  %}
  ins_pipe(pipe_slow); // XXX
%}

//----------BSWAP Instructions-------------------------------------------------
instruct bytes_reverse_int(rRegI dst) %{
  match(Set dst (ReverseBytesI dst));

  format %{ "bswapl  $dst" %}
  ins_encode %{
    __ bswapl($dst$$Register);
  %}
  ins_pipe( ialu_reg );
%}

instruct bytes_reverse_long(rRegL dst) %{
  match(Set dst (ReverseBytesL dst));

  format %{ "bswapq  $dst" %}
  ins_encode %{
    __ bswapq($dst$$Register);
  %}
  ins_pipe( ialu_reg);
%}

instruct bytes_reverse_unsigned_short(rRegI dst, rFlagsReg cr) %{
  match(Set dst (ReverseBytesUS dst));
  effect(KILL cr);

  format %{ "bswapl  $dst\n\t"
            "shrl    $dst,16\n\t" %}
  ins_encode %{
    __ bswapl($dst$$Register);
    __ shrl($dst$$Register, 16);
  %}
  ins_pipe( ialu_reg );
%}

instruct bytes_reverse_short(rRegI dst, rFlagsReg cr) %{
  match(Set dst (ReverseBytesS dst));
  effect(KILL cr);

  format %{ "bswapl  $dst\n\t"
            "sar     $dst,16\n\t" %}
  ins_encode %{
    __ bswapl($dst$$Register);
    __ sarl($dst$$Register, 16);
  %}
  ins_pipe( ialu_reg );
%}

//---------- Zeros Count Instructions ------------------------------------------

instruct countLeadingZerosI(rRegI dst, rRegI src, rFlagsReg cr) %{
  predicate(UseCountLeadingZerosInstruction);
  match(Set dst (CountLeadingZerosI src));
  effect(KILL cr);

  format %{ "lzcntl  $dst, $src\t# count leading zeros (int)" %}
  ins_encode %{
    __ lzcntl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct countLeadingZerosI_mem(rRegI dst, memory src, rFlagsReg cr) %{
  predicate(UseCountLeadingZerosInstruction);
  match(Set dst (CountLeadingZerosI (LoadI src)));
  effect(KILL cr);
  ins_cost(175);
  format %{ "lzcntl  $dst, $src\t# count leading zeros (int)" %}
  ins_encode %{
    __ lzcntl($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct countLeadingZerosI_bsr(rRegI dst, rRegI src, rFlagsReg cr) %{
  predicate(!UseCountLeadingZerosInstruction);
  match(Set dst (CountLeadingZerosI src));
  effect(KILL cr);

  format %{ "bsrl    $dst, $src\t# count leading zeros (int)\n\t"
            "jnz     skip\n\t"
            "movl    $dst, -1\n"
      "skip:\n\t"
            "negl    $dst\n\t"
            "addl    $dst, 31" %}
  ins_encode %{
    Register Rdst = $dst$$Register;
    Register Rsrc = $src$$Register;
    Label skip;
    __ bsrl(Rdst, Rsrc);
    __ jccb(Assembler::notZero, skip);
    __ movl(Rdst, -1);
    __ bind(skip);
    __ negl(Rdst);
    __ addl(Rdst, BitsPerInt - 1);
  %}
  ins_pipe(ialu_reg);
%}

instruct countLeadingZerosL(rRegI dst, rRegL src, rFlagsReg cr) %{
  predicate(UseCountLeadingZerosInstruction);
  match(Set dst (CountLeadingZerosL src));
  effect(KILL cr);

  format %{ "lzcntq  $dst, $src\t# count leading zeros (long)" %}
  ins_encode %{
    __ lzcntq($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct countLeadingZerosL_mem(rRegI dst, memory src, rFlagsReg cr) %{
  predicate(UseCountLeadingZerosInstruction);
  match(Set dst (CountLeadingZerosL (LoadL src)));
  effect(KILL cr);
  ins_cost(175);
  format %{ "lzcntq  $dst, $src\t# count leading zeros (long)" %}
  ins_encode %{
    __ lzcntq($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct countLeadingZerosL_bsr(rRegI dst, rRegL src, rFlagsReg cr) %{
  predicate(!UseCountLeadingZerosInstruction);
  match(Set dst (CountLeadingZerosL src));
  effect(KILL cr);

  format %{ "bsrq    $dst, $src\t# count leading zeros (long)\n\t"
            "jnz     skip\n\t"
            "movl    $dst, -1\n"
      "skip:\n\t"
            "negl    $dst\n\t"
            "addl    $dst, 63" %}
  ins_encode %{
    Register Rdst = $dst$$Register;
    Register Rsrc = $src$$Register;
    Label skip;
    __ bsrq(Rdst, Rsrc);
    __ jccb(Assembler::notZero, skip);
    __ movl(Rdst, -1);
    __ bind(skip);
    __ negl(Rdst);
    __ addl(Rdst, BitsPerLong - 1);
  %}
  ins_pipe(ialu_reg);
%}

instruct countTrailingZerosI(rRegI dst, rRegI src, rFlagsReg cr) %{
  predicate(UseCountTrailingZerosInstruction);
  match(Set dst (CountTrailingZerosI src));
  effect(KILL cr);

  format %{ "tzcntl    $dst, $src\t# count trailing zeros (int)" %}
  ins_encode %{
    __ tzcntl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct countTrailingZerosI_mem(rRegI dst, memory src, rFlagsReg cr) %{
  predicate(UseCountTrailingZerosInstruction);
  match(Set dst (CountTrailingZerosI (LoadI src)));
  effect(KILL cr);
  ins_cost(175);
  format %{ "tzcntl    $dst, $src\t# count trailing zeros (int)" %}
  ins_encode %{
    __ tzcntl($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct countTrailingZerosI_bsf(rRegI dst, rRegI src, rFlagsReg cr) %{
  predicate(!UseCountTrailingZerosInstruction);
  match(Set dst (CountTrailingZerosI src));
  effect(KILL cr);

  format %{ "bsfl    $dst, $src\t# count trailing zeros (int)\n\t"
            "jnz     done\n\t"
            "movl    $dst, 32\n"
      "done:" %}
  ins_encode %{
    Register Rdst = $dst$$Register;
    Label done;
    __ bsfl(Rdst, $src$$Register);
    __ jccb(Assembler::notZero, done);
    __ movl(Rdst, BitsPerInt);
    __ bind(done);
  %}
  ins_pipe(ialu_reg);
%}

instruct countTrailingZerosL(rRegI dst, rRegL src, rFlagsReg cr) %{
  predicate(UseCountTrailingZerosInstruction);
  match(Set dst (CountTrailingZerosL src));
  effect(KILL cr);

  format %{ "tzcntq    $dst, $src\t# count trailing zeros (long)" %}
  ins_encode %{
    __ tzcntq($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct countTrailingZerosL_mem(rRegI dst, memory src, rFlagsReg cr) %{
  predicate(UseCountTrailingZerosInstruction);
  match(Set dst (CountTrailingZerosL (LoadL src)));
  effect(KILL cr);
  ins_cost(175);
  format %{ "tzcntq    $dst, $src\t# count trailing zeros (long)" %}
  ins_encode %{
    __ tzcntq($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct countTrailingZerosL_bsf(rRegI dst, rRegL src, rFlagsReg cr) %{
  predicate(!UseCountTrailingZerosInstruction);
  match(Set dst (CountTrailingZerosL src));
  effect(KILL cr);

  format %{ "bsfq    $dst, $src\t# count trailing zeros (long)\n\t"
            "jnz     done\n\t"
            "movl    $dst, 64\n"
      "done:" %}
  ins_encode %{
    Register Rdst = $dst$$Register;
    Label done;
    __ bsfq(Rdst, $src$$Register);
    __ jccb(Assembler::notZero, done);
    __ movl(Rdst, BitsPerLong);
    __ bind(done);
  %}
  ins_pipe(ialu_reg);
%}

//--------------- Reverse Operation Instructions ----------------
instruct bytes_reversebit_int(rRegI dst, rRegI src, rRegI rtmp, rFlagsReg cr) %{
  predicate(!VM_Version::supports_gfni());
  match(Set dst (ReverseI src));
  effect(TEMP dst, TEMP rtmp, KILL cr);
  format %{ "reverse_int $dst $src\t! using $rtmp as TEMP" %}
  ins_encode %{
    __ reverseI($dst$$Register, $src$$Register, xnoreg, xnoreg, $rtmp$$Register);
  %}
  ins_pipe( ialu_reg );
%}

instruct bytes_reversebit_int_gfni(rRegI dst, rRegI src, vlRegF xtmp1, vlRegF xtmp2, rRegL rtmp, rFlagsReg cr) %{
  predicate(VM_Version::supports_gfni());
  match(Set dst (ReverseI src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp, KILL cr);
  format %{ "reverse_int $dst $src\t! using $rtmp, $xtmp1 and $xtmp2 as TEMP" %}
  ins_encode %{
    __ reverseI($dst$$Register, $src$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $rtmp$$Register);
  %}
  ins_pipe( ialu_reg );
%}

instruct bytes_reversebit_long(rRegL dst, rRegL src, rRegL rtmp1, rRegL rtmp2, rFlagsReg cr) %{
  predicate(!VM_Version::supports_gfni());
  match(Set dst (ReverseL src));
  effect(TEMP dst, TEMP rtmp1, TEMP rtmp2, KILL cr);
  format %{ "reverse_long $dst $src\t! using $rtmp1 and $rtmp2 as TEMP" %}
  ins_encode %{
    __ reverseL($dst$$Register, $src$$Register, xnoreg, xnoreg, $rtmp1$$Register, $rtmp2$$Register);
  %}
  ins_pipe( ialu_reg );
%}

instruct bytes_reversebit_long_gfni(rRegL dst, rRegL src, vlRegD xtmp1, vlRegD xtmp2, rRegL rtmp, rFlagsReg cr) %{
  predicate(VM_Version::supports_gfni());
  match(Set dst (ReverseL src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp, KILL cr);
  format %{ "reverse_long $dst $src\t! using $rtmp, $xtmp1 and $xtmp2 as TEMP" %}
  ins_encode %{
    __ reverseL($dst$$Register, $src$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $rtmp$$Register, noreg);
  %}
  ins_pipe( ialu_reg );
%}

//---------- Population Count Instructions -------------------------------------

instruct popCountI(rRegI dst, rRegI src, rFlagsReg cr) %{
  predicate(UsePopCountInstruction);
  match(Set dst (PopCountI src));
  effect(KILL cr);

  format %{ "popcnt  $dst, $src" %}
  ins_encode %{
    __ popcntl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct popCountI_mem(rRegI dst, memory mem, rFlagsReg cr) %{
  predicate(UsePopCountInstruction);
  match(Set dst (PopCountI (LoadI mem)));
  effect(KILL cr);

  format %{ "popcnt  $dst, $mem" %}
  ins_encode %{
    __ popcntl($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg);
%}

// Note: Long.bitCount(long) returns an int.
instruct popCountL(rRegI dst, rRegL src, rFlagsReg cr) %{
  predicate(UsePopCountInstruction);
  match(Set dst (PopCountL src));
  effect(KILL cr);

  format %{ "popcnt  $dst, $src" %}
  ins_encode %{
    __ popcntq($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg);
%}

// Note: Long.bitCount(long) returns an int.
instruct popCountL_mem(rRegI dst, memory mem, rFlagsReg cr) %{
  predicate(UsePopCountInstruction);
  match(Set dst (PopCountL (LoadL mem)));
  effect(KILL cr);

  format %{ "popcnt  $dst, $mem" %}
  ins_encode %{
    __ popcntq($dst$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_reg);
%}


//----------MemBar Instructions-----------------------------------------------
// Memory barrier flavors

instruct membar_acquire()
%{
  match(MemBarAcquire);
  match(LoadFence);
  ins_cost(0);

  size(0);
  format %{ "MEMBAR-acquire ! (empty encoding)" %}
  ins_encode();
  ins_pipe(empty);
%}

instruct membar_acquire_lock()
%{
  match(MemBarAcquireLock);
  ins_cost(0);

  size(0);
  format %{ "MEMBAR-acquire (prior CMPXCHG in FastLock so empty encoding)" %}
  ins_encode();
  ins_pipe(empty);
%}

instruct membar_release()
%{
  match(MemBarRelease);
  match(StoreFence);
  ins_cost(0);

  size(0);
  format %{ "MEMBAR-release ! (empty encoding)" %}
  ins_encode();
  ins_pipe(empty);
%}

instruct membar_release_lock()
%{
  match(MemBarReleaseLock);
  ins_cost(0);

  size(0);
  format %{ "MEMBAR-release (a FastUnlock follows so empty encoding)" %}
  ins_encode();
  ins_pipe(empty);
%}

instruct membar_volatile(rFlagsReg cr) %{
  match(MemBarVolatile);
  effect(KILL cr);
  ins_cost(400);

  format %{
    $$template
    $$emit$$"lock addl [rsp + #0], 0\t! membar_volatile"
  %}
  ins_encode %{
    __ membar(Assembler::StoreLoad);
  %}
  ins_pipe(pipe_slow);
%}

instruct unnecessary_membar_volatile()
%{
  match(MemBarVolatile);
  predicate(Matcher::post_store_load_barrier(n));
  ins_cost(0);

  size(0);
  format %{ "MEMBAR-volatile (unnecessary so empty encoding)" %}
  ins_encode();
  ins_pipe(empty);
%}

instruct membar_storestore() %{
  match(MemBarStoreStore);
  match(StoreStoreFence);
  ins_cost(0);

  size(0);
  format %{ "MEMBAR-storestore (empty encoding)" %}
  ins_encode( );
  ins_pipe(empty);
%}

//----------Move Instructions--------------------------------------------------

instruct castX2P(rRegP dst, rRegL src)
%{
  match(Set dst (CastX2P src));

  format %{ "movq    $dst, $src\t# long->ptr" %}
  ins_encode %{
    if ($dst$$reg != $src$$reg) {
      __ movptr($dst$$Register, $src$$Register);
    }
  %}
  ins_pipe(ialu_reg_reg); // XXX
%}

instruct castP2X(rRegL dst, rRegP src)
%{
  match(Set dst (CastP2X src));

  format %{ "movq    $dst, $src\t# ptr -> long" %}
  ins_encode %{
    if ($dst$$reg != $src$$reg) {
      __ movptr($dst$$Register, $src$$Register);
    }
  %}
  ins_pipe(ialu_reg_reg); // XXX
%}

// Convert oop into int for vectors alignment masking
instruct convP2I(rRegI dst, rRegP src)
%{
  match(Set dst (ConvL2I (CastP2X src)));

  format %{ "movl    $dst, $src\t# ptr -> int" %}
  ins_encode %{
    __ movl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg); // XXX
%}

// Convert compressed oop into int for vectors alignment masking
// in case of 32bit oops (heap < 4Gb).
instruct convN2I(rRegI dst, rRegN src)
%{
  predicate(CompressedOops::shift() == 0);
  match(Set dst (ConvL2I (CastP2X (DecodeN src))));

  format %{ "movl    $dst, $src\t# compressed ptr -> int" %}
  ins_encode %{
    __ movl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg); // XXX
%}

// Convert oop pointer into compressed form
instruct encodeHeapOop(rRegN dst, rRegP src, rFlagsReg cr) %{
  predicate(n->bottom_type()->make_ptr()->ptr() != TypePtr::NotNull);
  match(Set dst (EncodeP src));
  effect(KILL cr);
  format %{ "encode_heap_oop $dst,$src" %}
  ins_encode %{
    Register s = $src$$Register;
    Register d = $dst$$Register;
    if (s != d) {
      __ movq(d, s);
    }
    __ encode_heap_oop(d);
  %}
  ins_pipe(ialu_reg_long);
%}

instruct encodeHeapOop_not_null(rRegN dst, rRegP src, rFlagsReg cr) %{
  predicate(n->bottom_type()->make_ptr()->ptr() == TypePtr::NotNull);
  match(Set dst (EncodeP src));
  effect(KILL cr);
  format %{ "encode_heap_oop_not_null $dst,$src" %}
  ins_encode %{
    __ encode_heap_oop_not_null($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_long);
%}

instruct decodeHeapOop(rRegP dst, rRegN src, rFlagsReg cr) %{
  predicate(n->bottom_type()->is_ptr()->ptr() != TypePtr::NotNull &&
            n->bottom_type()->is_ptr()->ptr() != TypePtr::Constant);
  match(Set dst (DecodeN src));
  effect(KILL cr);
  format %{ "decode_heap_oop $dst,$src" %}
  ins_encode %{
    Register s = $src$$Register;
    Register d = $dst$$Register;
    if (s != d) {
      __ movq(d, s);
    }
    __ decode_heap_oop(d);
  %}
  ins_pipe(ialu_reg_long);
%}

instruct decodeHeapOop_not_null(rRegP dst, rRegN src, rFlagsReg cr) %{
  predicate(n->bottom_type()->is_ptr()->ptr() == TypePtr::NotNull ||
            n->bottom_type()->is_ptr()->ptr() == TypePtr::Constant);
  match(Set dst (DecodeN src));
  effect(KILL cr);
  format %{ "decode_heap_oop_not_null $dst,$src" %}
  ins_encode %{
    Register s = $src$$Register;
    Register d = $dst$$Register;
    if (s != d) {
      __ decode_heap_oop_not_null(d, s);
    } else {
      __ decode_heap_oop_not_null(d);
    }
  %}
  ins_pipe(ialu_reg_long);
%}

instruct encodeKlass_not_null(rRegN dst, rRegP src, rFlagsReg cr) %{
  match(Set dst (EncodePKlass src));
  effect(TEMP dst, KILL cr);
  format %{ "encode_and_move_klass_not_null $dst,$src" %}
  ins_encode %{
    __ encode_and_move_klass_not_null($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_long);
%}

instruct decodeKlass_not_null(rRegP dst, rRegN src, rFlagsReg cr) %{
  match(Set dst (DecodeNKlass src));
  effect(TEMP dst, KILL cr);
  format %{ "decode_and_move_klass_not_null $dst,$src" %}
  ins_encode %{
    __ decode_and_move_klass_not_null($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_long);
%}

//----------Conditional Move---------------------------------------------------
// Jump
// dummy instruction for generating temp registers
instruct jumpXtnd_offset(rRegL switch_val, immI2 shift, rRegI dest) %{
  match(Jump (LShiftL switch_val shift));
  ins_cost(350);
  predicate(false);
  effect(TEMP dest);

  format %{ "leaq    $dest, [$constantaddress]\n\t"
            "jmp     [$dest + $switch_val << $shift]\n\t" %}
  ins_encode %{
    // We could use jump(ArrayAddress) except that the macro assembler needs to use r10
    // to do that and the compiler is using that register as one it can allocate.
    // So we build it all by hand.
    // Address index(noreg, switch_reg, (Address::ScaleFactor)$shift$$constant);
    // ArrayAddress dispatch(table, index);
    Address dispatch($dest$$Register, $switch_val$$Register, (Address::ScaleFactor) $shift$$constant);
    __ lea($dest$$Register, $constantaddress);
    __ jmp(dispatch);
  %}
  ins_pipe(pipe_jmp);
%}

instruct jumpXtnd_addr(rRegL switch_val, immI2 shift, immL32 offset, rRegI dest) %{
  match(Jump (AddL (LShiftL switch_val shift) offset));
  ins_cost(350);
  effect(TEMP dest);

  format %{ "leaq    $dest, [$constantaddress]\n\t"
            "jmp     [$dest + $switch_val << $shift + $offset]\n\t" %}
  ins_encode %{
    // We could use jump(ArrayAddress) except that the macro assembler needs to use r10
    // to do that and the compiler is using that register as one it can allocate.
    // So we build it all by hand.
    // Address index(noreg, switch_reg, (Address::ScaleFactor) $shift$$constant, (int) $offset$$constant);
    // ArrayAddress dispatch(table, index);
    Address dispatch($dest$$Register, $switch_val$$Register, (Address::ScaleFactor) $shift$$constant, (int) $offset$$constant);
    __ lea($dest$$Register, $constantaddress);
    __ jmp(dispatch);
  %}
  ins_pipe(pipe_jmp);
%}

instruct jumpXtnd(rRegL switch_val, rRegI dest) %{
  match(Jump switch_val);
  ins_cost(350);
  effect(TEMP dest);

  format %{ "leaq    $dest, [$constantaddress]\n\t"
            "jmp     [$dest + $switch_val]\n\t" %}
  ins_encode %{
    // We could use jump(ArrayAddress) except that the macro assembler needs to use r10
    // to do that and the compiler is using that register as one it can allocate.
    // So we build it all by hand.
    // Address index(noreg, switch_reg, Address::times_1);
    // ArrayAddress dispatch(table, index);
    Address dispatch($dest$$Register, $switch_val$$Register, Address::times_1);
    __ lea($dest$$Register, $constantaddress);
    __ jmp(dispatch);
  %}
  ins_pipe(pipe_jmp);
%}

// Conditional move
instruct cmovI_imm_01(rRegI dst, immI_1 src, rFlagsReg cr, cmpOp cop)
%{
  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);
  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));

  ins_cost(100); // XXX
  format %{ "setbn$cop $dst\t# signed, int" %}
  ins_encode %{
    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);
    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct cmovI_reg(rRegI dst, rRegI src, rFlagsReg cr, cmpOp cop)
%{
  predicate(!UseAPX);
  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));

  ins_cost(200); // XXX
  format %{ "cmovl$cop $dst, $src\t# signed, int" %}
  ins_encode %{
    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovI_reg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr, cmpOp cop)
%{
  predicate(UseAPX);
  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));

  ins_cost(200);
  format %{ "ecmovl$cop $dst, $src1, $src2\t# signed, int ndd" %}
  ins_encode %{
    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovI_imm_01U(rRegI dst, immI_1 src, rFlagsRegU cr, cmpOpU cop)
%{
  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);
  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));

  ins_cost(100); // XXX
  format %{ "setbn$cop $dst\t# unsigned, int" %}
  ins_encode %{
    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);
    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct cmovI_regU(cmpOpU cop, rFlagsRegU cr, rRegI dst, rRegI src) %{
  predicate(!UseAPX);
  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));

  ins_cost(200); // XXX
  format %{ "cmovl$cop $dst, $src\t# unsigned, int" %}
  ins_encode %{
    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovI_regU_ndd(rRegI dst, cmpOpU cop, rFlagsRegU cr, rRegI src1, rRegI src2) %{
  predicate(UseAPX);
  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));

  ins_cost(200);
  format %{ "ecmovl$cop $dst, $src1, $src2\t# unsigned, int ndd" %}
  ins_encode %{
    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovI_imm_01UCF(rRegI dst, immI_1 src, rFlagsRegUCF cr, cmpOpUCF cop)
%{
  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);
  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));

  ins_cost(100); // XXX
  format %{ "setbn$cop $dst\t# unsigned, int" %}
  ins_encode %{
    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);
    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct cmovI_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{
  predicate(!UseAPX);
  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));
  ins_cost(200);
  expand %{
    cmovI_regU(cop, cr, dst, src);
  %}
%}

instruct cmovI_regUCF_ndd(rRegI dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegI src1, rRegI src2) %{
  predicate(UseAPX);
  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));
  ins_cost(200);
  format %{ "ecmovl$cop $dst, $src1, $src2\t# unsigned, int ndd" %}
  ins_encode %{
    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovI_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{
  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);
  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));

  ins_cost(200); // XXX
  format %{ "cmovpl  $dst, $src\n\t"
            "cmovnel $dst, $src" %}
  ins_encode %{
    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);
    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovI_regUCF2_ne_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src1, rRegI src2) %{
  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);
  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));
  effect(TEMP dst);

  ins_cost(200);
  format %{ "ecmovpl  $dst, $src1, $src2\n\t"
            "cmovnel  $dst, $src2" %}
  ins_encode %{
    __ ecmovl(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);
    __ cmovl(Assembler::notEqual, $dst$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

// Since (x == y) == !(x != y), we can flip the sense of the test by flipping the
// inputs of the CMove
instruct cmovI_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{
  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);
  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));
  effect(TEMP dst);

  ins_cost(200); // XXX
  format %{ "cmovpl  $dst, $src\n\t"
            "cmovnel $dst, $src" %}
  ins_encode %{
    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);
    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

// We need this special handling for only eq / neq comparison since NaN == NaN is false,
// and parity flag bit is set if any of the operand is a NaN.
instruct cmovI_regUCF2_eq_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src1, rRegI src2) %{
  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);
  match(Set dst (CMoveI (Binary cop cr) (Binary src2 src1)));
  effect(TEMP dst);

  ins_cost(200);
  format %{ "ecmovpl  $dst, $src1, $src2\n\t"
            "cmovnel  $dst, $src2" %}
  ins_encode %{
    __ ecmovl(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);
    __ cmovl(Assembler::notEqual, $dst$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

// Conditional move
instruct cmovI_mem(cmpOp cop, rFlagsReg cr, rRegI dst, memory src) %{
  predicate(!UseAPX);
  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));

  ins_cost(250); // XXX
  format %{ "cmovl$cop $dst, $src\t# signed, int" %}
  ins_encode %{
    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);
  %}
  ins_pipe(pipe_cmov_mem);
%}

// Conditional move
instruct cmovI_rReg_rReg_mem_ndd(rRegI dst, cmpOp cop, rFlagsReg cr, rRegI src1, memory src2)
%{
  predicate(UseAPX);
  match(Set dst (CMoveI (Binary cop cr) (Binary src1 (LoadI src2))));

  ins_cost(250);
  format %{ "ecmovl$cop $dst, $src1, $src2\t# signed, int ndd" %}
  ins_encode %{
    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);
  %}
  ins_pipe(pipe_cmov_mem);
%}

// Conditional move
instruct cmovI_memU(cmpOpU cop, rFlagsRegU cr, rRegI dst, memory src)
%{
  predicate(!UseAPX);
  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));

  ins_cost(250); // XXX
  format %{ "cmovl$cop $dst, $src\t# unsigned, int" %}
  ins_encode %{
    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);
  %}
  ins_pipe(pipe_cmov_mem);
%}

instruct cmovI_memUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegI dst, memory src) %{
  predicate(!UseAPX);
  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));
  ins_cost(250);
  expand %{
    cmovI_memU(cop, cr, dst, src);
  %}
%}

instruct cmovI_rReg_rReg_memU_ndd(rRegI dst, cmpOpU cop, rFlagsRegU cr, rRegI src1, memory src2)
%{
  predicate(UseAPX);
  match(Set dst (CMoveI (Binary cop cr) (Binary src1 (LoadI src2))));

  ins_cost(250);
  format %{ "ecmovl$cop $dst, $src1, $src2\t# unsigned, int ndd" %}
  ins_encode %{
    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);
  %}
  ins_pipe(pipe_cmov_mem);
%}

instruct cmovI_rReg_rReg_memUCF_ndd(rRegI dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegI src1, memory src2)
%{
  predicate(UseAPX);
  match(Set dst (CMoveI (Binary cop cr) (Binary src1 (LoadI src2))));
  ins_cost(250);
  format %{ "ecmovl$cop $dst, $src1, $src2\t# unsigned, int ndd" %}
  ins_encode %{
    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);
  %}
  ins_pipe(pipe_cmov_mem);
%}

// Conditional move
instruct cmovN_reg(rRegN dst, rRegN src, rFlagsReg cr, cmpOp cop)
%{
  predicate(!UseAPX);
  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));

  ins_cost(200); // XXX
  format %{ "cmovl$cop $dst, $src\t# signed, compressed ptr" %}
  ins_encode %{
    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

// Conditional move ndd
instruct cmovN_reg_ndd(rRegN dst, rRegN src1, rRegN src2, rFlagsReg cr, cmpOp cop)
%{
  predicate(UseAPX);
  match(Set dst (CMoveN (Binary cop cr) (Binary src1 src2)));

  ins_cost(200);
  format %{ "ecmovl$cop $dst, $src1, $src2\t# signed, compressed ptr ndd" %}
  ins_encode %{
    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

// Conditional move
instruct cmovN_regU(cmpOpU cop, rFlagsRegU cr, rRegN dst, rRegN src)
%{
  predicate(!UseAPX);
  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));

  ins_cost(200); // XXX
  format %{ "cmovl$cop $dst, $src\t# unsigned, compressed ptr" %}
  ins_encode %{
    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovN_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{
  predicate(!UseAPX);
  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));
  ins_cost(200);
  expand %{
    cmovN_regU(cop, cr, dst, src);
  %}
%}

// Conditional move ndd
instruct cmovN_regU_ndd(rRegN dst, cmpOpU cop, rFlagsRegU cr, rRegN src1, rRegN src2)
%{
  predicate(UseAPX);
  match(Set dst (CMoveN (Binary cop cr) (Binary src1 src2)));

  ins_cost(200);
  format %{ "ecmovl$cop $dst, $src1, $src2\t# unsigned, compressed ptr ndd" %}
  ins_encode %{
    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovN_regUCF_ndd(rRegN dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegN src1, rRegN src2) %{
  predicate(UseAPX);
  match(Set dst (CMoveN (Binary cop cr) (Binary src1 src2)));
  ins_cost(200);
  format %{ "ecmovl$cop $dst, $src1, $src2\t# unsigned, compressed ptr ndd" %}
  ins_encode %{
    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovN_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{
  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);
  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));

  ins_cost(200); // XXX
  format %{ "cmovpl  $dst, $src\n\t"
            "cmovnel $dst, $src" %}
  ins_encode %{
    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);
    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

// Since (x == y) == !(x != y), we can flip the sense of the test by flipping the
// inputs of the CMove
instruct cmovN_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{
  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);
  match(Set dst (CMoveN (Binary cop cr) (Binary src dst)));

  ins_cost(200); // XXX
  format %{ "cmovpl  $dst, $src\n\t"
            "cmovnel $dst, $src" %}
  ins_encode %{
    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);
    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

// Conditional move
instruct cmovP_reg(rRegP dst, rRegP src, rFlagsReg cr, cmpOp cop)
%{
  predicate(!UseAPX);
  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));

  ins_cost(200); // XXX
  format %{ "cmovq$cop $dst, $src\t# signed, ptr" %}
  ins_encode %{
    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg);  // XXX
%}

// Conditional move ndd
instruct cmovP_reg_ndd(rRegP dst, rRegP src1, rRegP src2, rFlagsReg cr, cmpOp cop)
%{
  predicate(UseAPX);
  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));

  ins_cost(200);
  format %{ "ecmovq$cop $dst, $src1, $src2\t# signed, ptr ndd" %}
  ins_encode %{
    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

// Conditional move
instruct cmovP_regU(cmpOpU cop, rFlagsRegU cr, rRegP dst, rRegP src)
%{
  predicate(!UseAPX);
  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));

  ins_cost(200); // XXX
  format %{ "cmovq$cop $dst, $src\t# unsigned, ptr" %}
  ins_encode %{
    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg); // XXX
%}

// Conditional move ndd
instruct cmovP_regU_ndd(rRegP dst, cmpOpU cop, rFlagsRegU cr, rRegP src1, rRegP src2)
%{
  predicate(UseAPX);
  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));

  ins_cost(200);
  format %{ "ecmovq$cop $dst, $src1, $src2\t# unsigned, ptr ndd" %}
  ins_encode %{
    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovP_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{
  predicate(!UseAPX);
  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));
  ins_cost(200);
  expand %{
    cmovP_regU(cop, cr, dst, src);
  %}
%}

instruct cmovP_regUCF_ndd(rRegP dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegP src1, rRegP src2) %{
  predicate(UseAPX);
  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));
  ins_cost(200);
  format %{ "ecmovq$cop $dst, $src1, $src2\t# unsigned, ptr ndd" %}
  ins_encode %{
    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovP_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{
  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);
  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));

  ins_cost(200); // XXX
  format %{ "cmovpq  $dst, $src\n\t"
            "cmovneq $dst, $src" %}
  ins_encode %{
    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);
    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovP_regUCF2_ne_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src1, rRegP src2) %{
  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);
  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));
  effect(TEMP dst);

  ins_cost(200);
  format %{ "ecmovpq  $dst, $src1, $src2\n\t"
            "cmovneq  $dst, $src2" %}
  ins_encode %{
    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);
    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

// Since (x == y) == !(x != y), we can flip the sense of the test by flipping the
// inputs of the CMove
instruct cmovP_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{
  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);
  match(Set dst (CMoveP (Binary cop cr) (Binary src dst)));

  ins_cost(200); // XXX
  format %{ "cmovpq  $dst, $src\n\t"
            "cmovneq $dst, $src" %}
  ins_encode %{
    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);
    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovP_regUCF2_eq_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src1, rRegP src2) %{
  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);
  match(Set dst (CMoveP (Binary cop cr) (Binary src2 src1)));
  effect(TEMP dst);

  ins_cost(200);
  format %{ "ecmovpq  $dst, $src1, $src2\n\t"
            "cmovneq  $dst, $src2" %}
  ins_encode %{
    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);
    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovL_imm_01(rRegL dst, immL1 src, rFlagsReg cr, cmpOp cop)
%{
  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);
  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));

  ins_cost(100); // XXX
  format %{ "setbn$cop $dst\t# signed, long" %}
  ins_encode %{
    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);
    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct cmovL_reg(cmpOp cop, rFlagsReg cr, rRegL dst, rRegL src)
%{
  predicate(!UseAPX);
  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));

  ins_cost(200); // XXX
  format %{ "cmovq$cop $dst, $src\t# signed, long" %}
  ins_encode %{
    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg);  // XXX
%}

instruct cmovL_reg_ndd(rRegL dst, cmpOp cop, rFlagsReg cr, rRegL src1, rRegL src2)
%{
  predicate(UseAPX);
  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));

  ins_cost(200);
  format %{ "ecmovq$cop $dst, $src1, $src2\t# signed, long ndd" %}
  ins_encode %{
    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovL_mem(cmpOp cop, rFlagsReg cr, rRegL dst, memory src)
%{
  predicate(!UseAPX);
  match(Set dst (CMoveL (Binary cop cr) (Binary dst (LoadL src))));

  ins_cost(200); // XXX
  format %{ "cmovq$cop $dst, $src\t# signed, long" %}
  ins_encode %{
    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);
  %}
  ins_pipe(pipe_cmov_mem);  // XXX
%}

instruct cmovL_rReg_rReg_mem_ndd(rRegL dst, cmpOp cop, rFlagsReg cr, rRegL src1, memory src2)
%{
  predicate(UseAPX);
  match(Set dst (CMoveL (Binary cop cr) (Binary src1 (LoadL src2))));

  ins_cost(200);
  format %{ "ecmovq$cop $dst, $src1, $src2\t# signed, long ndd" %}
  ins_encode %{
    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);
  %}
  ins_pipe(pipe_cmov_mem);
%}

instruct cmovL_imm_01U(rRegL dst, immL1 src, rFlagsRegU cr, cmpOpU cop)
%{
  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);
  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));

  ins_cost(100); // XXX
  format %{ "setbn$cop $dst\t# unsigned, long" %}
  ins_encode %{
    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);
    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct cmovL_regU(cmpOpU cop, rFlagsRegU cr, rRegL dst, rRegL src)
%{
  predicate(!UseAPX);
  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));

  ins_cost(200); // XXX
  format %{ "cmovq$cop $dst, $src\t# unsigned, long" %}
  ins_encode %{
    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg); // XXX
%}

instruct cmovL_regU_ndd(rRegL dst, cmpOpU cop, rFlagsRegU cr, rRegL src1, rRegL src2)
%{
  predicate(UseAPX);
  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));

  ins_cost(200);
  format %{ "ecmovq$cop $dst, $src1, $src2\t# unsigned, long ndd" %}
  ins_encode %{
    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovL_imm_01UCF(rRegL dst, immL1 src, rFlagsRegUCF cr, cmpOpUCF cop)
%{
  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);
  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));

  ins_cost(100); // XXX
  format %{ "setbn$cop $dst\t# unsigned, long" %}
  ins_encode %{
    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);
    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct cmovL_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{
  predicate(!UseAPX);
  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));
  ins_cost(200);
  expand %{
    cmovL_regU(cop, cr, dst, src);
  %}
%}

instruct cmovL_regUCF_ndd(rRegL dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegL src1, rRegL src2)
%{
  predicate(UseAPX);
  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));
  ins_cost(200);
  format %{ "ecmovq$cop $dst, $src1, $src2\t# unsigned, long ndd" %}
  ins_encode %{
    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovL_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{
  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);
  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));

  ins_cost(200); // XXX
  format %{ "cmovpq  $dst, $src\n\t"
            "cmovneq $dst, $src" %}
  ins_encode %{
    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);
    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovL_regUCF2_ne_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src1, rRegL src2) %{
  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);
  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));
  effect(TEMP dst);

  ins_cost(200);
  format %{ "ecmovpq  $dst, $src1, $src2\n\t"
            "cmovneq  $dst, $src2" %}
  ins_encode %{
    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);
    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

// Since (x == y) == !(x != y), we can flip the sense of the test by flipping the
// inputs of the CMove
instruct cmovL_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{
  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);
  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));

  ins_cost(200); // XXX
  format %{ "cmovpq  $dst, $src\n\t"
            "cmovneq $dst, $src" %}
  ins_encode %{
    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);
    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovL_regUCF2_eq_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src1, rRegL src2) %{
  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);
  match(Set dst (CMoveL (Binary cop cr) (Binary src2 src1)));
  effect(TEMP dst);

  ins_cost(200);
  format %{ "ecmovpq  $dst, $src1, $src2\n\t"
            "cmovneq $dst, $src2" %}
  ins_encode %{
    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);
    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovL_memU(cmpOpU cop, rFlagsRegU cr, rRegL dst, memory src)
%{
  predicate(!UseAPX);
  match(Set dst (CMoveL (Binary cop cr) (Binary dst (LoadL src))));

  ins_cost(200); // XXX
  format %{ "cmovq$cop $dst, $src\t# unsigned, long" %}
  ins_encode %{
    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);
  %}
  ins_pipe(pipe_cmov_mem); // XXX
%}

instruct cmovL_memUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegL dst, memory src) %{
  predicate(!UseAPX);
  match(Set dst (CMoveL (Binary cop cr) (Binary dst (LoadL src))));
  ins_cost(200);
  expand %{
    cmovL_memU(cop, cr, dst, src);
  %}
%}

instruct cmovL_rReg_rReg_memU_ndd(rRegL dst, cmpOpU cop, rFlagsRegU cr, rRegL src1, memory src2)
%{
  predicate(UseAPX);
  match(Set dst (CMoveL (Binary cop cr) (Binary src1 (LoadL src2))));

  ins_cost(200);
  format %{ "ecmovq$cop $dst, $src1, $src2\t# unsigned, long ndd" %}
  ins_encode %{
    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);
  %}
  ins_pipe(pipe_cmov_mem);
%}

instruct cmovL_rReg_rReg_memUCF_ndd(rRegL dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegL src1, memory src2)
%{
  predicate(UseAPX);
  match(Set dst (CMoveL (Binary cop cr) (Binary src1 (LoadL src2))));
  ins_cost(200);
  format %{ "ecmovq$cop $dst, $src1, $src2\t# unsigned, long ndd" %}
  ins_encode %{
    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);
  %}
  ins_pipe(pipe_cmov_mem);
%}

instruct cmovF_reg(cmpOp cop, rFlagsReg cr, regF dst, regF src)
%{
  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));

  ins_cost(200); // XXX
  format %{ "jn$cop    skip\t# signed cmove float\n\t"
            "movss     $dst, $src\n"
    "skip:" %}
  ins_encode %{
    Label Lskip;
    // Invert sense of branch from sense of CMOV
    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);
    __ movflt($dst$$XMMRegister, $src$$XMMRegister);
    __ bind(Lskip);
  %}
  ins_pipe(pipe_slow);
%}

instruct cmovF_regU(cmpOpU cop, rFlagsRegU cr, regF dst, regF src)
%{
  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));

  ins_cost(200); // XXX
  format %{ "jn$cop    skip\t# unsigned cmove float\n\t"
            "movss     $dst, $src\n"
    "skip:" %}
  ins_encode %{
    Label Lskip;
    // Invert sense of branch from sense of CMOV
    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);
    __ movflt($dst$$XMMRegister, $src$$XMMRegister);
    __ bind(Lskip);
  %}
  ins_pipe(pipe_slow);
%}

instruct cmovF_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, regF dst, regF src) %{
  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));
  ins_cost(200);
  expand %{
    cmovF_regU(cop, cr, dst, src);
  %}
%}

instruct cmovD_reg(cmpOp cop, rFlagsReg cr, regD dst, regD src)
%{
  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));

  ins_cost(200); // XXX
  format %{ "jn$cop    skip\t# signed cmove double\n\t"
            "movsd     $dst, $src\n"
    "skip:" %}
  ins_encode %{
    Label Lskip;
    // Invert sense of branch from sense of CMOV
    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);
    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
    __ bind(Lskip);
  %}
  ins_pipe(pipe_slow);
%}

instruct cmovD_regU(cmpOpU cop, rFlagsRegU cr, regD dst, regD src)
%{
  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));

  ins_cost(200); // XXX
  format %{ "jn$cop    skip\t# unsigned cmove double\n\t"
            "movsd     $dst, $src\n"
    "skip:" %}
  ins_encode %{
    Label Lskip;
    // Invert sense of branch from sense of CMOV
    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);
    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
    __ bind(Lskip);
  %}
  ins_pipe(pipe_slow);
%}

instruct cmovD_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, regD dst, regD src) %{
  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));
  ins_cost(200);
  expand %{
    cmovD_regU(cop, cr, dst, src);
  %}
%}

//----------Arithmetic Instructions--------------------------------------------
//----------Addition Instructions----------------------------------------------

instruct addI_rReg(rRegI dst, rRegI src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (AddI dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);
  format %{ "addl    $dst, $src\t# int" %}
  ins_encode %{
    __ addl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct addI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (AddI src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "eaddl    $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ eaddl($dst$$Register, $src1$$Register, $src2$$Register, false);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct addI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (AddI dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "addl    $dst, $src\t# int" %}
  ins_encode %{
    __ addl($dst$$Register, $src$$constant);
  %}
  ins_pipe( ialu_reg );
%}

instruct addI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (AddI src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "eaddl    $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ eaddl($dst$$Register, $src1$$Register, $src2$$constant, false);
  %}
  ins_pipe( ialu_reg );
%}

instruct addI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (AddI (LoadI src1) src2));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "eaddl    $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ eaddl($dst$$Register, $src1$$Address, $src2$$constant, false);
  %}
  ins_pipe( ialu_reg );
%}

instruct addI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (AddI dst (LoadI src)));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  ins_cost(150); // XXX
  format %{ "addl    $dst, $src\t# int" %}
  ins_encode %{
    __ addl($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct addI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (AddI src1 (LoadI src2)));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  ins_cost(150);
  format %{ "eaddl    $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ eaddl($dst$$Register, $src1$$Register, $src2$$Address, false);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct addI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)
%{
  match(Set dst (StoreI dst (AddI (LoadI dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  ins_cost(150); // XXX
  format %{ "addl    $dst, $src\t# int" %}
  ins_encode %{
    __ addl($dst$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct addI_mem_imm(memory dst, immI src, rFlagsReg cr)
%{
  match(Set dst (StoreI dst (AddI (LoadI dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);


  ins_cost(125); // XXX
  format %{ "addl    $dst, $src\t# int" %}
  ins_encode %{
    __ addl($dst$$Address, $src$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

instruct incI_rReg(rRegI dst, immI_1 src, rFlagsReg cr)
%{
  predicate(!UseAPX && UseIncDec);
  match(Set dst (AddI dst src));
  effect(KILL cr);

  format %{ "incl    $dst\t# int" %}
  ins_encode %{
    __ incrementl($dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct incI_rReg_ndd(rRegI dst, rRegI src, immI_1 val, rFlagsReg cr)
%{
  predicate(UseAPX && UseIncDec);
  match(Set dst (AddI src val));
  effect(KILL cr);

  format %{ "eincl    $dst, $src\t# int ndd" %}
  ins_encode %{
    __ eincl($dst$$Register, $src$$Register, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct incI_rReg_mem_ndd(rRegI dst, memory src, immI_1 val, rFlagsReg cr)
%{
  predicate(UseAPX && UseIncDec);
  match(Set dst (AddI (LoadI src) val));
  effect(KILL cr);

  format %{ "eincl    $dst, $src\t# int ndd" %}
  ins_encode %{
    __ eincl($dst$$Register, $src$$Address, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct incI_mem(memory dst, immI_1 src, rFlagsReg cr)
%{
  predicate(UseIncDec);
  match(Set dst (StoreI dst (AddI (LoadI dst) src)));
  effect(KILL cr);

  ins_cost(125); // XXX
  format %{ "incl    $dst\t# int" %}
  ins_encode %{
    __ incrementl($dst$$Address);
  %}
  ins_pipe(ialu_mem_imm);
%}

// XXX why does that use AddI
instruct decI_rReg(rRegI dst, immI_M1 src, rFlagsReg cr)
%{
  predicate(!UseAPX && UseIncDec);
  match(Set dst (AddI dst src));
  effect(KILL cr);

  format %{ "decl    $dst\t# int" %}
  ins_encode %{
    __ decrementl($dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct decI_rReg_ndd(rRegI dst, rRegI src, immI_M1 val, rFlagsReg cr)
%{
  predicate(UseAPX && UseIncDec);
  match(Set dst (AddI src val));
  effect(KILL cr);

  format %{ "edecl    $dst, $src\t# int ndd" %}
  ins_encode %{
    __ edecl($dst$$Register, $src$$Register, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct decI_rReg_mem_ndd(rRegI dst, memory src, immI_M1 val, rFlagsReg cr)
%{
  predicate(UseAPX && UseIncDec);
  match(Set dst (AddI (LoadI src) val));
  effect(KILL cr);

  format %{ "edecl    $dst, $src\t# int ndd" %}
  ins_encode %{
    __ edecl($dst$$Register, $src$$Address, false);
  %}
  ins_pipe(ialu_reg);
%}

// XXX why does that use AddI
instruct decI_mem(memory dst, immI_M1 src, rFlagsReg cr)
%{
  predicate(UseIncDec);
  match(Set dst (StoreI dst (AddI (LoadI dst) src)));
  effect(KILL cr);

  ins_cost(125); // XXX
  format %{ "decl    $dst\t# int" %}
  ins_encode %{
    __ decrementl($dst$$Address);
  %}
  ins_pipe(ialu_mem_imm);
%}

instruct leaI_rReg_immI2_immI(rRegI dst, rRegI index, immI2 scale, immI disp)
%{
  predicate(VM_Version::supports_fast_2op_lea());
  match(Set dst (AddI (LShiftI index scale) disp));

  format %{ "leal $dst, [$index << $scale + $disp]\t# int" %}
  ins_encode %{
    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);
    __ leal($dst$$Register, Address(noreg, $index$$Register, scale, $disp$$constant));
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct leaI_rReg_rReg_immI(rRegI dst, rRegI base, rRegI index, immI disp)
%{
  predicate(VM_Version::supports_fast_3op_lea());
  match(Set dst (AddI (AddI base index) disp));

  format %{ "leal $dst, [$base + $index + $disp]\t# int" %}
  ins_encode %{
    __ leal($dst$$Register, Address($base$$Register, $index$$Register, Address::times_1, $disp$$constant));
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct leaI_rReg_rReg_immI2(rRegI dst, no_rbp_r13_RegI base, rRegI index, immI2 scale)
%{
  predicate(VM_Version::supports_fast_2op_lea());
  match(Set dst (AddI base (LShiftI index scale)));

  format %{ "leal $dst, [$base + $index << $scale]\t# int" %}
  ins_encode %{
    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);
    __ leal($dst$$Register, Address($base$$Register, $index$$Register, scale));
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct leaI_rReg_rReg_immI2_immI(rRegI dst, rRegI base, rRegI index, immI2 scale, immI disp)
%{
  predicate(VM_Version::supports_fast_3op_lea());
  match(Set dst (AddI (AddI base (LShiftI index scale)) disp));

  format %{ "leal $dst, [$base + $index << $scale + $disp]\t# int" %}
  ins_encode %{
    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);
    __ leal($dst$$Register, Address($base$$Register, $index$$Register, scale, $disp$$constant));
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct addL_rReg(rRegL dst, rRegL src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (AddL dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "addq    $dst, $src\t# long" %}
  ins_encode %{
    __ addq($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct addL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (AddL src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "eaddq    $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ eaddq($dst$$Register, $src1$$Register, $src2$$Register, false);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct addL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (AddL dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "addq    $dst, $src\t# long" %}
  ins_encode %{
    __ addq($dst$$Register, $src$$constant);
  %}
  ins_pipe( ialu_reg );
%}

instruct addL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (AddL src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "eaddq    $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ eaddq($dst$$Register, $src1$$Register, $src2$$constant, false);
  %}
  ins_pipe( ialu_reg );
%}

instruct addL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (AddL (LoadL src1) src2));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "eaddq    $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ eaddq($dst$$Register, $src1$$Address, $src2$$constant, false);
  %}
  ins_pipe( ialu_reg );
%}

instruct addL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (AddL dst (LoadL src)));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  ins_cost(150); // XXX
  format %{ "addq    $dst, $src\t# long" %}
  ins_encode %{
    __ addq($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct addL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (AddL src1 (LoadL src2)));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  ins_cost(150);
  format %{ "eaddq    $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ eaddq($dst$$Register, $src1$$Register, $src2$$Address, false);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct addL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)
%{
  match(Set dst (StoreL dst (AddL (LoadL dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  ins_cost(150); // XXX
  format %{ "addq    $dst, $src\t# long" %}
  ins_encode %{
    __ addq($dst$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct addL_mem_imm(memory dst, immL32 src, rFlagsReg cr)
%{
  match(Set dst (StoreL dst (AddL (LoadL dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  ins_cost(125); // XXX
  format %{ "addq    $dst, $src\t# long" %}
  ins_encode %{
    __ addq($dst$$Address, $src$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

instruct incL_rReg(rRegL dst, immL1 src, rFlagsReg cr)
%{
  predicate(!UseAPX && UseIncDec);
  match(Set dst (AddL dst src));
  effect(KILL cr);

  format %{ "incq    $dst\t# long" %}
  ins_encode %{
    __ incrementq($dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct incL_rReg_ndd(rRegL dst, rRegI src, immL1 val, rFlagsReg cr)
%{
  predicate(UseAPX && UseIncDec);
  match(Set dst (AddL src val));
  effect(KILL cr);

  format %{ "eincq    $dst, $src\t# long ndd" %}
  ins_encode %{
    __ eincq($dst$$Register, $src$$Register, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct incL_rReg_mem_ndd(rRegL dst, memory src, immL1 val, rFlagsReg cr)
%{
  predicate(UseAPX && UseIncDec);
  match(Set dst (AddL (LoadL src) val));
  effect(KILL cr);

  format %{ "eincq    $dst, $src\t# long ndd" %}
  ins_encode %{
    __ eincq($dst$$Register, $src$$Address, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct incL_mem(memory dst, immL1 src, rFlagsReg cr)
%{
  predicate(UseIncDec);
  match(Set dst (StoreL dst (AddL (LoadL dst) src)));
  effect(KILL cr);

  ins_cost(125); // XXX
  format %{ "incq    $dst\t# long" %}
  ins_encode %{
    __ incrementq($dst$$Address);
  %}
  ins_pipe(ialu_mem_imm);
%}

// XXX why does that use AddL
instruct decL_rReg(rRegL dst, immL_M1 src, rFlagsReg cr)
%{
  predicate(!UseAPX && UseIncDec);
  match(Set dst (AddL dst src));
  effect(KILL cr);

  format %{ "decq    $dst\t# long" %}
  ins_encode %{
    __ decrementq($dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct decL_rReg_ndd(rRegL dst, rRegL src, immL_M1 val, rFlagsReg cr)
%{
  predicate(UseAPX && UseIncDec);
  match(Set dst (AddL src val));
  effect(KILL cr);

  format %{ "edecq    $dst, $src\t# long ndd" %}
  ins_encode %{
    __ edecq($dst$$Register, $src$$Register, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct decL_rReg_mem_ndd(rRegL dst, memory src, immL_M1 val, rFlagsReg cr)
%{
  predicate(UseAPX && UseIncDec);
  match(Set dst (AddL (LoadL src) val));
  effect(KILL cr);

  format %{ "edecq    $dst, $src\t# long ndd" %}
  ins_encode %{
    __ edecq($dst$$Register, $src$$Address, false);
  %}
  ins_pipe(ialu_reg);
%}

// XXX why does that use AddL
instruct decL_mem(memory dst, immL_M1 src, rFlagsReg cr)
%{
  predicate(UseIncDec);
  match(Set dst (StoreL dst (AddL (LoadL dst) src)));
  effect(KILL cr);

  ins_cost(125); // XXX
  format %{ "decq    $dst\t# long" %}
  ins_encode %{
    __ decrementq($dst$$Address);
  %}
  ins_pipe(ialu_mem_imm);
%}

instruct leaL_rReg_immI2_immL32(rRegL dst, rRegL index, immI2 scale, immL32 disp)
%{
  predicate(VM_Version::supports_fast_2op_lea());
  match(Set dst (AddL (LShiftL index scale) disp));

  format %{ "leaq $dst, [$index << $scale + $disp]\t# long" %}
  ins_encode %{
    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);
    __ leaq($dst$$Register, Address(noreg, $index$$Register, scale, $disp$$constant));
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct leaL_rReg_rReg_immL32(rRegL dst, rRegL base, rRegL index, immL32 disp)
%{
  predicate(VM_Version::supports_fast_3op_lea());
  match(Set dst (AddL (AddL base index) disp));

  format %{ "leaq $dst, [$base + $index + $disp]\t# long" %}
  ins_encode %{
    __ leaq($dst$$Register, Address($base$$Register, $index$$Register, Address::times_1, $disp$$constant));
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct leaL_rReg_rReg_immI2(rRegL dst, no_rbp_r13_RegL base, rRegL index, immI2 scale)
%{
  predicate(VM_Version::supports_fast_2op_lea());
  match(Set dst (AddL base (LShiftL index scale)));

  format %{ "leaq $dst, [$base + $index << $scale]\t# long" %}
  ins_encode %{
    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);
    __ leaq($dst$$Register, Address($base$$Register, $index$$Register, scale));
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct leaL_rReg_rReg_immI2_immL32(rRegL dst, rRegL base, rRegL index, immI2 scale, immL32 disp)
%{
  predicate(VM_Version::supports_fast_3op_lea());
  match(Set dst (AddL (AddL base (LShiftL index scale)) disp));

  format %{ "leaq $dst, [$base + $index << $scale + $disp]\t# long" %}
  ins_encode %{
    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);
    __ leaq($dst$$Register, Address($base$$Register, $index$$Register, scale, $disp$$constant));
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct addP_rReg(rRegP dst, rRegL src, rFlagsReg cr)
%{
  match(Set dst (AddP dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "addq    $dst, $src\t# ptr" %}
  ins_encode %{
    __ addq($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct addP_rReg_imm(rRegP dst, immL32 src, rFlagsReg cr)
%{
  match(Set dst (AddP dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "addq    $dst, $src\t# ptr" %}
  ins_encode %{
    __ addq($dst$$Register, $src$$constant);
  %}
  ins_pipe( ialu_reg );
%}

// XXX addP mem ops ????

instruct checkCastPP(rRegP dst)
%{
  match(Set dst (CheckCastPP dst));

  size(0);
  format %{ "# checkcastPP of $dst" %}
  ins_encode(/* empty encoding */);
  ins_pipe(empty);
%}

instruct castPP(rRegP dst)
%{
  match(Set dst (CastPP dst));

  size(0);
  format %{ "# castPP of $dst" %}
  ins_encode(/* empty encoding */);
  ins_pipe(empty);
%}

instruct castII(rRegI dst)
%{
  predicate(VerifyConstraintCasts == 0);
  match(Set dst (CastII dst));

  size(0);
  format %{ "# castII of $dst" %}
  ins_encode(/* empty encoding */);
  ins_cost(0);
  ins_pipe(empty);
%}

instruct castII_checked(rRegI dst, rFlagsReg cr)
%{
  predicate(VerifyConstraintCasts > 0);
  match(Set dst (CastII dst));

  effect(KILL cr);
  format %{ "# cast_checked_II $dst" %}
  ins_encode %{
    __ verify_int_in_range(_idx, bottom_type()->is_int(), $dst$$Register);
  %}
  ins_pipe(pipe_slow);
%}

instruct castLL(rRegL dst)
%{
  predicate(VerifyConstraintCasts == 0);
  match(Set dst (CastLL dst));

  size(0);
  format %{ "# castLL of $dst" %}
  ins_encode(/* empty encoding */);
  ins_cost(0);
  ins_pipe(empty);
%}

instruct castLL_checked_L32(rRegL dst, rFlagsReg cr)
%{
  predicate(VerifyConstraintCasts > 0 && castLL_is_imm32(n));
  match(Set dst (CastLL dst));

  effect(KILL cr);
  format %{ "# cast_checked_LL $dst" %}
  ins_encode %{
    __ verify_long_in_range(_idx, bottom_type()->is_long(), $dst$$Register, noreg);
  %}
  ins_pipe(pipe_slow);
%}

instruct castLL_checked(rRegL dst, rRegL tmp, rFlagsReg cr)
%{
  predicate(VerifyConstraintCasts > 0 && !castLL_is_imm32(n));
  match(Set dst (CastLL dst));

  effect(KILL cr, TEMP tmp);
  format %{ "# cast_checked_LL $dst\tusing $tmp as TEMP" %}
  ins_encode %{
    __ verify_long_in_range(_idx, bottom_type()->is_long(), $dst$$Register, $tmp$$Register);
  %}
  ins_pipe(pipe_slow);
%}

instruct castFF(regF dst)
%{
  match(Set dst (CastFF dst));

  size(0);
  format %{ "# castFF of $dst" %}
  ins_encode(/* empty encoding */);
  ins_cost(0);
  ins_pipe(empty);
%}

instruct castHH(regF dst)
%{
  match(Set dst (CastHH dst));

  size(0);
  format %{ "# castHH of $dst" %}
  ins_encode(/* empty encoding */);
  ins_cost(0);
  ins_pipe(empty);
%}

instruct castDD(regD dst)
%{
  match(Set dst (CastDD dst));

  size(0);
  format %{ "# castDD of $dst" %}
  ins_encode(/* empty encoding */);
  ins_cost(0);
  ins_pipe(empty);
%}

// XXX No flag versions for CompareAndSwap{P,I,L} because matcher can't match them
instruct compareAndSwapP(rRegI res,
                         memory mem_ptr,
                         rax_RegP oldval, rRegP newval,
                         rFlagsReg cr)
%{
  predicate(n->as_LoadStore()->barrier_data() == 0);
  match(Set res (CompareAndSwapP mem_ptr (Binary oldval newval)));
  match(Set res (WeakCompareAndSwapP mem_ptr (Binary oldval newval)));
  effect(KILL cr, KILL oldval);

  format %{ "cmpxchgq $mem_ptr,$newval\t# "
            "If rax == $mem_ptr then store $newval into $mem_ptr\n\t"
            "setcc $res \t# emits sete + movzbl or setzue for APX" %}
  ins_encode %{
    __ lock();
    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);
    __ setcc(Assembler::equal, $res$$Register);
  %}
  ins_pipe( pipe_cmpxchg );
%}

instruct compareAndSwapL(rRegI res,
                         memory mem_ptr,
                         rax_RegL oldval, rRegL newval,
                         rFlagsReg cr)
%{
  match(Set res (CompareAndSwapL mem_ptr (Binary oldval newval)));
  match(Set res (WeakCompareAndSwapL mem_ptr (Binary oldval newval)));
  effect(KILL cr, KILL oldval);

  format %{ "cmpxchgq $mem_ptr,$newval\t# "
            "If rax == $mem_ptr then store $newval into $mem_ptr\n\t"
            "setcc $res \t# emits sete + movzbl or setzue for APX" %}
  ins_encode %{
    __ lock();
    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);
    __ setcc(Assembler::equal, $res$$Register);
  %}
  ins_pipe( pipe_cmpxchg );
%}

instruct compareAndSwapI(rRegI res,
                         memory mem_ptr,
                         rax_RegI oldval, rRegI newval,
                         rFlagsReg cr)
%{
  match(Set res (CompareAndSwapI mem_ptr (Binary oldval newval)));
  match(Set res (WeakCompareAndSwapI mem_ptr (Binary oldval newval)));
  effect(KILL cr, KILL oldval);

  format %{ "cmpxchgl $mem_ptr,$newval\t# "
            "If rax == $mem_ptr then store $newval into $mem_ptr\n\t"
            "setcc $res \t# emits sete + movzbl or setzue for APX" %}
  ins_encode %{
    __ lock();
    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);
    __ setcc(Assembler::equal, $res$$Register);
  %}
  ins_pipe( pipe_cmpxchg );
%}

instruct compareAndSwapB(rRegI res,
                         memory mem_ptr,
                         rax_RegI oldval, rRegI newval,
                         rFlagsReg cr)
%{
  match(Set res (CompareAndSwapB mem_ptr (Binary oldval newval)));
  match(Set res (WeakCompareAndSwapB mem_ptr (Binary oldval newval)));
  effect(KILL cr, KILL oldval);

  format %{ "cmpxchgb $mem_ptr,$newval\t# "
            "If rax == $mem_ptr then store $newval into $mem_ptr\n\t"
            "setcc $res \t# emits sete + movzbl or setzue for APX" %}
  ins_encode %{
    __ lock();
    __ cmpxchgb($newval$$Register, $mem_ptr$$Address);
    __ setcc(Assembler::equal, $res$$Register);
  %}
  ins_pipe( pipe_cmpxchg );
%}

instruct compareAndSwapS(rRegI res,
                         memory mem_ptr,
                         rax_RegI oldval, rRegI newval,
                         rFlagsReg cr)
%{
  match(Set res (CompareAndSwapS mem_ptr (Binary oldval newval)));
  match(Set res (WeakCompareAndSwapS mem_ptr (Binary oldval newval)));
  effect(KILL cr, KILL oldval);

  format %{ "cmpxchgw $mem_ptr,$newval\t# "
            "If rax == $mem_ptr then store $newval into $mem_ptr\n\t"
            "setcc $res \t# emits sete + movzbl or setzue for APX" %}
  ins_encode %{
    __ lock();
    __ cmpxchgw($newval$$Register, $mem_ptr$$Address);
    __ setcc(Assembler::equal, $res$$Register);
  %}
  ins_pipe( pipe_cmpxchg );
%}

instruct compareAndSwapN(rRegI res,
                          memory mem_ptr,
                          rax_RegN oldval, rRegN newval,
                          rFlagsReg cr) %{
  predicate(n->as_LoadStore()->barrier_data() == 0);
  match(Set res (CompareAndSwapN mem_ptr (Binary oldval newval)));
  match(Set res (WeakCompareAndSwapN mem_ptr (Binary oldval newval)));
  effect(KILL cr, KILL oldval);

  format %{ "cmpxchgl $mem_ptr,$newval\t# "
            "If rax == $mem_ptr then store $newval into $mem_ptr\n\t"
            "setcc $res \t# emits sete + movzbl or setzue for APX" %}
  ins_encode %{
    __ lock();
    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);
    __ setcc(Assembler::equal, $res$$Register);
  %}
  ins_pipe( pipe_cmpxchg );
%}

instruct compareAndExchangeB(
                         memory mem_ptr,
                         rax_RegI oldval, rRegI newval,
                         rFlagsReg cr)
%{
  match(Set oldval (CompareAndExchangeB mem_ptr (Binary oldval newval)));
  effect(KILL cr);

  format %{ "cmpxchgb $mem_ptr,$newval\t# "
            "If rax == $mem_ptr then store $newval into $mem_ptr\n\t"  %}
  ins_encode %{
    __ lock();
    __ cmpxchgb($newval$$Register, $mem_ptr$$Address);
  %}
  ins_pipe( pipe_cmpxchg );
%}

instruct compareAndExchangeS(
                         memory mem_ptr,
                         rax_RegI oldval, rRegI newval,
                         rFlagsReg cr)
%{
  match(Set oldval (CompareAndExchangeS mem_ptr (Binary oldval newval)));
  effect(KILL cr);

  format %{ "cmpxchgw $mem_ptr,$newval\t# "
            "If rax == $mem_ptr then store $newval into $mem_ptr\n\t"  %}
  ins_encode %{
    __ lock();
    __ cmpxchgw($newval$$Register, $mem_ptr$$Address);
  %}
  ins_pipe( pipe_cmpxchg );
%}

instruct compareAndExchangeI(
                         memory mem_ptr,
                         rax_RegI oldval, rRegI newval,
                         rFlagsReg cr)
%{
  match(Set oldval (CompareAndExchangeI mem_ptr (Binary oldval newval)));
  effect(KILL cr);

  format %{ "cmpxchgl $mem_ptr,$newval\t# "
            "If rax == $mem_ptr then store $newval into $mem_ptr\n\t"  %}
  ins_encode %{
    __ lock();
    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);
  %}
  ins_pipe( pipe_cmpxchg );
%}

instruct compareAndExchangeL(
                         memory mem_ptr,
                         rax_RegL oldval, rRegL newval,
                         rFlagsReg cr)
%{
  match(Set oldval (CompareAndExchangeL mem_ptr (Binary oldval newval)));
  effect(KILL cr);

  format %{ "cmpxchgq $mem_ptr,$newval\t# "
            "If rax == $mem_ptr then store $newval into $mem_ptr\n\t"  %}
  ins_encode %{
    __ lock();
    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);
  %}
  ins_pipe( pipe_cmpxchg );
%}

instruct compareAndExchangeN(
                          memory mem_ptr,
                          rax_RegN oldval, rRegN newval,
                          rFlagsReg cr) %{
  predicate(n->as_LoadStore()->barrier_data() == 0);
  match(Set oldval (CompareAndExchangeN mem_ptr (Binary oldval newval)));
  effect(KILL cr);

  format %{ "cmpxchgl $mem_ptr,$newval\t# "
            "If rax == $mem_ptr then store $newval into $mem_ptr\n\t" %}
  ins_encode %{
    __ lock();
    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);
  %}
  ins_pipe( pipe_cmpxchg );
%}

instruct compareAndExchangeP(
                         memory mem_ptr,
                         rax_RegP oldval, rRegP newval,
                         rFlagsReg cr)
%{
  predicate(n->as_LoadStore()->barrier_data() == 0);
  match(Set oldval (CompareAndExchangeP mem_ptr (Binary oldval newval)));
  effect(KILL cr);

  format %{ "cmpxchgq $mem_ptr,$newval\t# "
            "If rax == $mem_ptr then store $newval into $mem_ptr\n\t" %}
  ins_encode %{
    __ lock();
    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);
  %}
  ins_pipe( pipe_cmpxchg );
%}

instruct xaddB_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{
  predicate(n->as_LoadStore()->result_not_used());
  match(Set dummy (GetAndAddB mem add));
  effect(KILL cr);
  format %{ "addb_lock   $mem, $add" %}
  ins_encode %{
    __ lock();
    __ addb($mem$$Address, $add$$Register);
  %}
  ins_pipe(pipe_cmpxchg);
%}

instruct xaddB_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{
  predicate(n->as_LoadStore()->result_not_used());
  match(Set dummy (GetAndAddB mem add));
  effect(KILL cr);
  format %{ "addb_lock   $mem, $add" %}
  ins_encode %{
    __ lock();
    __ addb($mem$$Address, $add$$constant);
  %}
  ins_pipe(pipe_cmpxchg);
%}

instruct xaddB(memory mem, rRegI newval, rFlagsReg cr) %{
  predicate(!n->as_LoadStore()->result_not_used());
  match(Set newval (GetAndAddB mem newval));
  effect(KILL cr);
  format %{ "xaddb_lock  $mem, $newval" %}
  ins_encode %{
    __ lock();
    __ xaddb($mem$$Address, $newval$$Register);
  %}
  ins_pipe(pipe_cmpxchg);
%}

instruct xaddS_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{
  predicate(n->as_LoadStore()->result_not_used());
  match(Set dummy (GetAndAddS mem add));
  effect(KILL cr);
  format %{ "addw_lock   $mem, $add" %}
  ins_encode %{
    __ lock();
    __ addw($mem$$Address, $add$$Register);
  %}
  ins_pipe(pipe_cmpxchg);
%}

instruct xaddS_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{
  predicate(UseStoreImmI16 && n->as_LoadStore()->result_not_used());
  match(Set dummy (GetAndAddS mem add));
  effect(KILL cr);
  format %{ "addw_lock   $mem, $add" %}
  ins_encode %{
    __ lock();
    __ addw($mem$$Address, $add$$constant);
  %}
  ins_pipe(pipe_cmpxchg);
%}

instruct xaddS(memory mem, rRegI newval, rFlagsReg cr) %{
  predicate(!n->as_LoadStore()->result_not_used());
  match(Set newval (GetAndAddS mem newval));
  effect(KILL cr);
  format %{ "xaddw_lock  $mem, $newval" %}
  ins_encode %{
    __ lock();
    __ xaddw($mem$$Address, $newval$$Register);
  %}
  ins_pipe(pipe_cmpxchg);
%}

instruct xaddI_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{
  predicate(n->as_LoadStore()->result_not_used());
  match(Set dummy (GetAndAddI mem add));
  effect(KILL cr);
  format %{ "addl_lock   $mem, $add" %}
  ins_encode %{
    __ lock();
    __ addl($mem$$Address, $add$$Register);
  %}
  ins_pipe(pipe_cmpxchg);
%}

instruct xaddI_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{
  predicate(n->as_LoadStore()->result_not_used());
  match(Set dummy (GetAndAddI mem add));
  effect(KILL cr);
  format %{ "addl_lock   $mem, $add" %}
  ins_encode %{
    __ lock();
    __ addl($mem$$Address, $add$$constant);
  %}
  ins_pipe(pipe_cmpxchg);
%}

instruct xaddI(memory mem, rRegI newval, rFlagsReg cr) %{
  predicate(!n->as_LoadStore()->result_not_used());
  match(Set newval (GetAndAddI mem newval));
  effect(KILL cr);
  format %{ "xaddl_lock  $mem, $newval" %}
  ins_encode %{
    __ lock();
    __ xaddl($mem$$Address, $newval$$Register);
  %}
  ins_pipe(pipe_cmpxchg);
%}

instruct xaddL_reg_no_res(memory mem, Universe dummy, rRegL add, rFlagsReg cr) %{
  predicate(n->as_LoadStore()->result_not_used());
  match(Set dummy (GetAndAddL mem add));
  effect(KILL cr);
  format %{ "addq_lock   $mem, $add" %}
  ins_encode %{
    __ lock();
    __ addq($mem$$Address, $add$$Register);
  %}
  ins_pipe(pipe_cmpxchg);
%}

instruct xaddL_imm_no_res(memory mem, Universe dummy, immL32 add, rFlagsReg cr) %{
  predicate(n->as_LoadStore()->result_not_used());
  match(Set dummy (GetAndAddL mem add));
  effect(KILL cr);
  format %{ "addq_lock   $mem, $add" %}
  ins_encode %{
    __ lock();
    __ addq($mem$$Address, $add$$constant);
  %}
  ins_pipe(pipe_cmpxchg);
%}

instruct xaddL(memory mem, rRegL newval, rFlagsReg cr) %{
  predicate(!n->as_LoadStore()->result_not_used());
  match(Set newval (GetAndAddL mem newval));
  effect(KILL cr);
  format %{ "xaddq_lock  $mem, $newval" %}
  ins_encode %{
    __ lock();
    __ xaddq($mem$$Address, $newval$$Register);
  %}
  ins_pipe(pipe_cmpxchg);
%}

instruct xchgB( memory mem, rRegI newval) %{
  match(Set newval (GetAndSetB mem newval));
  format %{ "XCHGB  $newval,[$mem]" %}
  ins_encode %{
    __ xchgb($newval$$Register, $mem$$Address);
  %}
  ins_pipe( pipe_cmpxchg );
%}

instruct xchgS( memory mem, rRegI newval) %{
  match(Set newval (GetAndSetS mem newval));
  format %{ "XCHGW  $newval,[$mem]" %}
  ins_encode %{
    __ xchgw($newval$$Register, $mem$$Address);
  %}
  ins_pipe( pipe_cmpxchg );
%}

instruct xchgI( memory mem, rRegI newval) %{
  match(Set newval (GetAndSetI mem newval));
  format %{ "XCHGL  $newval,[$mem]" %}
  ins_encode %{
    __ xchgl($newval$$Register, $mem$$Address);
  %}
  ins_pipe( pipe_cmpxchg );
%}

instruct xchgL( memory mem, rRegL newval) %{
  match(Set newval (GetAndSetL mem newval));
  format %{ "XCHGL  $newval,[$mem]" %}
  ins_encode %{
    __ xchgq($newval$$Register, $mem$$Address);
  %}
  ins_pipe( pipe_cmpxchg );
%}

instruct xchgP( memory mem, rRegP newval) %{
  match(Set newval (GetAndSetP mem newval));
  predicate(n->as_LoadStore()->barrier_data() == 0);
  format %{ "XCHGQ  $newval,[$mem]" %}
  ins_encode %{
    __ xchgq($newval$$Register, $mem$$Address);
  %}
  ins_pipe( pipe_cmpxchg );
%}

instruct xchgN( memory mem, rRegN newval) %{
  predicate(n->as_LoadStore()->barrier_data() == 0);
  match(Set newval (GetAndSetN mem newval));
  format %{ "XCHGL  $newval,$mem]" %}
  ins_encode %{
    __ xchgl($newval$$Register, $mem$$Address);
  %}
  ins_pipe( pipe_cmpxchg );
%}

//----------Abs Instructions-------------------------------------------

// Integer Absolute Instructions
instruct absI_rReg(rRegI dst, rRegI src, rFlagsReg cr)
%{
  match(Set dst (AbsI src));
  effect(TEMP dst, KILL cr);
  format %{ "xorl    $dst, $dst\t# abs int\n\t"
            "subl    $dst, $src\n\t"
            "cmovll  $dst, $src" %}
  ins_encode %{
    __ xorl($dst$$Register, $dst$$Register);
    __ subl($dst$$Register, $src$$Register);
    __ cmovl(Assembler::less, $dst$$Register, $src$$Register);
  %}

  ins_pipe(ialu_reg_reg);
%}

// Long Absolute Instructions
instruct absL_rReg(rRegL dst, rRegL src, rFlagsReg cr)
%{
  match(Set dst (AbsL src));
  effect(TEMP dst, KILL cr);
  format %{ "xorl    $dst, $dst\t# abs long\n\t"
            "subq    $dst, $src\n\t"
            "cmovlq  $dst, $src" %}
  ins_encode %{
    __ xorl($dst$$Register, $dst$$Register);
    __ subq($dst$$Register, $src$$Register);
    __ cmovq(Assembler::less, $dst$$Register, $src$$Register);
  %}

  ins_pipe(ialu_reg_reg);
%}

//----------Subtraction Instructions-------------------------------------------

// Integer Subtraction Instructions
instruct subI_rReg(rRegI dst, rRegI src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (SubI dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "subl    $dst, $src\t# int" %}
  ins_encode %{
    __ subl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct subI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (SubI src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "esubl    $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ esubl($dst$$Register, $src1$$Register, $src2$$Register, false);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct subI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (SubI src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "esubl    $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ esubl($dst$$Register, $src1$$Register, $src2$$constant, false);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct subI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (SubI (LoadI src1) src2));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "esubl    $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ esubl($dst$$Register, $src1$$Address, $src2$$constant, false);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct subI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (SubI dst (LoadI src)));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  ins_cost(150);
  format %{ "subl    $dst, $src\t# int" %}
  ins_encode %{
    __ subl($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct subI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (SubI src1 (LoadI src2)));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  ins_cost(150);
  format %{ "esubl    $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ esubl($dst$$Register, $src1$$Register, $src2$$Address, false);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct subI_rReg_mem_rReg_ndd(rRegI dst, memory src1, rRegI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (SubI (LoadI src1) src2));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  ins_cost(150);
  format %{ "esubl    $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ esubl($dst$$Register, $src1$$Address, $src2$$Register, false);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct subI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)
%{
  match(Set dst (StoreI dst (SubI (LoadI dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  ins_cost(150);
  format %{ "subl    $dst, $src\t# int" %}
  ins_encode %{
    __ subl($dst$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct subL_rReg(rRegL dst, rRegL src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (SubL dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "subq    $dst, $src\t# long" %}
  ins_encode %{
    __ subq($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct subL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (SubL src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "esubq    $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ esubq($dst$$Register, $src1$$Register, $src2$$Register, false);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct subL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (SubL src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "esubq    $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ esubq($dst$$Register, $src1$$Register, $src2$$constant, false);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct subL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (SubL (LoadL src1) src2));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  format %{ "esubq    $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ esubq($dst$$Register, $src1$$Address, $src2$$constant, false);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct subL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (SubL dst (LoadL src)));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  ins_cost(150);
  format %{ "subq    $dst, $src\t# long" %}
  ins_encode %{
    __ subq($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct subL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (SubL src1 (LoadL src2)));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  ins_cost(150);
  format %{ "esubq    $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ esubq($dst$$Register, $src1$$Register, $src2$$Address, false);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct subL_rReg_mem_rReg_ndd(rRegL dst, memory src1, rRegL src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (SubL (LoadL src1) src2));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  ins_cost(150);
  format %{ "esubq    $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ esubq($dst$$Register, $src1$$Address, $src2$$Register, false);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct subL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)
%{
  match(Set dst (StoreL dst (SubL (LoadL dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);

  ins_cost(150);
  format %{ "subq    $dst, $src\t# long" %}
  ins_encode %{
    __ subq($dst$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

// Subtract from a pointer
// XXX hmpf???
instruct subP_rReg(rRegP dst, rRegI src, immI_0 zero, rFlagsReg cr)
%{
  match(Set dst (AddP dst (SubI zero src)));
  effect(KILL cr);

  format %{ "subq    $dst, $src\t# ptr - int" %}
  ins_encode %{
    __ subq($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct negI_rReg(rRegI dst, immI_0 zero, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (SubI zero dst));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);

  format %{ "negl    $dst\t# int" %}
  ins_encode %{
    __ negl($dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct negI_rReg_ndd(rRegI dst, rRegI src, immI_0 zero, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (SubI zero src));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);

  format %{ "enegl    $dst, $src\t# int ndd" %}
  ins_encode %{
    __ enegl($dst$$Register, $src$$Register, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct negI_rReg_2(rRegI dst, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (NegI dst));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);

  format %{ "negl    $dst\t# int" %}
  ins_encode %{
    __ negl($dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct negI_rReg_2_ndd(rRegI dst, rRegI src, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (NegI src));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);

  format %{ "enegl    $dst, $src\t# int ndd" %}
  ins_encode %{
    __ enegl($dst$$Register, $src$$Register, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct negI_mem(memory dst, immI_0 zero, rFlagsReg cr)
%{
  match(Set dst (StoreI dst (SubI zero (LoadI dst))));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);

  format %{ "negl    $dst\t# int" %}
  ins_encode %{
    __ negl($dst$$Address);
  %}
  ins_pipe(ialu_reg);
%}

instruct negL_rReg(rRegL dst, immL0 zero, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (SubL zero dst));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);

  format %{ "negq    $dst\t# long" %}
  ins_encode %{
    __ negq($dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct negL_rReg_ndd(rRegL dst, rRegL src, immL0 zero, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (SubL zero src));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);

  format %{ "enegq    $dst, $src\t# long ndd" %}
  ins_encode %{
    __ enegq($dst$$Register, $src$$Register, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct negL_rReg_2(rRegL dst, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (NegL dst));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);

  format %{ "negq    $dst\t# int" %}
  ins_encode %{
    __ negq($dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct negL_rReg_2_ndd(rRegL dst, rRegL src, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (NegL src));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);

  format %{ "enegq    $dst, $src\t# long ndd" %}
  ins_encode %{
    __ enegq($dst$$Register, $src$$Register, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct negL_mem(memory dst, immL0 zero, rFlagsReg cr)
%{
  match(Set dst (StoreL dst (SubL zero (LoadL dst))));
  effect(KILL cr);
  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);

  format %{ "negq    $dst\t# long" %}
  ins_encode %{
    __ negq($dst$$Address);
  %}
  ins_pipe(ialu_reg);
%}

//----------Multiplication/Division Instructions-------------------------------
// Integer Multiplication Instructions
// Multiply Register

instruct mulI_rReg(rRegI dst, rRegI src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (MulI dst src));
  effect(KILL cr);

  ins_cost(300);
  format %{ "imull   $dst, $src\t# int" %}
  ins_encode %{
    __ imull($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg_alu0);
%}

instruct mulI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (MulI src1 src2));
  effect(KILL cr);

  ins_cost(300);
  format %{ "eimull   $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ eimull($dst$$Register, $src1$$Register, $src2$$Register, false);
  %}
  ins_pipe(ialu_reg_reg_alu0);
%}

instruct mulI_rReg_imm(rRegI dst, rRegI src, immI imm, rFlagsReg cr)
%{
  match(Set dst (MulI src imm));
  effect(KILL cr);

  ins_cost(300);
  format %{ "imull   $dst, $src, $imm\t# int" %}
  ins_encode %{
    __ imull($dst$$Register, $src$$Register, $imm$$constant);
  %}
  ins_pipe(ialu_reg_reg_alu0);
%}

instruct mulI_mem(rRegI dst, memory src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (MulI dst (LoadI src)));
  effect(KILL cr);

  ins_cost(350);
  format %{ "imull   $dst, $src\t# int" %}
  ins_encode %{
    __ imull($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem_alu0);
%}

instruct mulI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (MulI src1 (LoadI src2)));
  effect(KILL cr);

  ins_cost(350);
  format %{ "eimull   $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ eimull($dst$$Register, $src1$$Register, $src2$$Address, false);
  %}
  ins_pipe(ialu_reg_mem_alu0);
%}

instruct mulI_mem_imm(rRegI dst, memory src, immI imm, rFlagsReg cr)
%{
  match(Set dst (MulI (LoadI src) imm));
  effect(KILL cr);

  ins_cost(300);
  format %{ "imull   $dst, $src, $imm\t# int" %}
  ins_encode %{
    __ imull($dst$$Register, $src$$Address, $imm$$constant);
  %}
  ins_pipe(ialu_reg_mem_alu0);
%}

instruct mulAddS2I_rReg(rRegI dst, rRegI src1, rRegI src2, rRegI src3, rFlagsReg cr)
%{
  match(Set dst (MulAddS2I (Binary dst src1) (Binary src2 src3)));
  effect(KILL cr, KILL src2);

  expand %{ mulI_rReg(dst, src1, cr);
           mulI_rReg(src2, src3, cr);
           addI_rReg(dst, src2, cr); %}
%}

instruct mulL_rReg(rRegL dst, rRegL src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (MulL dst src));
  effect(KILL cr);

  ins_cost(300);
  format %{ "imulq   $dst, $src\t# long" %}
  ins_encode %{
    __ imulq($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg_alu0);
%}

instruct mulL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (MulL src1 src2));
  effect(KILL cr);

  ins_cost(300);
  format %{ "eimulq   $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ eimulq($dst$$Register, $src1$$Register, $src2$$Register, false);
  %}
  ins_pipe(ialu_reg_reg_alu0);
%}

instruct mulL_rReg_imm(rRegL dst, rRegL src, immL32 imm, rFlagsReg cr)
%{
  match(Set dst (MulL src imm));
  effect(KILL cr);

  ins_cost(300);
  format %{ "imulq   $dst, $src, $imm\t# long" %}
  ins_encode %{
    __ imulq($dst$$Register, $src$$Register, $imm$$constant);
  %}
  ins_pipe(ialu_reg_reg_alu0);
%}

instruct mulL_mem(rRegL dst, memory src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (MulL dst (LoadL src)));
  effect(KILL cr);

  ins_cost(350);
  format %{ "imulq   $dst, $src\t# long" %}
  ins_encode %{
    __ imulq($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem_alu0);
%}

instruct mulL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (MulL src1 (LoadL src2)));
  effect(KILL cr);

  ins_cost(350);
  format %{ "eimulq   $dst, $src1, $src2 \t# long" %}
  ins_encode %{
    __ eimulq($dst$$Register, $src1$$Register, $src2$$Address, false);
  %}
  ins_pipe(ialu_reg_mem_alu0);
%}

instruct mulL_mem_imm(rRegL dst, memory src, immL32 imm, rFlagsReg cr)
%{
  match(Set dst (MulL (LoadL src) imm));
  effect(KILL cr);

  ins_cost(300);
  format %{ "imulq   $dst, $src, $imm\t# long" %}
  ins_encode %{
    __ imulq($dst$$Register, $src$$Address, $imm$$constant);
  %}
  ins_pipe(ialu_reg_mem_alu0);
%}

instruct mulHiL_rReg(rdx_RegL dst, rRegL src, rax_RegL rax, rFlagsReg cr)
%{
  match(Set dst (MulHiL src rax));
  effect(USE_KILL rax, KILL cr);

  ins_cost(300);
  format %{ "imulq   RDX:RAX, RAX, $src\t# mulhi" %}
  ins_encode %{
    __ imulq($src$$Register);
  %}
  ins_pipe(ialu_reg_reg_alu0);
%}

instruct umulHiL_rReg(rdx_RegL dst, rRegL src, rax_RegL rax, rFlagsReg cr)
%{
  match(Set dst (UMulHiL src rax));
  effect(USE_KILL rax, KILL cr);

  ins_cost(300);
  format %{ "mulq   RDX:RAX, RAX, $src\t# umulhi" %}
  ins_encode %{
    __ mulq($src$$Register);
  %}
  ins_pipe(ialu_reg_reg_alu0);
%}

instruct divI_rReg(rax_RegI rax, rdx_RegI rdx, no_rax_rdx_RegI div,
                   rFlagsReg cr)
%{
  match(Set rax (DivI rax div));
  effect(KILL rdx, KILL cr);

  ins_cost(30*100+10*100); // XXX
  format %{ "cmpl    rax, 0x80000000\t# idiv\n\t"
            "jne,s   normal\n\t"
            "xorl    rdx, rdx\n\t"
            "cmpl    $div, -1\n\t"
            "je,s    done\n"
    "normal: cdql\n\t"
            "idivl   $div\n"
    "done:"        %}
  ins_encode(cdql_enc(div));
  ins_pipe(ialu_reg_reg_alu0);
%}

instruct divL_rReg(rax_RegL rax, rdx_RegL rdx, no_rax_rdx_RegL div,
                   rFlagsReg cr)
%{
  match(Set rax (DivL rax div));
  effect(KILL rdx, KILL cr);

  ins_cost(30*100+10*100); // XXX
  format %{ "movq    rdx, 0x8000000000000000\t# ldiv\n\t"
            "cmpq    rax, rdx\n\t"
            "jne,s   normal\n\t"
            "xorl    rdx, rdx\n\t"
            "cmpq    $div, -1\n\t"
            "je,s    done\n"
    "normal: cdqq\n\t"
            "idivq   $div\n"
    "done:"        %}
  ins_encode(cdqq_enc(div));
  ins_pipe(ialu_reg_reg_alu0);
%}

instruct udivI_rReg(rax_RegI rax, rdx_RegI rdx, no_rax_rdx_RegI div, rFlagsReg cr)
%{
  match(Set rax (UDivI rax div));
  effect(KILL rdx, KILL cr);

  ins_cost(300);
  format %{ "udivl $rax,$rax,$div\t# UDivI\n" %}
  ins_encode %{
    __ udivI($rax$$Register, $div$$Register, $rdx$$Register);
  %}
  ins_pipe(ialu_reg_reg_alu0);
%}

instruct udivL_rReg(rax_RegL rax, rdx_RegL rdx, no_rax_rdx_RegL div, rFlagsReg cr)
%{
  match(Set rax (UDivL rax div));
  effect(KILL rdx, KILL cr);

  ins_cost(300);
  format %{ "udivq $rax,$rax,$div\t# UDivL\n" %}
  ins_encode %{
     __ udivL($rax$$Register, $div$$Register, $rdx$$Register);
  %}
  ins_pipe(ialu_reg_reg_alu0);
%}

// Integer DIVMOD with Register, both quotient and mod results
instruct divModI_rReg_divmod(rax_RegI rax, rdx_RegI rdx, no_rax_rdx_RegI div,
                             rFlagsReg cr)
%{
  match(DivModI rax div);
  effect(KILL cr);

  ins_cost(30*100+10*100); // XXX
  format %{ "cmpl    rax, 0x80000000\t# idiv\n\t"
            "jne,s   normal\n\t"
            "xorl    rdx, rdx\n\t"
            "cmpl    $div, -1\n\t"
            "je,s    done\n"
    "normal: cdql\n\t"
            "idivl   $div\n"
    "done:"        %}
  ins_encode(cdql_enc(div));
  ins_pipe(pipe_slow);
%}

// Long DIVMOD with Register, both quotient and mod results
instruct divModL_rReg_divmod(rax_RegL rax, rdx_RegL rdx, no_rax_rdx_RegL div,
                             rFlagsReg cr)
%{
  match(DivModL rax div);
  effect(KILL cr);

  ins_cost(30*100+10*100); // XXX
  format %{ "movq    rdx, 0x8000000000000000\t# ldiv\n\t"
            "cmpq    rax, rdx\n\t"
            "jne,s   normal\n\t"
            "xorl    rdx, rdx\n\t"
            "cmpq    $div, -1\n\t"
            "je,s    done\n"
    "normal: cdqq\n\t"
            "idivq   $div\n"
    "done:"        %}
  ins_encode(cdqq_enc(div));
  ins_pipe(pipe_slow);
%}

// Unsigned integer DIVMOD with Register, both quotient and mod results
instruct udivModI_rReg_divmod(rax_RegI rax, no_rax_rdx_RegI tmp, rdx_RegI rdx,
                              no_rax_rdx_RegI div, rFlagsReg cr)
%{
  match(UDivModI rax div);
  effect(TEMP tmp, KILL cr);

  ins_cost(300);
  format %{ "udivl $rax,$rax,$div\t# begin UDivModI\n\t"
            "umodl $rdx,$rax,$div\t! using $tmp as TEMP # end UDivModI\n"
          %}
  ins_encode %{
    __ udivmodI($rax$$Register, $div$$Register, $rdx$$Register, $tmp$$Register);
  %}
  ins_pipe(pipe_slow);
%}

// Unsigned long DIVMOD with Register, both quotient and mod results
instruct udivModL_rReg_divmod(rax_RegL rax, no_rax_rdx_RegL tmp, rdx_RegL rdx,
                              no_rax_rdx_RegL div, rFlagsReg cr)
%{
  match(UDivModL rax div);
  effect(TEMP tmp, KILL cr);

  ins_cost(300);
  format %{ "udivq $rax,$rax,$div\t# begin UDivModL\n\t"
            "umodq $rdx,$rax,$div\t! using $tmp as TEMP # end UDivModL\n"
          %}
  ins_encode %{
    __ udivmodL($rax$$Register, $div$$Register, $rdx$$Register, $tmp$$Register);
  %}
  ins_pipe(pipe_slow);
%}

instruct modI_rReg(rdx_RegI rdx, rax_RegI rax, no_rax_rdx_RegI div,
                   rFlagsReg cr)
%{
  match(Set rdx (ModI rax div));
  effect(KILL rax, KILL cr);

  ins_cost(300); // XXX
  format %{ "cmpl    rax, 0x80000000\t# irem\n\t"
            "jne,s   normal\n\t"
            "xorl    rdx, rdx\n\t"
            "cmpl    $div, -1\n\t"
            "je,s    done\n"
    "normal: cdql\n\t"
            "idivl   $div\n"
    "done:"        %}
  ins_encode(cdql_enc(div));
  ins_pipe(ialu_reg_reg_alu0);
%}

instruct modL_rReg(rdx_RegL rdx, rax_RegL rax, no_rax_rdx_RegL div,
                   rFlagsReg cr)
%{
  match(Set rdx (ModL rax div));
  effect(KILL rax, KILL cr);

  ins_cost(300); // XXX
  format %{ "movq    rdx, 0x8000000000000000\t# lrem\n\t"
            "cmpq    rax, rdx\n\t"
            "jne,s   normal\n\t"
            "xorl    rdx, rdx\n\t"
            "cmpq    $div, -1\n\t"
            "je,s    done\n"
    "normal: cdqq\n\t"
            "idivq   $div\n"
    "done:"        %}
  ins_encode(cdqq_enc(div));
  ins_pipe(ialu_reg_reg_alu0);
%}

instruct umodI_rReg(rdx_RegI rdx, rax_RegI rax, no_rax_rdx_RegI div, rFlagsReg cr)
%{
  match(Set rdx (UModI rax div));
  effect(KILL rax, KILL cr);

  ins_cost(300);
  format %{ "umodl $rdx,$rax,$div\t# UModI\n" %}
  ins_encode %{
    __ umodI($rax$$Register, $div$$Register, $rdx$$Register);
  %}
  ins_pipe(ialu_reg_reg_alu0);
%}

instruct umodL_rReg(rdx_RegL rdx, rax_RegL rax, no_rax_rdx_RegL div, rFlagsReg cr)
%{
  match(Set rdx (UModL rax div));
  effect(KILL rax, KILL cr);

  ins_cost(300);
  format %{ "umodq $rdx,$rax,$div\t# UModL\n" %}
  ins_encode %{
    __ umodL($rax$$Register, $div$$Register, $rdx$$Register);
  %}
  ins_pipe(ialu_reg_reg_alu0);
%}

// Integer Shift Instructions
// Shift Left by one, two, three
instruct salI_rReg_immI2(rRegI dst, immI2 shift, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (LShiftI dst shift));
  effect(KILL cr);

  format %{ "sall    $dst, $shift" %}
  ins_encode %{
    __ sall($dst$$Register, $shift$$constant);
  %}
  ins_pipe(ialu_reg);
%}

// Shift Left by one, two, three
instruct salI_rReg_immI2_ndd(rRegI dst, rRegI src, immI2 shift, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (LShiftI src shift));
  effect(KILL cr);

  format %{ "esall    $dst, $src, $shift\t# int(ndd)" %}
  ins_encode %{
    __ esall($dst$$Register, $src$$Register, $shift$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

// Shift Left by 8-bit immediate
instruct salI_rReg_imm(rRegI dst, immI8 shift, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (LShiftI dst shift));
  effect(KILL cr);

  format %{ "sall    $dst, $shift" %}
  ins_encode %{
    __ sall($dst$$Register, $shift$$constant);
  %}
  ins_pipe(ialu_reg);
%}

// Shift Left by 8-bit immediate
instruct salI_rReg_imm_ndd(rRegI dst, rRegI src, immI8 shift, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (LShiftI src shift));
  effect(KILL cr);

  format %{ "esall    $dst, $src, $shift\t# int (ndd)" %}
  ins_encode %{
    __ esall($dst$$Register, $src$$Register, $shift$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct salI_rReg_mem_imm_ndd(rRegI dst, memory src, immI8 shift, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (LShiftI (LoadI src) shift));
  effect(KILL cr);

  format %{ "esall    $dst, $src, $shift\t# int (ndd)" %}
  ins_encode %{
    __ esall($dst$$Register, $src$$Address, $shift$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

// Shift Left by 8-bit immediate
instruct salI_mem_imm(memory dst, immI8 shift, rFlagsReg cr)
%{
  match(Set dst (StoreI dst (LShiftI (LoadI dst) shift)));
  effect(KILL cr);

  format %{ "sall    $dst, $shift" %}
  ins_encode %{
    __ sall($dst$$Address, $shift$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

// Shift Left by variable
instruct salI_rReg_CL(rRegI dst, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_bmi2());
  match(Set dst (LShiftI dst shift));
  effect(KILL cr);

  format %{ "sall    $dst, $shift" %}
  ins_encode %{
    __ sall($dst$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Shift Left by variable
instruct salI_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_bmi2());
  match(Set dst (StoreI dst (LShiftI (LoadI dst) shift)));
  effect(KILL cr);

  format %{ "sall    $dst, $shift" %}
  ins_encode %{
    __ sall($dst$$Address);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct salI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)
%{
  predicate(VM_Version::supports_bmi2());
  match(Set dst (LShiftI src shift));

  format %{ "shlxl   $dst, $src, $shift" %}
  ins_encode %{
    __ shlxl($dst$$Register, $src$$Register, $shift$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct salI_mem_rReg(rRegI dst, memory src, rRegI shift)
%{
  predicate(VM_Version::supports_bmi2());
  match(Set dst (LShiftI (LoadI src) shift));
  ins_cost(175);
  format %{ "shlxl   $dst, $src, $shift" %}
  ins_encode %{
    __ shlxl($dst$$Register, $src$$Address, $shift$$Register);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Arithmetic Shift Right by 8-bit immediate
instruct sarI_rReg_imm(rRegI dst, immI8 shift, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (RShiftI dst shift));
  effect(KILL cr);

  format %{ "sarl    $dst, $shift" %}
  ins_encode %{
    __ sarl($dst$$Register, $shift$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

// Arithmetic Shift Right by 8-bit immediate
instruct sarI_rReg_imm_ndd(rRegI dst, rRegI src, immI8 shift, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (RShiftI src shift));
  effect(KILL cr);

  format %{ "esarl    $dst, $src, $shift\t# int (ndd)" %}
  ins_encode %{
    __ esarl($dst$$Register, $src$$Register, $shift$$constant, false);
  %}
  ins_pipe(ialu_mem_imm);
%}

instruct sarI_rReg_mem_imm_ndd(rRegI dst, memory src, immI8 shift, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (RShiftI (LoadI src) shift));
  effect(KILL cr);

  format %{ "esarl    $dst, $src, $shift\t# int (ndd)" %}
  ins_encode %{
    __ esarl($dst$$Register, $src$$Address, $shift$$constant, false);
  %}
  ins_pipe(ialu_mem_imm);
%}

// Arithmetic Shift Right by 8-bit immediate
instruct sarI_mem_imm(memory dst, immI8 shift, rFlagsReg cr)
%{
  match(Set dst (StoreI dst (RShiftI (LoadI dst) shift)));
  effect(KILL cr);

  format %{ "sarl    $dst, $shift" %}
  ins_encode %{
    __ sarl($dst$$Address, $shift$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

// Arithmetic Shift Right by variable
instruct sarI_rReg_CL(rRegI dst, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_bmi2());
  match(Set dst (RShiftI dst shift));
  effect(KILL cr);

  format %{ "sarl    $dst, $shift" %}
  ins_encode %{
    __ sarl($dst$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Arithmetic Shift Right by variable
instruct sarI_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_bmi2());
  match(Set dst (StoreI dst (RShiftI (LoadI dst) shift)));
  effect(KILL cr);

  format %{ "sarl    $dst, $shift" %}
  ins_encode %{
    __ sarl($dst$$Address);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct sarI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)
%{
  predicate(VM_Version::supports_bmi2());
  match(Set dst (RShiftI src shift));

  format %{ "sarxl   $dst, $src, $shift" %}
  ins_encode %{
    __ sarxl($dst$$Register, $src$$Register, $shift$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct sarI_mem_rReg(rRegI dst, memory src, rRegI shift)
%{
  predicate(VM_Version::supports_bmi2());
  match(Set dst (RShiftI (LoadI src) shift));
  ins_cost(175);
  format %{ "sarxl   $dst, $src, $shift" %}
  ins_encode %{
    __ sarxl($dst$$Register, $src$$Address, $shift$$Register);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Logical Shift Right by 8-bit immediate
instruct shrI_rReg_imm(rRegI dst, immI8 shift, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (URShiftI dst shift));
  effect(KILL cr);

  format %{ "shrl    $dst, $shift" %}
  ins_encode %{
    __ shrl($dst$$Register, $shift$$constant);
  %}
  ins_pipe(ialu_reg);
%}

// Logical Shift Right by 8-bit immediate
instruct shrI_rReg_imm_ndd(rRegI dst, rRegI src, immI8 shift, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (URShiftI src shift));
  effect(KILL cr);

  format %{ "eshrl    $dst, $src, $shift\t # int (ndd)" %}
  ins_encode %{
    __ eshrl($dst$$Register, $src$$Register, $shift$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct shrI_rReg_mem_imm_ndd(rRegI dst, memory src, immI8 shift, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (URShiftI (LoadI src) shift));
  effect(KILL cr);

  format %{ "eshrl    $dst, $src, $shift\t # int (ndd)" %}
  ins_encode %{
    __ eshrl($dst$$Register, $src$$Address, $shift$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

// Logical Shift Right by 8-bit immediate
instruct shrI_mem_imm(memory dst, immI8 shift, rFlagsReg cr)
%{
  match(Set dst (StoreI dst (URShiftI (LoadI dst) shift)));
  effect(KILL cr);

  format %{ "shrl    $dst, $shift" %}
  ins_encode %{
    __ shrl($dst$$Address, $shift$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

// Logical Shift Right by variable
instruct shrI_rReg_CL(rRegI dst, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_bmi2());
  match(Set dst (URShiftI dst shift));
  effect(KILL cr);

  format %{ "shrl    $dst, $shift" %}
  ins_encode %{
    __ shrl($dst$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Logical Shift Right by variable
instruct shrI_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_bmi2());
  match(Set dst (StoreI dst (URShiftI (LoadI dst) shift)));
  effect(KILL cr);

  format %{ "shrl    $dst, $shift" %}
  ins_encode %{
    __ shrl($dst$$Address);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct shrI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)
%{
  predicate(VM_Version::supports_bmi2());
  match(Set dst (URShiftI src shift));

  format %{ "shrxl   $dst, $src, $shift" %}
  ins_encode %{
    __ shrxl($dst$$Register, $src$$Register, $shift$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct shrI_mem_rReg(rRegI dst, memory src, rRegI shift)
%{
  predicate(VM_Version::supports_bmi2());
  match(Set dst (URShiftI (LoadI src) shift));
  ins_cost(175);
  format %{ "shrxl   $dst, $src, $shift" %}
  ins_encode %{
    __ shrxl($dst$$Register, $src$$Address, $shift$$Register);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Long Shift Instructions
// Shift Left by one, two, three
instruct salL_rReg_immI2(rRegL dst, immI2 shift, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (LShiftL dst shift));
  effect(KILL cr);

  format %{ "salq    $dst, $shift" %}
  ins_encode %{
    __ salq($dst$$Register, $shift$$constant);
  %}
  ins_pipe(ialu_reg);
%}

// Shift Left by one, two, three
instruct salL_rReg_immI2_ndd(rRegL dst, rRegL src, immI2 shift, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (LShiftL src shift));
  effect(KILL cr);

  format %{ "esalq    $dst, $src, $shift\t# long (ndd)" %}
  ins_encode %{
    __ esalq($dst$$Register, $src$$Register, $shift$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

// Shift Left by 8-bit immediate
instruct salL_rReg_imm(rRegL dst, immI8 shift, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (LShiftL dst shift));
  effect(KILL cr);

  format %{ "salq    $dst, $shift" %}
  ins_encode %{
    __ salq($dst$$Register, $shift$$constant);
  %}
  ins_pipe(ialu_reg);
%}

// Shift Left by 8-bit immediate
instruct salL_rReg_imm_ndd(rRegL dst, rRegL src, immI8 shift, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (LShiftL src shift));
  effect(KILL cr);

  format %{ "esalq    $dst, $src, $shift\t# long (ndd)" %}
  ins_encode %{
    __ esalq($dst$$Register, $src$$Register, $shift$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct salL_rReg_mem_imm_ndd(rRegL dst, memory src, immI8 shift, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (LShiftL (LoadL src) shift));
  effect(KILL cr);

  format %{ "esalq    $dst, $src, $shift\t# long (ndd)" %}
  ins_encode %{
    __ esalq($dst$$Register, $src$$Address, $shift$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

// Shift Left by 8-bit immediate
instruct salL_mem_imm(memory dst, immI8 shift, rFlagsReg cr)
%{
  match(Set dst (StoreL dst (LShiftL (LoadL dst) shift)));
  effect(KILL cr);

  format %{ "salq    $dst, $shift" %}
  ins_encode %{
    __ salq($dst$$Address, $shift$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

// Shift Left by variable
instruct salL_rReg_CL(rRegL dst, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_bmi2());
  match(Set dst (LShiftL dst shift));
  effect(KILL cr);

  format %{ "salq    $dst, $shift" %}
  ins_encode %{
    __ salq($dst$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Shift Left by variable
instruct salL_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_bmi2());
  match(Set dst (StoreL dst (LShiftL (LoadL dst) shift)));
  effect(KILL cr);

  format %{ "salq    $dst, $shift" %}
  ins_encode %{
    __ salq($dst$$Address);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct salL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)
%{
  predicate(VM_Version::supports_bmi2());
  match(Set dst (LShiftL src shift));

  format %{ "shlxq   $dst, $src, $shift" %}
  ins_encode %{
    __ shlxq($dst$$Register, $src$$Register, $shift$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct salL_mem_rReg(rRegL dst, memory src, rRegI shift)
%{
  predicate(VM_Version::supports_bmi2());
  match(Set dst (LShiftL (LoadL src) shift));
  ins_cost(175);
  format %{ "shlxq   $dst, $src, $shift" %}
  ins_encode %{
    __ shlxq($dst$$Register, $src$$Address, $shift$$Register);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Arithmetic Shift Right by 8-bit immediate
instruct sarL_rReg_imm(rRegL dst, immI shift, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (RShiftL dst shift));
  effect(KILL cr);

  format %{ "sarq    $dst, $shift" %}
  ins_encode %{
    __ sarq($dst$$Register, (unsigned char)($shift$$constant & 0x3F));
  %}
  ins_pipe(ialu_mem_imm);
%}

// Arithmetic Shift Right by 8-bit immediate
instruct sarL_rReg_imm_ndd(rRegL dst, rRegL src, immI shift, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (RShiftL src shift));
  effect(KILL cr);

  format %{ "esarq    $dst, $src, $shift\t# long (ndd)" %}
  ins_encode %{
    __ esarq($dst$$Register, $src$$Register, (unsigned char)($shift$$constant & 0x3F), false);
  %}
  ins_pipe(ialu_mem_imm);
%}

instruct sarL_rReg_mem_imm_ndd(rRegL dst, memory src, immI shift, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (RShiftL (LoadL src) shift));
  effect(KILL cr);

  format %{ "esarq    $dst, $src, $shift\t# long (ndd)" %}
  ins_encode %{
    __ esarq($dst$$Register, $src$$Address, (unsigned char)($shift$$constant & 0x3F), false);
  %}
  ins_pipe(ialu_mem_imm);
%}

// Arithmetic Shift Right by 8-bit immediate
instruct sarL_mem_imm(memory dst, immI shift, rFlagsReg cr)
%{
  match(Set dst (StoreL dst (RShiftL (LoadL dst) shift)));
  effect(KILL cr);

  format %{ "sarq    $dst, $shift" %}
  ins_encode %{
    __ sarq($dst$$Address, (unsigned char)($shift$$constant & 0x3F));
  %}
  ins_pipe(ialu_mem_imm);
%}

// Arithmetic Shift Right by variable
instruct sarL_rReg_CL(rRegL dst, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_bmi2());
  match(Set dst (RShiftL dst shift));
  effect(KILL cr);

  format %{ "sarq    $dst, $shift" %}
  ins_encode %{
    __ sarq($dst$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Arithmetic Shift Right by variable
instruct sarL_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_bmi2());
  match(Set dst (StoreL dst (RShiftL (LoadL dst) shift)));
  effect(KILL cr);

  format %{ "sarq    $dst, $shift" %}
  ins_encode %{
    __ sarq($dst$$Address);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct sarL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)
%{
  predicate(VM_Version::supports_bmi2());
  match(Set dst (RShiftL src shift));

  format %{ "sarxq   $dst, $src, $shift" %}
  ins_encode %{
    __ sarxq($dst$$Register, $src$$Register, $shift$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct sarL_mem_rReg(rRegL dst, memory src, rRegI shift)
%{
  predicate(VM_Version::supports_bmi2());
  match(Set dst (RShiftL (LoadL src) shift));
  ins_cost(175);
  format %{ "sarxq   $dst, $src, $shift" %}
  ins_encode %{
    __ sarxq($dst$$Register, $src$$Address, $shift$$Register);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Logical Shift Right by 8-bit immediate
instruct shrL_rReg_imm(rRegL dst, immI8 shift, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (URShiftL dst shift));
  effect(KILL cr);

  format %{ "shrq    $dst, $shift" %}
  ins_encode %{
    __ shrq($dst$$Register, $shift$$constant);
  %}
  ins_pipe(ialu_reg);
%}

// Logical Shift Right by 8-bit immediate
instruct shrL_rReg_imm_ndd(rRegL dst, rRegL src, immI8 shift, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (URShiftL src shift));
  effect(KILL cr);

  format %{ "eshrq    $dst, $src, $shift\t# long (ndd)" %}
  ins_encode %{
    __ eshrq($dst$$Register, $src$$Register, $shift$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct shrL_rReg_mem_imm_ndd(rRegL dst, memory src, immI8 shift, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (URShiftL (LoadL src) shift));
  effect(KILL cr);

  format %{ "eshrq    $dst, $src, $shift\t# long (ndd)" %}
  ins_encode %{
    __ eshrq($dst$$Register, $src$$Address, $shift$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

// Logical Shift Right by 8-bit immediate
instruct shrL_mem_imm(memory dst, immI8 shift, rFlagsReg cr)
%{
  match(Set dst (StoreL dst (URShiftL (LoadL dst) shift)));
  effect(KILL cr);

  format %{ "shrq    $dst, $shift" %}
  ins_encode %{
    __ shrq($dst$$Address, $shift$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

// Logical Shift Right by variable
instruct shrL_rReg_CL(rRegL dst, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_bmi2());
  match(Set dst (URShiftL dst shift));
  effect(KILL cr);

  format %{ "shrq    $dst, $shift" %}
  ins_encode %{
    __ shrq($dst$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Logical Shift Right by variable
instruct shrL_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_bmi2());
  match(Set dst (StoreL dst (URShiftL (LoadL dst) shift)));
  effect(KILL cr);

  format %{ "shrq    $dst, $shift" %}
  ins_encode %{
    __ shrq($dst$$Address);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct shrL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)
%{
  predicate(VM_Version::supports_bmi2());
  match(Set dst (URShiftL src shift));

  format %{ "shrxq   $dst, $src, $shift" %}
  ins_encode %{
    __ shrxq($dst$$Register, $src$$Register, $shift$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct shrL_mem_rReg(rRegL dst, memory src, rRegI shift)
%{
  predicate(VM_Version::supports_bmi2());
  match(Set dst (URShiftL (LoadL src) shift));
  ins_cost(175);
  format %{ "shrxq   $dst, $src, $shift" %}
  ins_encode %{
    __ shrxq($dst$$Register, $src$$Address, $shift$$Register);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Logical Shift Right by 24, followed by Arithmetic Shift Left by 24.
// This idiom is used by the compiler for the i2b bytecode.
instruct i2b(rRegI dst, rRegI src, immI_24 twentyfour)
%{
  match(Set dst (RShiftI (LShiftI src twentyfour) twentyfour));

  format %{ "movsbl  $dst, $src\t# i2b" %}
  ins_encode %{
    __ movsbl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Logical Shift Right by 16, followed by Arithmetic Shift Left by 16.
// This idiom is used by the compiler the i2s bytecode.
instruct i2s(rRegI dst, rRegI src, immI_16 sixteen)
%{
  match(Set dst (RShiftI (LShiftI src sixteen) sixteen));

  format %{ "movswl  $dst, $src\t# i2s" %}
  ins_encode %{
    __ movswl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// ROL/ROR instructions

// Rotate left by constant.
instruct rolI_immI8_legacy(rRegI dst, immI8 shift, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);
  match(Set dst (RotateLeft dst shift));
  effect(KILL cr);
  format %{ "roll    $dst, $shift" %}
  ins_encode %{
    __ roll($dst$$Register, $shift$$constant);
  %}
  ins_pipe(ialu_reg);
%}

instruct rolI_immI8(rRegI dst, rRegI src, immI8 shift)
%{
  predicate(!UseAPX && VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);
  match(Set dst (RotateLeft src shift));
  format %{ "rolxl   $dst, $src, $shift" %}
  ins_encode %{
    int shift = 32 - ($shift$$constant & 31);
    __ rorxl($dst$$Register, $src$$Register, shift);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct rolI_mem_immI8(rRegI dst, memory src, immI8 shift)
%{
  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);
  match(Set dst (RotateLeft (LoadI src) shift));
  ins_cost(175);
  format %{ "rolxl   $dst, $src, $shift" %}
  ins_encode %{
    int shift = 32 - ($shift$$constant & 31);
    __ rorxl($dst$$Register, $src$$Address, shift);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Rotate Left by variable
instruct rolI_rReg_Var(rRegI dst, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(!UseAPX && n->bottom_type()->basic_type() == T_INT);
  match(Set dst (RotateLeft dst shift));
  effect(KILL cr);
  format %{ "roll    $dst, $shift" %}
  ins_encode %{
    __ roll($dst$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Rotate Left by variable
instruct rolI_rReg_Var_ndd(rRegI dst, rRegI src, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(UseAPX && n->bottom_type()->basic_type() == T_INT);
  match(Set dst (RotateLeft src shift));
  effect(KILL cr);

  format %{ "eroll    $dst, $src, $shift\t# rotate left (int ndd)" %}
  ins_encode %{
    __ eroll($dst$$Register, $src$$Register, false);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Rotate Right by constant.
instruct rorI_immI8_legacy(rRegI dst, immI8 shift, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);
  match(Set dst (RotateRight dst shift));
  effect(KILL cr);
  format %{ "rorl    $dst, $shift" %}
  ins_encode %{
    __ rorl($dst$$Register, $shift$$constant);
  %}
  ins_pipe(ialu_reg);
%}

// Rotate Right by constant.
instruct rorI_immI8(rRegI dst, rRegI src, immI8 shift)
%{
  predicate(!UseAPX && VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);
  match(Set dst (RotateRight src shift));
  format %{ "rorxl   $dst, $src, $shift" %}
  ins_encode %{
    __ rorxl($dst$$Register, $src$$Register, $shift$$constant);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct rorI_mem_immI8(rRegI dst, memory src, immI8 shift)
%{
  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);
  match(Set dst (RotateRight (LoadI src) shift));
  ins_cost(175);
  format %{ "rorxl   $dst, $src, $shift" %}
  ins_encode %{
    __ rorxl($dst$$Register, $src$$Address, $shift$$constant);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Rotate Right by variable
instruct rorI_rReg_Var(rRegI dst, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(!UseAPX && n->bottom_type()->basic_type() == T_INT);
  match(Set dst (RotateRight dst shift));
  effect(KILL cr);
  format %{ "rorl    $dst, $shift" %}
  ins_encode %{
    __ rorl($dst$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Rotate Right by variable
instruct rorI_rReg_Var_ndd(rRegI dst, rRegI src, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(UseAPX && n->bottom_type()->basic_type() == T_INT);
  match(Set dst (RotateRight src shift));
  effect(KILL cr);

  format %{ "erorl    $dst, $src, $shift\t# rotate right(int ndd)" %}
  ins_encode %{
    __ erorl($dst$$Register, $src$$Register, false);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Rotate Left by constant.
instruct rolL_immI8_legacy(rRegL dst, immI8 shift, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);
  match(Set dst (RotateLeft dst shift));
  effect(KILL cr);
  format %{ "rolq    $dst, $shift" %}
  ins_encode %{
    __ rolq($dst$$Register, $shift$$constant);
  %}
  ins_pipe(ialu_reg);
%}

instruct rolL_immI8(rRegL dst, rRegL src, immI8 shift)
%{
  predicate(!UseAPX && VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);
  match(Set dst (RotateLeft src shift));
  format %{ "rolxq   $dst, $src, $shift" %}
  ins_encode %{
    int shift = 64 - ($shift$$constant & 63);
    __ rorxq($dst$$Register, $src$$Register, shift);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct rolL_mem_immI8(rRegL dst, memory src, immI8 shift)
%{
  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);
  match(Set dst (RotateLeft (LoadL src) shift));
  ins_cost(175);
  format %{ "rolxq   $dst, $src, $shift" %}
  ins_encode %{
    int shift = 64 - ($shift$$constant & 63);
    __ rorxq($dst$$Register, $src$$Address, shift);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Rotate Left by variable
instruct rolL_rReg_Var(rRegL dst, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(!UseAPX && n->bottom_type()->basic_type() == T_LONG);
  match(Set dst (RotateLeft dst shift));
  effect(KILL cr);
  format %{ "rolq    $dst, $shift" %}
  ins_encode %{
    __ rolq($dst$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Rotate Left by variable
instruct rolL_rReg_Var_ndd(rRegL dst, rRegL src, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(UseAPX && n->bottom_type()->basic_type() == T_LONG);
  match(Set dst (RotateLeft src shift));
  effect(KILL cr);

  format %{ "erolq    $dst, $src, $shift\t# rotate left(long ndd)" %}
  ins_encode %{
    __ erolq($dst$$Register, $src$$Register, false);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Rotate Right by constant.
instruct rorL_immI8_legacy(rRegL dst, immI8 shift, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);
  match(Set dst (RotateRight dst shift));
  effect(KILL cr);
  format %{ "rorq    $dst, $shift" %}
  ins_encode %{
    __ rorq($dst$$Register, $shift$$constant);
  %}
  ins_pipe(ialu_reg);
%}

// Rotate Right by constant
instruct rorL_immI8(rRegL dst, rRegL src, immI8 shift)
%{
  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);
  match(Set dst (RotateRight src shift));
  format %{ "rorxq   $dst, $src, $shift" %}
  ins_encode %{
    __ rorxq($dst$$Register, $src$$Register, $shift$$constant);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct rorL_mem_immI8(rRegL dst, memory src, immI8 shift)
%{
  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);
  match(Set dst (RotateRight (LoadL src) shift));
  ins_cost(175);
  format %{ "rorxq   $dst, $src, $shift" %}
  ins_encode %{
    __ rorxq($dst$$Register, $src$$Address, $shift$$constant);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Rotate Right by variable
instruct rorL_rReg_Var(rRegL dst, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(!UseAPX && n->bottom_type()->basic_type() == T_LONG);
  match(Set dst (RotateRight dst shift));
  effect(KILL cr);
  format %{ "rorq    $dst, $shift" %}
  ins_encode %{
    __ rorq($dst$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Rotate Right by variable
instruct rorL_rReg_Var_ndd(rRegL dst, rRegL src, rcx_RegI shift, rFlagsReg cr)
%{
  predicate(UseAPX && n->bottom_type()->basic_type() == T_LONG);
  match(Set dst (RotateRight src shift));
  effect(KILL cr);

  format %{ "erorq    $dst, $src, $shift\t# rotate right(long ndd)" %}
  ins_encode %{
    __ erorq($dst$$Register, $src$$Register, false);
  %}
  ins_pipe(ialu_reg_reg);
%}

//----------------------------- CompressBits/ExpandBits ------------------------

instruct compressBitsL_reg(rRegL dst, rRegL src, rRegL mask) %{
  predicate(n->bottom_type()->isa_long());
  match(Set dst (CompressBits src mask));
  format %{ "pextq  $dst, $src, $mask\t! parallel bit extract" %}
  ins_encode %{
    __ pextq($dst$$Register, $src$$Register, $mask$$Register);
  %}
  ins_pipe( pipe_slow );
%}

instruct expandBitsL_reg(rRegL dst, rRegL src, rRegL mask) %{
  predicate(n->bottom_type()->isa_long());
  match(Set dst (ExpandBits src mask));
  format %{ "pdepq  $dst, $src, $mask\t! parallel bit deposit" %}
  ins_encode %{
    __ pdepq($dst$$Register, $src$$Register, $mask$$Register);
  %}
  ins_pipe( pipe_slow );
%}

instruct compressBitsL_mem(rRegL dst, rRegL src, memory mask) %{
  predicate(n->bottom_type()->isa_long());
  match(Set dst (CompressBits src (LoadL mask)));
  format %{ "pextq  $dst, $src, $mask\t! parallel bit extract" %}
  ins_encode %{
    __ pextq($dst$$Register, $src$$Register, $mask$$Address);
  %}
  ins_pipe( pipe_slow );
%}

instruct expandBitsL_mem(rRegL dst, rRegL src, memory mask) %{
  predicate(n->bottom_type()->isa_long());
  match(Set dst (ExpandBits src (LoadL mask)));
  format %{ "pdepq  $dst, $src, $mask\t! parallel bit deposit" %}
  ins_encode %{
    __ pdepq($dst$$Register, $src$$Register, $mask$$Address);
  %}
  ins_pipe( pipe_slow );
%}


// Logical Instructions

// Integer Logical Instructions

// And Instructions
// And Register with Register
instruct andI_rReg(rRegI dst, rRegI src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (AndI dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "andl    $dst, $src\t# int" %}
  ins_encode %{
    __ andl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// And Register with Register using New Data Destination (NDD)
instruct andI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (AndI src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "eandl     $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ eandl($dst$$Register, $src1$$Register, $src2$$Register, false);

  %}
  ins_pipe(ialu_reg_reg);
%}

// And Register with Immediate 255
instruct andI_rReg_imm255(rRegI dst, rRegI src, immI_255 mask)
%{
  match(Set dst (AndI src mask));

  format %{ "movzbl  $dst, $src\t# int & 0xFF" %}
  ins_encode %{
    __ movzbl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg);
%}

// And Register with Immediate 255 and promote to long
instruct andI2L_rReg_imm255(rRegL dst, rRegI src, immI_255 mask)
%{
  match(Set dst (ConvI2L (AndI src mask)));

  format %{ "movzbl  $dst, $src\t# int & 0xFF -> long" %}
  ins_encode %{
    __ movzbl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg);
%}

// And Register with Immediate 65535
instruct andI_rReg_imm65535(rRegI dst, rRegI src, immI_65535 mask)
%{
  match(Set dst (AndI src mask));

  format %{ "movzwl  $dst, $src\t# int & 0xFFFF" %}
  ins_encode %{
    __ movzwl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg);
%}

// And Register with Immediate 65535 and promote to long
instruct andI2L_rReg_imm65535(rRegL dst, rRegI src, immI_65535 mask)
%{
  match(Set dst (ConvI2L (AndI src mask)));

  format %{ "movzwl  $dst, $src\t# int & 0xFFFF -> long" %}
  ins_encode %{
    __ movzwl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg);
%}

// Can skip int2long conversions after AND with small bitmask
instruct convI2LAndI_reg_immIbitmask(rRegL dst, rRegI src,  immI_Pow2M1 mask, rRegI tmp, rFlagsReg cr)
%{
  predicate(VM_Version::supports_bmi2());
  ins_cost(125);
  effect(TEMP tmp, KILL cr);
  match(Set dst (ConvI2L (AndI src mask)));
  format %{ "bzhiq $dst, $src, $mask \t# using $tmp as TEMP, int &  immI_Pow2M1 -> long" %}
  ins_encode %{
    __ movl($tmp$$Register, exact_log2($mask$$constant + 1));
    __ bzhiq($dst$$Register, $src$$Register, $tmp$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// And Register with Immediate
instruct andI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (AndI dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "andl    $dst, $src\t# int" %}
  ins_encode %{
    __ andl($dst$$Register, $src$$constant);
  %}
  ins_pipe(ialu_reg);
%}

instruct andI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (AndI src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "eandl    $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ eandl($dst$$Register, $src1$$Register, $src2$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct andI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (AndI (LoadI src1) src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "eandl    $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ eandl($dst$$Register, $src1$$Address, $src2$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

// And Register with Memory
instruct andI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (AndI dst (LoadI src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "andl    $dst, $src\t# int" %}
  ins_encode %{
    __ andl($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct andI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (AndI src1 (LoadI src2)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "eandl    $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ eandl($dst$$Register, $src1$$Register, $src2$$Address, false);
  %}
  ins_pipe(ialu_reg_mem);
%}

// And Memory with Register
instruct andB_mem_rReg(memory dst, rRegI src, rFlagsReg cr)
%{
  match(Set dst (StoreB dst (AndI (LoadB dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "andb    $dst, $src\t# byte" %}
  ins_encode %{
    __ andb($dst$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct andI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)
%{
  match(Set dst (StoreI dst (AndI (LoadI dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "andl    $dst, $src\t# int" %}
  ins_encode %{
    __ andl($dst$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

// And Memory with Immediate
instruct andI_mem_imm(memory dst, immI src, rFlagsReg cr)
%{
  match(Set dst (StoreI dst (AndI (LoadI dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(125);
  format %{ "andl    $dst, $src\t# int" %}
  ins_encode %{
    __ andl($dst$$Address, $src$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

// BMI1 instructions
instruct andnI_rReg_rReg_mem(rRegI dst, rRegI src1, memory src2, immI_M1 minus_1, rFlagsReg cr) %{
  match(Set dst (AndI (XorI src1 minus_1) (LoadI src2)));
  predicate(UseBMI1Instructions);
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(125);
  format %{ "andnl  $dst, $src1, $src2" %}

  ins_encode %{
    __ andnl($dst$$Register, $src1$$Register, $src2$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct andnI_rReg_rReg_rReg(rRegI dst, rRegI src1, rRegI src2, immI_M1 minus_1, rFlagsReg cr) %{
  match(Set dst (AndI (XorI src1 minus_1) src2));
  predicate(UseBMI1Instructions);
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "andnl  $dst, $src1, $src2" %}

  ins_encode %{
    __ andnl($dst$$Register, $src1$$Register, $src2$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct blsiI_rReg_rReg(rRegI dst, rRegI src, immI_0 imm_zero, rFlagsReg cr) %{
  match(Set dst (AndI (SubI imm_zero src) src));
  predicate(UseBMI1Instructions);
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);

  format %{ "blsil  $dst, $src" %}

  ins_encode %{
    __ blsil($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct blsiI_rReg_mem(rRegI dst, memory src, immI_0 imm_zero, rFlagsReg cr) %{
  match(Set dst (AndI (SubI imm_zero (LoadI src) ) (LoadI src) ));
  predicate(UseBMI1Instructions);
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);

  ins_cost(125);
  format %{ "blsil  $dst, $src" %}

  ins_encode %{
    __ blsil($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct blsmskI_rReg_mem(rRegI dst, memory src, immI_M1 minus_1, rFlagsReg cr)
%{
  match(Set dst (XorI (AddI (LoadI src) minus_1) (LoadI src) ) );
  predicate(UseBMI1Instructions);
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);

  ins_cost(125);
  format %{ "blsmskl $dst, $src" %}

  ins_encode %{
    __ blsmskl($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct blsmskI_rReg_rReg(rRegI dst, rRegI src, immI_M1 minus_1, rFlagsReg cr)
%{
  match(Set dst (XorI (AddI src minus_1) src));
  predicate(UseBMI1Instructions);
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);

  format %{ "blsmskl $dst, $src" %}

  ins_encode %{
    __ blsmskl($dst$$Register, $src$$Register);
  %}

  ins_pipe(ialu_reg);
%}

instruct blsrI_rReg_rReg(rRegI dst, rRegI src, immI_M1 minus_1, rFlagsReg cr)
%{
  match(Set dst (AndI (AddI src minus_1) src) );
  predicate(UseBMI1Instructions);
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);

  format %{ "blsrl  $dst, $src" %}

  ins_encode %{
    __ blsrl($dst$$Register, $src$$Register);
  %}

  ins_pipe(ialu_reg_mem);
%}

instruct blsrI_rReg_mem(rRegI dst, memory src, immI_M1 minus_1, rFlagsReg cr)
%{
  match(Set dst (AndI (AddI (LoadI src) minus_1) (LoadI src) ) );
  predicate(UseBMI1Instructions);
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);

  ins_cost(125);
  format %{ "blsrl  $dst, $src" %}

  ins_encode %{
    __ blsrl($dst$$Register, $src$$Address);
  %}

  ins_pipe(ialu_reg);
%}

// Or Instructions
// Or Register with Register
instruct orI_rReg(rRegI dst, rRegI src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (OrI dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "orl     $dst, $src\t# int" %}
  ins_encode %{
    __ orl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Or Register with Register using New Data Destination (NDD)
instruct orI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (OrI src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "eorl     $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ eorl($dst$$Register, $src1$$Register, $src2$$Register, false);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Or Register with Immediate
instruct orI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (OrI dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "orl     $dst, $src\t# int" %}
  ins_encode %{
    __ orl($dst$$Register, $src$$constant);
  %}
  ins_pipe(ialu_reg);
%}

instruct orI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (OrI src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "eorl     $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ eorl($dst$$Register, $src1$$Register, $src2$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct orI_rReg_imm_rReg_ndd(rRegI dst, immI src1, rRegI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (OrI src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "eorl     $dst, $src2, $src1\t# int ndd" %}
  ins_encode %{
    __ eorl($dst$$Register, $src2$$Register, $src1$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct orI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (OrI (LoadI src1) src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "eorl     $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ eorl($dst$$Register, $src1$$Address, $src2$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

// Or Register with Memory
instruct orI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (OrI dst (LoadI src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "orl     $dst, $src\t# int" %}
  ins_encode %{
    __ orl($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct orI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (OrI src1 (LoadI src2)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "eorl     $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ eorl($dst$$Register, $src1$$Register, $src2$$Address, false);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Or Memory with Register
instruct orB_mem_rReg(memory dst, rRegI src, rFlagsReg cr)
%{
  match(Set dst (StoreB dst (OrI (LoadB dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "orb    $dst, $src\t# byte" %}
  ins_encode %{
    __ orb($dst$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct orI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)
%{
  match(Set dst (StoreI dst (OrI (LoadI dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "orl     $dst, $src\t# int" %}
  ins_encode %{
    __ orl($dst$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

// Or Memory with Immediate
instruct orI_mem_imm(memory dst, immI src, rFlagsReg cr)
%{
  match(Set dst (StoreI dst (OrI (LoadI dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(125);
  format %{ "orl     $dst, $src\t# int" %}
  ins_encode %{
    __ orl($dst$$Address, $src$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

// Xor Instructions
// Xor Register with Register
instruct xorI_rReg(rRegI dst, rRegI src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (XorI dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "xorl    $dst, $src\t# int" %}
  ins_encode %{
    __ xorl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Xor Register with Register using New Data Destination (NDD)
instruct xorI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (XorI src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "exorl    $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ exorl($dst$$Register, $src1$$Register, $src2$$Register, false);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Xor Register with Immediate -1
instruct xorI_rReg_im1(rRegI dst, immI_M1 imm)
%{
  predicate(!UseAPX);
  match(Set dst (XorI dst imm));

  format %{ "notl    $dst" %}
  ins_encode %{
     __ notl($dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct xorI_rReg_im1_ndd(rRegI dst, rRegI src, immI_M1 imm)
%{
  match(Set dst (XorI src imm));
  predicate(UseAPX);

  format %{ "enotl    $dst, $src" %}
  ins_encode %{
     __ enotl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg);
%}

// Xor Register with Immediate
instruct xorI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)
%{
  // Strict predicate check to make selection of xorI_rReg_im1 cost agnostic if immI src is -1.
  predicate(!UseAPX && n->in(2)->bottom_type()->is_int()->get_con() != -1);
  match(Set dst (XorI dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "xorl    $dst, $src\t# int" %}
  ins_encode %{
    __ xorl($dst$$Register, $src$$constant);
  %}
  ins_pipe(ialu_reg);
%}

instruct xorI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)
%{
  // Strict predicate check to make selection of xorI_rReg_im1_ndd cost agnostic if immI src2 is -1.
  predicate(UseAPX && n->in(2)->bottom_type()->is_int()->get_con() != -1);
  match(Set dst (XorI src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "exorl    $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ exorl($dst$$Register, $src1$$Register, $src2$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

// Xor Memory with Immediate
instruct xorI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (XorI (LoadI src1) src2));
  effect(KILL cr);
  ins_cost(150);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "exorl    $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ exorl($dst$$Register, $src1$$Address, $src2$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

// Xor Register with Memory
instruct xorI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (XorI dst (LoadI src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "xorl    $dst, $src\t# int" %}
  ins_encode %{
    __ xorl($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct xorI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (XorI src1 (LoadI src2)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "exorl    $dst, $src1, $src2\t# int ndd" %}
  ins_encode %{
    __ exorl($dst$$Register, $src1$$Register, $src2$$Address, false);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Xor Memory with Register
instruct xorB_mem_rReg(memory dst, rRegI src, rFlagsReg cr)
%{
  match(Set dst (StoreB dst (XorI (LoadB dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "xorb    $dst, $src\t# byte" %}
  ins_encode %{
    __ xorb($dst$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct xorI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)
%{
  match(Set dst (StoreI dst (XorI (LoadI dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "xorl    $dst, $src\t# int" %}
  ins_encode %{
    __ xorl($dst$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

// Xor Memory with Immediate
instruct xorI_mem_imm(memory dst, immI src, rFlagsReg cr)
%{
  match(Set dst (StoreI dst (XorI (LoadI dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(125);
  format %{ "xorl    $dst, $src\t# int" %}
  ins_encode %{
    __ xorl($dst$$Address, $src$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}


// Long Logical Instructions

// And Instructions
// And Register with Register
instruct andL_rReg(rRegL dst, rRegL src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (AndL dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "andq    $dst, $src\t# long" %}
  ins_encode %{
    __ andq($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// And Register with Register using New Data Destination (NDD)
instruct andL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (AndL src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "eandq     $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ eandq($dst$$Register, $src1$$Register, $src2$$Register, false);

  %}
  ins_pipe(ialu_reg_reg);
%}

// And Register with Immediate 255
instruct andL_rReg_imm255(rRegL dst, rRegL src, immL_255 mask)
%{
  match(Set dst (AndL src mask));

  format %{ "movzbl  $dst, $src\t# long & 0xFF" %}
  ins_encode %{
    // movzbl zeroes out the upper 32-bit and does not need REX.W
    __ movzbl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg);
%}

// And Register with Immediate 65535
instruct andL_rReg_imm65535(rRegL dst, rRegL src, immL_65535 mask)
%{
  match(Set dst (AndL src mask));

  format %{ "movzwl  $dst, $src\t# long & 0xFFFF" %}
  ins_encode %{
    // movzwl zeroes out the upper 32-bit and does not need REX.W
    __ movzwl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg);
%}

// And Register with Immediate
instruct andL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (AndL dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "andq    $dst, $src\t# long" %}
  ins_encode %{
    __ andq($dst$$Register, $src$$constant);
  %}
  ins_pipe(ialu_reg);
%}

instruct andL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (AndL src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "eandq    $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ eandq($dst$$Register, $src1$$Register, $src2$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct andL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (AndL (LoadL src1) src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "eandq    $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ eandq($dst$$Register, $src1$$Address, $src2$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

// And Register with Memory
instruct andL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (AndL dst (LoadL src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "andq    $dst, $src\t# long" %}
  ins_encode %{
    __ andq($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct andL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (AndL src1 (LoadL src2)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "eandq    $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ eandq($dst$$Register, $src1$$Register, $src2$$Address, false);
  %}
  ins_pipe(ialu_reg_mem);
%}

// And Memory with Register
instruct andL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)
%{
  match(Set dst (StoreL dst (AndL (LoadL dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "andq    $dst, $src\t# long" %}
  ins_encode %{
    __ andq($dst$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

// And Memory with Immediate
instruct andL_mem_imm(memory dst, immL32 src, rFlagsReg cr)
%{
  match(Set dst (StoreL dst (AndL (LoadL dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(125);
  format %{ "andq    $dst, $src\t# long" %}
  ins_encode %{
    __ andq($dst$$Address, $src$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

instruct btrL_mem_imm(memory dst, immL_NotPow2 con, rFlagsReg cr)
%{
  // con should be a pure 64-bit immediate given that not(con) is a power of 2
  // because AND/OR works well enough for 8/32-bit values.
  predicate(log2i_graceful(~n->in(3)->in(2)->get_long()) > 30);

  match(Set dst (StoreL dst (AndL (LoadL dst) con)));
  effect(KILL cr);

  ins_cost(125);
  format %{ "btrq    $dst, log2(not($con))\t# long" %}
  ins_encode %{
    __ btrq($dst$$Address, log2i_exact((julong)~$con$$constant));
  %}
  ins_pipe(ialu_mem_imm);
%}

// BMI1 instructions
instruct andnL_rReg_rReg_mem(rRegL dst, rRegL src1, memory src2, immL_M1 minus_1, rFlagsReg cr) %{
  match(Set dst (AndL (XorL src1 minus_1) (LoadL src2)));
  predicate(UseBMI1Instructions);
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(125);
  format %{ "andnq  $dst, $src1, $src2" %}

  ins_encode %{
    __ andnq($dst$$Register, $src1$$Register, $src2$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct andnL_rReg_rReg_rReg(rRegL dst, rRegL src1, rRegL src2, immL_M1 minus_1, rFlagsReg cr) %{
  match(Set dst (AndL (XorL src1 minus_1) src2));
  predicate(UseBMI1Instructions);
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "andnq  $dst, $src1, $src2" %}

  ins_encode %{
  __ andnq($dst$$Register, $src1$$Register, $src2$$Register);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct blsiL_rReg_rReg(rRegL dst, rRegL src, immL0 imm_zero, rFlagsReg cr) %{
  match(Set dst (AndL (SubL imm_zero src) src));
  predicate(UseBMI1Instructions);
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);

  format %{ "blsiq  $dst, $src" %}

  ins_encode %{
    __ blsiq($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct blsiL_rReg_mem(rRegL dst, memory src, immL0 imm_zero, rFlagsReg cr) %{
  match(Set dst (AndL (SubL imm_zero (LoadL src) ) (LoadL src) ));
  predicate(UseBMI1Instructions);
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);

  ins_cost(125);
  format %{ "blsiq  $dst, $src" %}

  ins_encode %{
    __ blsiq($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct blsmskL_rReg_mem(rRegL dst, memory src, immL_M1 minus_1, rFlagsReg cr)
%{
  match(Set dst (XorL (AddL (LoadL src) minus_1) (LoadL src) ) );
  predicate(UseBMI1Instructions);
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);

  ins_cost(125);
  format %{ "blsmskq $dst, $src" %}

  ins_encode %{
    __ blsmskq($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct blsmskL_rReg_rReg(rRegL dst, rRegL src, immL_M1 minus_1, rFlagsReg cr)
%{
  match(Set dst (XorL (AddL src minus_1) src));
  predicate(UseBMI1Instructions);
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);

  format %{ "blsmskq $dst, $src" %}

  ins_encode %{
    __ blsmskq($dst$$Register, $src$$Register);
  %}

  ins_pipe(ialu_reg);
%}

instruct blsrL_rReg_rReg(rRegL dst, rRegL src, immL_M1 minus_1, rFlagsReg cr)
%{
  match(Set dst (AndL (AddL src minus_1) src) );
  predicate(UseBMI1Instructions);
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);

  format %{ "blsrq  $dst, $src" %}

  ins_encode %{
    __ blsrq($dst$$Register, $src$$Register);
  %}

  ins_pipe(ialu_reg);
%}

instruct blsrL_rReg_mem(rRegL dst, memory src, immL_M1 minus_1, rFlagsReg cr)
%{
  match(Set dst (AndL (AddL (LoadL src) minus_1) (LoadL src)) );
  predicate(UseBMI1Instructions);
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);

  ins_cost(125);
  format %{ "blsrq  $dst, $src" %}

  ins_encode %{
    __ blsrq($dst$$Register, $src$$Address);
  %}

  ins_pipe(ialu_reg);
%}

// Or Instructions
// Or Register with Register
instruct orL_rReg(rRegL dst, rRegL src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (OrL dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "orq     $dst, $src\t# long" %}
  ins_encode %{
    __ orq($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Or Register with Register using New Data Destination (NDD)
instruct orL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (OrL src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "eorq     $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ eorq($dst$$Register, $src1$$Register, $src2$$Register, false);

  %}
  ins_pipe(ialu_reg_reg);
%}

// Use any_RegP to match R15 (TLS register) without spilling.
instruct orL_rReg_castP2X(rRegL dst, any_RegP src, rFlagsReg cr) %{
  match(Set dst (OrL dst (CastP2X src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "orq     $dst, $src\t# long" %}
  ins_encode %{
    __ orq($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct orL_rReg_castP2X_ndd(rRegL dst, any_RegP src1, any_RegP src2, rFlagsReg cr) %{
  match(Set dst (OrL src1 (CastP2X src2)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "eorq     $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ eorq($dst$$Register, $src1$$Register, $src2$$Register, false);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Or Register with Immediate
instruct orL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (OrL dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "orq     $dst, $src\t# long" %}
  ins_encode %{
    __ orq($dst$$Register, $src$$constant);
  %}
  ins_pipe(ialu_reg);
%}

instruct orL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (OrL src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "eorq     $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ eorq($dst$$Register, $src1$$Register, $src2$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

instruct orL_rReg_imm_rReg_ndd(rRegL dst, immL32 src1, rRegL src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (OrL src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "eorq     $dst, $src2, $src1\t# long ndd" %}
  ins_encode %{
    __ eorq($dst$$Register, $src2$$Register, $src1$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

// Or Memory with Immediate
instruct orL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (OrL (LoadL src1) src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "eorq     $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ eorq($dst$$Register, $src1$$Address, $src2$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

// Or Register with Memory
instruct orL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (OrL dst (LoadL src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "orq     $dst, $src\t# long" %}
  ins_encode %{
    __ orq($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct orL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (OrL src1 (LoadL src2)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "eorq     $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ eorq($dst$$Register, $src1$$Register, $src2$$Address, false);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Or Memory with Register
instruct orL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)
%{
  match(Set dst (StoreL dst (OrL (LoadL dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "orq     $dst, $src\t# long" %}
  ins_encode %{
    __ orq($dst$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

// Or Memory with Immediate
instruct orL_mem_imm(memory dst, immL32 src, rFlagsReg cr)
%{
  match(Set dst (StoreL dst (OrL (LoadL dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(125);
  format %{ "orq     $dst, $src\t# long" %}
  ins_encode %{
    __ orq($dst$$Address, $src$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

instruct btsL_mem_imm(memory dst, immL_Pow2 con, rFlagsReg cr)
%{
  // con should be a pure 64-bit power of 2 immediate
  // because AND/OR works well enough for 8/32-bit values.
  predicate(log2i_graceful(n->in(3)->in(2)->get_long()) > 31);

  match(Set dst (StoreL dst (OrL (LoadL dst) con)));
  effect(KILL cr);

  ins_cost(125);
  format %{ "btsq    $dst, log2($con)\t# long" %}
  ins_encode %{
    __ btsq($dst$$Address, log2i_exact((julong)$con$$constant));
  %}
  ins_pipe(ialu_mem_imm);
%}

// Xor Instructions
// Xor Register with Register
instruct xorL_rReg(rRegL dst, rRegL src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (XorL dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "xorq    $dst, $src\t# long" %}
  ins_encode %{
    __ xorq($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Xor Register with Register using New Data Destination (NDD)
instruct xorL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (XorL src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "exorq    $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ exorq($dst$$Register, $src1$$Register, $src2$$Register, false);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Xor Register with Immediate -1
instruct xorL_rReg_im1(rRegL dst, immL_M1 imm)
%{
  predicate(!UseAPX);
  match(Set dst (XorL dst imm));

  format %{ "notq   $dst" %}
  ins_encode %{
     __ notq($dst$$Register);
  %}
  ins_pipe(ialu_reg);
%}

instruct xorL_rReg_im1_ndd(rRegL dst,rRegL src, immL_M1 imm)
%{
  predicate(UseAPX);
  match(Set dst (XorL src imm));

  format %{ "enotq   $dst, $src" %}
  ins_encode %{
    __ enotq($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg);
%}

// Xor Register with Immediate
instruct xorL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)
%{
  // Strict predicate check to make selection of xorL_rReg_im1 cost agnostic if immL32 src is -1.
  predicate(!UseAPX && n->in(2)->bottom_type()->is_long()->get_con() != -1L);
  match(Set dst (XorL dst src));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "xorq    $dst, $src\t# long" %}
  ins_encode %{
    __ xorq($dst$$Register, $src$$constant);
  %}
  ins_pipe(ialu_reg);
%}

instruct xorL_rReg_rReg_imm(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)
%{
  // Strict predicate check to make selection of xorL_rReg_im1_ndd cost agnostic if immL32 src2 is -1.
  predicate(UseAPX && n->in(2)->bottom_type()->is_long()->get_con() != -1L);
  match(Set dst (XorL src1 src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  format %{ "exorq    $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ exorq($dst$$Register, $src1$$Register, $src2$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

// Xor Memory with Immediate
instruct xorL_rReg_mem_imm(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (XorL (LoadL src1) src2));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);
  ins_cost(150);

  format %{ "exorq    $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ exorq($dst$$Register, $src1$$Address, $src2$$constant, false);
  %}
  ins_pipe(ialu_reg);
%}

// Xor Register with Memory
instruct xorL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  match(Set dst (XorL dst (LoadL src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "xorq    $dst, $src\t# long" %}
  ins_encode %{
    __ xorq($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct xorL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  match(Set dst (XorL src1 (LoadL src2)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "exorq    $dst, $src1, $src2\t# long ndd" %}
  ins_encode %{
    __ exorq($dst$$Register, $src1$$Register, $src2$$Address, false);
  %}
  ins_pipe(ialu_reg_mem);
%}

// Xor Memory with Register
instruct xorL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)
%{
  match(Set dst (StoreL dst (XorL (LoadL dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(150);
  format %{ "xorq    $dst, $src\t# long" %}
  ins_encode %{
    __ xorq($dst$$Address, $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

// Xor Memory with Immediate
instruct xorL_mem_imm(memory dst, immL32 src, rFlagsReg cr)
%{
  match(Set dst (StoreL dst (XorL (LoadL dst) src)));
  effect(KILL cr);
  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);

  ins_cost(125);
  format %{ "xorq    $dst, $src\t# long" %}
  ins_encode %{
    __ xorq($dst$$Address, $src$$constant);
  %}
  ins_pipe(ialu_mem_imm);
%}

instruct cmpLTMask(rRegI dst, rRegI p, rRegI q, rFlagsReg cr)
%{
  match(Set dst (CmpLTMask p q));
  effect(KILL cr);

  ins_cost(400);
  format %{ "cmpl    $p, $q\t# cmpLTMask\n\t"
            "setcc   $dst \t# emits setlt + movzbl or setzul for APX"
            "negl    $dst" %}
  ins_encode %{
    __ cmpl($p$$Register, $q$$Register);
    __ setcc(Assembler::less, $dst$$Register);
    __ negl($dst$$Register);
  %}
  ins_pipe(pipe_slow);
%}

instruct cmpLTMask0(rRegI dst, immI_0 zero, rFlagsReg cr)
%{
  match(Set dst (CmpLTMask dst zero));
  effect(KILL cr);

  ins_cost(100);
  format %{ "sarl    $dst, #31\t# cmpLTMask0" %}
  ins_encode %{
    __ sarl($dst$$Register, 31);
  %}
  ins_pipe(ialu_reg);
%}

/* Better to save a register than avoid a branch */
instruct cadd_cmpLTMask(rRegI p, rRegI q, rRegI y, rFlagsReg cr)
%{
  match(Set p (AddI (AndI (CmpLTMask p q) y) (SubI p q)));
  effect(KILL cr);
  ins_cost(300);
  format %{ "subl    $p,$q\t# cadd_cmpLTMask\n\t"
            "jge     done\n\t"
            "addl    $p,$y\n"
            "done:   " %}
  ins_encode %{
    Register Rp = $p$$Register;
    Register Rq = $q$$Register;
    Register Ry = $y$$Register;
    Label done;
    __ subl(Rp, Rq);
    __ jccb(Assembler::greaterEqual, done);
    __ addl(Rp, Ry);
    __ bind(done);
  %}
  ins_pipe(pipe_cmplt);
%}

/* Better to save a register than avoid a branch */
instruct and_cmpLTMask(rRegI p, rRegI q, rRegI y, rFlagsReg cr)
%{
  match(Set y (AndI (CmpLTMask p q) y));
  effect(KILL cr);

  ins_cost(300);

  format %{ "cmpl    $p, $q\t# and_cmpLTMask\n\t"
            "jlt     done\n\t"
            "xorl    $y, $y\n"
            "done:   " %}
  ins_encode %{
    Register Rp = $p$$Register;
    Register Rq = $q$$Register;
    Register Ry = $y$$Register;
    Label done;
    __ cmpl(Rp, Rq);
    __ jccb(Assembler::less, done);
    __ xorl(Ry, Ry);
    __ bind(done);
  %}
  ins_pipe(pipe_cmplt);
%}


//---------- FP Instructions------------------------------------------------

// Really expensive, avoid
instruct cmpF_cc_reg(rFlagsRegU cr, regF src1, regF src2)
%{
  match(Set cr (CmpF src1 src2));

  ins_cost(500);
  format %{ "ucomiss $src1, $src2\n\t"
            "jnp,s   exit\n\t"
            "pushfq\t# saw NaN, set CF\n\t"
            "andq    [rsp], #0xffffff2b\n\t"
            "popfq\n"
    "exit:" %}
  ins_encode %{
    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);
    emit_cmpfp_fixup(masm);
  %}
  ins_pipe(pipe_slow);
%}

instruct cmpF_cc_reg_CF(rFlagsRegUCF cr, regF src1, regF src2) %{
  match(Set cr (CmpF src1 src2));

  ins_cost(100);
  format %{ "ucomiss $src1, $src2" %}
  ins_encode %{
    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct cmpF_cc_memCF(rFlagsRegUCF cr, regF src1, memory src2) %{
  match(Set cr (CmpF src1 (LoadF src2)));

  ins_cost(100);
  format %{ "ucomiss $src1, $src2" %}
  ins_encode %{
    __ ucomiss($src1$$XMMRegister, $src2$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct cmpF_cc_immCF(rFlagsRegUCF cr, regF src, immF con) %{
  match(Set cr (CmpF src con));
  ins_cost(100);
  format %{ "ucomiss $src, [$constantaddress]\t# load from constant table: float=$con" %}
  ins_encode %{
    __ ucomiss($src$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

// Really expensive, avoid
instruct cmpD_cc_reg(rFlagsRegU cr, regD src1, regD src2)
%{
  match(Set cr (CmpD src1 src2));

  ins_cost(500);
  format %{ "ucomisd $src1, $src2\n\t"
            "jnp,s   exit\n\t"
            "pushfq\t# saw NaN, set CF\n\t"
            "andq    [rsp], #0xffffff2b\n\t"
            "popfq\n"
    "exit:" %}
  ins_encode %{
    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);
    emit_cmpfp_fixup(masm);
  %}
  ins_pipe(pipe_slow);
%}

instruct cmpD_cc_reg_CF(rFlagsRegUCF cr, regD src1, regD src2) %{
  match(Set cr (CmpD src1 src2));

  ins_cost(100);
  format %{ "ucomisd $src1, $src2 test" %}
  ins_encode %{
    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct cmpD_cc_memCF(rFlagsRegUCF cr, regD src1, memory src2) %{
  match(Set cr (CmpD src1 (LoadD src2)));

  ins_cost(100);
  format %{ "ucomisd $src1, $src2" %}
  ins_encode %{
    __ ucomisd($src1$$XMMRegister, $src2$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct cmpD_cc_immCF(rFlagsRegUCF cr, regD src, immD con) %{
  match(Set cr (CmpD src con));
  ins_cost(100);
  format %{ "ucomisd $src, [$constantaddress]\t# load from constant table: double=$con" %}
  ins_encode %{
    __ ucomisd($src$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

// Compare into -1,0,1
instruct cmpF_reg(rRegI dst, regF src1, regF src2, rFlagsReg cr)
%{
  match(Set dst (CmpF3 src1 src2));
  effect(KILL cr);

  ins_cost(275);
  format %{ "ucomiss $src1, $src2\n\t"
            "movl    $dst, #-1\n\t"
            "jp,s    done\n\t"
            "jb,s    done\n\t"
            "setne   $dst\n\t"
            "movzbl  $dst, $dst\n"
    "done:" %}
  ins_encode %{
    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);
    emit_cmpfp3(masm, $dst$$Register);
  %}
  ins_pipe(pipe_slow);
%}

// Compare into -1,0,1
instruct cmpF_mem(rRegI dst, regF src1, memory src2, rFlagsReg cr)
%{
  match(Set dst (CmpF3 src1 (LoadF src2)));
  effect(KILL cr);

  ins_cost(275);
  format %{ "ucomiss $src1, $src2\n\t"
            "movl    $dst, #-1\n\t"
            "jp,s    done\n\t"
            "jb,s    done\n\t"
            "setne   $dst\n\t"
            "movzbl  $dst, $dst\n"
    "done:" %}
  ins_encode %{
    __ ucomiss($src1$$XMMRegister, $src2$$Address);
    emit_cmpfp3(masm, $dst$$Register);
  %}
  ins_pipe(pipe_slow);
%}

// Compare into -1,0,1
instruct cmpF_imm(rRegI dst, regF src, immF con, rFlagsReg cr) %{
  match(Set dst (CmpF3 src con));
  effect(KILL cr);

  ins_cost(275);
  format %{ "ucomiss $src, [$constantaddress]\t# load from constant table: float=$con\n\t"
            "movl    $dst, #-1\n\t"
            "jp,s    done\n\t"
            "jb,s    done\n\t"
            "setne   $dst\n\t"
            "movzbl  $dst, $dst\n"
    "done:" %}
  ins_encode %{
    __ ucomiss($src$$XMMRegister, $constantaddress($con));
    emit_cmpfp3(masm, $dst$$Register);
  %}
  ins_pipe(pipe_slow);
%}

// Compare into -1,0,1
instruct cmpD_reg(rRegI dst, regD src1, regD src2, rFlagsReg cr)
%{
  match(Set dst (CmpD3 src1 src2));
  effect(KILL cr);

  ins_cost(275);
  format %{ "ucomisd $src1, $src2\n\t"
            "movl    $dst, #-1\n\t"
            "jp,s    done\n\t"
            "jb,s    done\n\t"
            "setne   $dst\n\t"
            "movzbl  $dst, $dst\n"
    "done:" %}
  ins_encode %{
    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);
    emit_cmpfp3(masm, $dst$$Register);
  %}
  ins_pipe(pipe_slow);
%}

// Compare into -1,0,1
instruct cmpD_mem(rRegI dst, regD src1, memory src2, rFlagsReg cr)
%{
  match(Set dst (CmpD3 src1 (LoadD src2)));
  effect(KILL cr);

  ins_cost(275);
  format %{ "ucomisd $src1, $src2\n\t"
            "movl    $dst, #-1\n\t"
            "jp,s    done\n\t"
            "jb,s    done\n\t"
            "setne   $dst\n\t"
            "movzbl  $dst, $dst\n"
    "done:" %}
  ins_encode %{
    __ ucomisd($src1$$XMMRegister, $src2$$Address);
    emit_cmpfp3(masm, $dst$$Register);
  %}
  ins_pipe(pipe_slow);
%}

// Compare into -1,0,1
instruct cmpD_imm(rRegI dst, regD src, immD con, rFlagsReg cr) %{
  match(Set dst (CmpD3 src con));
  effect(KILL cr);

  ins_cost(275);
  format %{ "ucomisd $src, [$constantaddress]\t# load from constant table: double=$con\n\t"
            "movl    $dst, #-1\n\t"
            "jp,s    done\n\t"
            "jb,s    done\n\t"
            "setne   $dst\n\t"
            "movzbl  $dst, $dst\n"
    "done:" %}
  ins_encode %{
    __ ucomisd($src$$XMMRegister, $constantaddress($con));
    emit_cmpfp3(masm, $dst$$Register);
  %}
  ins_pipe(pipe_slow);
%}

//----------Arithmetic Conversion Instructions---------------------------------

instruct convF2D_reg_reg(regD dst, regF src)
%{
  match(Set dst (ConvF2D src));

  format %{ "cvtss2sd $dst, $src" %}
  ins_encode %{
    __ cvtss2sd ($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct convF2D_reg_mem(regD dst, memory src)
%{
  predicate(UseAVX == 0);
  match(Set dst (ConvF2D (LoadF src)));

  format %{ "cvtss2sd $dst, $src" %}
  ins_encode %{
    __ cvtss2sd ($dst$$XMMRegister, $src$$Address);
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct convD2F_reg_reg(regF dst, regD src)
%{
  match(Set dst (ConvD2F src));

  format %{ "cvtsd2ss $dst, $src" %}
  ins_encode %{
    __ cvtsd2ss ($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct convD2F_reg_mem(regF dst, memory src)
%{
  predicate(UseAVX == 0);
  match(Set dst (ConvD2F (LoadD src)));

  format %{ "cvtsd2ss $dst, $src" %}
  ins_encode %{
    __ cvtsd2ss ($dst$$XMMRegister, $src$$Address);
  %}
  ins_pipe(pipe_slow); // XXX
%}

// XXX do mem variants
instruct convF2I_reg_reg(rRegI dst, regF src, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_avx10_2());
  match(Set dst (ConvF2I src));
  effect(KILL cr);
  format %{ "convert_f2i $dst, $src" %}
  ins_encode %{
    __ convertF2I(T_INT, T_FLOAT, $dst$$Register, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct convF2I_reg_reg_avx10(rRegI dst, regF src)
%{
  predicate(VM_Version::supports_avx10_2());
  match(Set dst (ConvF2I src));
  format %{ "evcvttss2sisl $dst, $src" %}
  ins_encode %{
    __ evcvttss2sisl($dst$$Register, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct convF2I_reg_mem_avx10(rRegI dst, memory src)
%{
  predicate(VM_Version::supports_avx10_2());
  match(Set dst (ConvF2I (LoadF src)));
  format %{ "evcvttss2sisl $dst, $src" %}
  ins_encode %{
    __ evcvttss2sisl($dst$$Register, $src$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct convF2L_reg_reg(rRegL dst, regF src, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_avx10_2());
  match(Set dst (ConvF2L src));
  effect(KILL cr);
  format %{ "convert_f2l $dst, $src"%}
  ins_encode %{
    __ convertF2I(T_LONG, T_FLOAT, $dst$$Register, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct convF2L_reg_reg_avx10(rRegL dst, regF src)
%{
  predicate(VM_Version::supports_avx10_2());
  match(Set dst (ConvF2L src));
  format %{ "evcvttss2sisq $dst, $src" %}
  ins_encode %{
    __ evcvttss2sisq($dst$$Register, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct convF2L_reg_mem_avx10(rRegL dst, memory src)
%{
  predicate(VM_Version::supports_avx10_2());
  match(Set dst (ConvF2L (LoadF src)));
  format %{ "evcvttss2sisq $dst, $src" %}
  ins_encode %{
    __ evcvttss2sisq($dst$$Register, $src$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct convD2I_reg_reg(rRegI dst, regD src, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_avx10_2());
  match(Set dst (ConvD2I src));
  effect(KILL cr);
  format %{ "convert_d2i $dst, $src"%}
  ins_encode %{
    __ convertF2I(T_INT, T_DOUBLE, $dst$$Register, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct convD2I_reg_reg_avx10(rRegI dst, regD src)
%{
  predicate(VM_Version::supports_avx10_2());
  match(Set dst (ConvD2I src));
  format %{ "evcvttsd2sisl $dst, $src" %}
  ins_encode %{
    __ evcvttsd2sisl($dst$$Register, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct convD2I_reg_mem_avx10(rRegI dst, memory src)
%{
  predicate(VM_Version::supports_avx10_2());
  match(Set dst (ConvD2I (LoadD src)));
  format %{ "evcvttsd2sisl $dst, $src" %}
  ins_encode %{
    __ evcvttsd2sisl($dst$$Register, $src$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct convD2L_reg_reg(rRegL dst, regD src, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_avx10_2());
  match(Set dst (ConvD2L src));
  effect(KILL cr);
  format %{ "convert_d2l $dst, $src"%}
  ins_encode %{
    __ convertF2I(T_LONG, T_DOUBLE, $dst$$Register, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct convD2L_reg_reg_avx10(rRegL dst, regD src)
%{
  predicate(VM_Version::supports_avx10_2());
  match(Set dst (ConvD2L src));
  format %{ "evcvttsd2sisq $dst, $src" %}
  ins_encode %{
    __ evcvttsd2sisq($dst$$Register, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct convD2L_reg_mem_avx10(rRegL dst, memory src)
%{
  predicate(VM_Version::supports_avx10_2());
  match(Set dst (ConvD2L (LoadD src)));
  format %{ "evcvttsd2sisq $dst, $src" %}
  ins_encode %{
    __ evcvttsd2sisq($dst$$Register, $src$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct round_double_reg(rRegL dst, regD src, rRegL rtmp, rcx_RegL rcx, rFlagsReg cr)
%{
  match(Set dst (RoundD src));
  effect(TEMP dst, TEMP rtmp, TEMP rcx, KILL cr);
  format %{ "round_double $dst,$src \t! using $rtmp and $rcx as TEMP"%}
  ins_encode %{
    __ round_double($dst$$Register, $src$$XMMRegister, $rtmp$$Register, $rcx$$Register);
  %}
  ins_pipe(pipe_slow);
%}

instruct round_float_reg(rRegI dst, regF src, rRegL rtmp, rcx_RegL rcx, rFlagsReg cr)
%{
  match(Set dst (RoundF src));
  effect(TEMP dst, TEMP rtmp, TEMP rcx, KILL cr);
  format %{ "round_float $dst,$src" %}
  ins_encode %{
    __ round_float($dst$$Register, $src$$XMMRegister, $rtmp$$Register, $rcx$$Register);
  %}
  ins_pipe(pipe_slow);
%}

instruct convI2F_reg_reg(vlRegF dst, rRegI src)
%{
  predicate(!UseXmmI2F);
  match(Set dst (ConvI2F src));

  format %{ "cvtsi2ssl $dst, $src\t# i2f" %}
  ins_encode %{
    if (UseAVX > 0) {
      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
    }
    __ cvtsi2ssl ($dst$$XMMRegister, $src$$Register);
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct convI2F_reg_mem(regF dst, memory src)
%{
  predicate(UseAVX == 0);
  match(Set dst (ConvI2F (LoadI src)));

  format %{ "cvtsi2ssl $dst, $src\t# i2f" %}
  ins_encode %{
    __ cvtsi2ssl ($dst$$XMMRegister, $src$$Address);
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct convI2D_reg_reg(vlRegD dst, rRegI src)
%{
  predicate(!UseXmmI2D);
  match(Set dst (ConvI2D src));

  format %{ "cvtsi2sdl $dst, $src\t# i2d" %}
  ins_encode %{
    if (UseAVX > 0) {
      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
    }
    __ cvtsi2sdl ($dst$$XMMRegister, $src$$Register);
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct convI2D_reg_mem(regD dst, memory src)
%{
  predicate(UseAVX == 0);
  match(Set dst (ConvI2D (LoadI src)));

  format %{ "cvtsi2sdl $dst, $src\t# i2d" %}
  ins_encode %{
    __ cvtsi2sdl ($dst$$XMMRegister, $src$$Address);
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct convXI2F_reg(regF dst, rRegI src)
%{
  predicate(UseXmmI2F);
  match(Set dst (ConvI2F src));

  format %{ "movdl $dst, $src\n\t"
            "cvtdq2psl $dst, $dst\t# i2f" %}
  ins_encode %{
    __ movdl($dst$$XMMRegister, $src$$Register);
    __ cvtdq2ps($dst$$XMMRegister, $dst$$XMMRegister);
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct convXI2D_reg(regD dst, rRegI src)
%{
  predicate(UseXmmI2D);
  match(Set dst (ConvI2D src));

  format %{ "movdl $dst, $src\n\t"
            "cvtdq2pdl $dst, $dst\t# i2d" %}
  ins_encode %{
    __ movdl($dst$$XMMRegister, $src$$Register);
    __ cvtdq2pd($dst$$XMMRegister, $dst$$XMMRegister);
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct convL2F_reg_reg(vlRegF dst, rRegL src)
%{
  match(Set dst (ConvL2F src));

  format %{ "cvtsi2ssq $dst, $src\t# l2f" %}
  ins_encode %{
    if (UseAVX > 0) {
      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
    }
    __ cvtsi2ssq ($dst$$XMMRegister, $src$$Register);
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct convL2F_reg_mem(regF dst, memory src)
%{
  predicate(UseAVX == 0);
  match(Set dst (ConvL2F (LoadL src)));

  format %{ "cvtsi2ssq $dst, $src\t# l2f" %}
  ins_encode %{
    __ cvtsi2ssq ($dst$$XMMRegister, $src$$Address);
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct convL2D_reg_reg(vlRegD dst, rRegL src)
%{
  match(Set dst (ConvL2D src));

  format %{ "cvtsi2sdq $dst, $src\t# l2d" %}
  ins_encode %{
    if (UseAVX > 0) {
      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
    }
    __ cvtsi2sdq ($dst$$XMMRegister, $src$$Register);
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct convL2D_reg_mem(regD dst, memory src)
%{
  predicate(UseAVX == 0);
  match(Set dst (ConvL2D (LoadL src)));

  format %{ "cvtsi2sdq $dst, $src\t# l2d" %}
  ins_encode %{
    __ cvtsi2sdq ($dst$$XMMRegister, $src$$Address);
  %}
  ins_pipe(pipe_slow); // XXX
%}

instruct convI2L_reg_reg(rRegL dst, rRegI src)
%{
  match(Set dst (ConvI2L src));

  ins_cost(125);
  format %{ "movslq  $dst, $src\t# i2l" %}
  ins_encode %{
    __ movslq($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

// Zero-extend convert int to long
instruct convI2L_reg_reg_zex(rRegL dst, rRegI src, immL_32bits mask)
%{
  match(Set dst (AndL (ConvI2L src) mask));

  format %{ "movl    $dst, $src\t# i2l zero-extend\n\t" %}
  ins_encode %{
    if ($dst$$reg != $src$$reg) {
      __ movl($dst$$Register, $src$$Register);
    }
  %}
  ins_pipe(ialu_reg_reg);
%}

// Zero-extend convert int to long
instruct convI2L_reg_mem_zex(rRegL dst, memory src, immL_32bits mask)
%{
  match(Set dst (AndL (ConvI2L (LoadI src)) mask));

  format %{ "movl    $dst, $src\t# i2l zero-extend\n\t" %}
  ins_encode %{
    __ movl($dst$$Register, $src$$Address);
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct zerox_long_reg_reg(rRegL dst, rRegL src, immL_32bits mask)
%{
  match(Set dst (AndL src mask));

  format %{ "movl    $dst, $src\t# zero-extend long" %}
  ins_encode %{
    __ movl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct convL2I_reg_reg(rRegI dst, rRegL src)
%{
  match(Set dst (ConvL2I src));

  format %{ "movl    $dst, $src\t# l2i" %}
  ins_encode %{
    __ movl($dst$$Register, $src$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}


instruct MoveF2I_stack_reg(rRegI dst, stackSlotF src) %{
  match(Set dst (MoveF2I src));
  effect(DEF dst, USE src);

  ins_cost(125);
  format %{ "movl    $dst, $src\t# MoveF2I_stack_reg" %}
  ins_encode %{
    __ movl($dst$$Register, Address(rsp, $src$$disp));
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct MoveI2F_stack_reg(regF dst, stackSlotI src) %{
  match(Set dst (MoveI2F src));
  effect(DEF dst, USE src);

  ins_cost(125);
  format %{ "movss   $dst, $src\t# MoveI2F_stack_reg" %}
  ins_encode %{
    __ movflt($dst$$XMMRegister, Address(rsp, $src$$disp));
  %}
  ins_pipe(pipe_slow);
%}

instruct MoveD2L_stack_reg(rRegL dst, stackSlotD src) %{
  match(Set dst (MoveD2L src));
  effect(DEF dst, USE src);

  ins_cost(125);
  format %{ "movq    $dst, $src\t# MoveD2L_stack_reg" %}
  ins_encode %{
    __ movq($dst$$Register, Address(rsp, $src$$disp));
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct MoveL2D_stack_reg_partial(regD dst, stackSlotL src) %{
  predicate(!UseXmmLoadAndClearUpper);
  match(Set dst (MoveL2D src));
  effect(DEF dst, USE src);

  ins_cost(125);
  format %{ "movlpd  $dst, $src\t# MoveL2D_stack_reg" %}
  ins_encode %{
    __ movdbl($dst$$XMMRegister, Address(rsp, $src$$disp));
  %}
  ins_pipe(pipe_slow);
%}

instruct MoveL2D_stack_reg(regD dst, stackSlotL src) %{
  predicate(UseXmmLoadAndClearUpper);
  match(Set dst (MoveL2D src));
  effect(DEF dst, USE src);

  ins_cost(125);
  format %{ "movsd   $dst, $src\t# MoveL2D_stack_reg" %}
  ins_encode %{
    __ movdbl($dst$$XMMRegister, Address(rsp, $src$$disp));
  %}
  ins_pipe(pipe_slow);
%}


instruct MoveF2I_reg_stack(stackSlotI dst, regF src) %{
  match(Set dst (MoveF2I src));
  effect(DEF dst, USE src);

  ins_cost(95); // XXX
  format %{ "movss   $dst, $src\t# MoveF2I_reg_stack" %}
  ins_encode %{
    __ movflt(Address(rsp, $dst$$disp), $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct MoveI2F_reg_stack(stackSlotF dst, rRegI src) %{
  match(Set dst (MoveI2F src));
  effect(DEF dst, USE src);

  ins_cost(100);
  format %{ "movl    $dst, $src\t# MoveI2F_reg_stack" %}
  ins_encode %{
    __ movl(Address(rsp, $dst$$disp), $src$$Register);
  %}
  ins_pipe( ialu_mem_reg );
%}

instruct MoveD2L_reg_stack(stackSlotL dst, regD src) %{
  match(Set dst (MoveD2L src));
  effect(DEF dst, USE src);

  ins_cost(95); // XXX
  format %{ "movsd   $dst, $src\t# MoveL2D_reg_stack" %}
  ins_encode %{
    __ movdbl(Address(rsp, $dst$$disp), $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct MoveL2D_reg_stack(stackSlotD dst, rRegL src) %{
  match(Set dst (MoveL2D src));
  effect(DEF dst, USE src);

  ins_cost(100);
  format %{ "movq    $dst, $src\t# MoveL2D_reg_stack" %}
  ins_encode %{
    __ movq(Address(rsp, $dst$$disp), $src$$Register);
  %}
  ins_pipe(ialu_mem_reg);
%}

instruct MoveF2I_reg_reg(rRegI dst, regF src) %{
  match(Set dst (MoveF2I src));
  effect(DEF dst, USE src);
  ins_cost(85);
  format %{ "movd    $dst,$src\t# MoveF2I" %}
  ins_encode %{
    __ movdl($dst$$Register, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct MoveD2L_reg_reg(rRegL dst, regD src) %{
  match(Set dst (MoveD2L src));
  effect(DEF dst, USE src);
  ins_cost(85);
  format %{ "movd    $dst,$src\t# MoveD2L" %}
  ins_encode %{
    __ movdq($dst$$Register, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct MoveI2F_reg_reg(regF dst, rRegI src) %{
  match(Set dst (MoveI2F src));
  effect(DEF dst, USE src);
  ins_cost(100);
  format %{ "movd    $dst,$src\t# MoveI2F" %}
  ins_encode %{
    __ movdl($dst$$XMMRegister, $src$$Register);
  %}
  ins_pipe( pipe_slow );
%}

instruct MoveL2D_reg_reg(regD dst, rRegL src) %{
  match(Set dst (MoveL2D src));
  effect(DEF dst, USE src);
  ins_cost(100);
  format %{ "movd    $dst,$src\t# MoveL2D" %}
  ins_encode %{
     __ movdq($dst$$XMMRegister, $src$$Register);
  %}
  ins_pipe( pipe_slow );
%}

// Fast clearing of an array
// Small non-constant lenght ClearArray for non-AVX512 targets.
instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,
                  Universe dummy, rFlagsReg cr)
%{
  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2));
  match(Set dummy (ClearArray cnt base));
  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);

  format %{ $$template
    $$emit$$"xorq    rax, rax\t# ClearArray:\n\t"
    $$emit$$"cmp     InitArrayShortSize,rcx\n\t"
    $$emit$$"jg      LARGE\n\t"
    $$emit$$"dec     rcx\n\t"
    $$emit$$"js      DONE\t# Zero length\n\t"
    $$emit$$"mov     rax,(rdi,rcx,8)\t# LOOP\n\t"
    $$emit$$"dec     rcx\n\t"
    $$emit$$"jge     LOOP\n\t"
    $$emit$$"jmp     DONE\n\t"
    $$emit$$"# LARGE:\n\t"
    if (UseFastStosb) {
       $$emit$$"shlq    rcx,3\t# Convert doublewords to bytes\n\t"
       $$emit$$"rep     stosb\t# Store rax to *rdi++ while rcx--\n\t"
    } else if (UseXMMForObjInit) {
       $$emit$$"mov     rdi,rax\n\t"
       $$emit$$"vpxor   ymm0,ymm0,ymm0\n\t"
       $$emit$$"jmpq    L_zero_64_bytes\n\t"
       $$emit$$"# L_loop:\t# 64-byte LOOP\n\t"
       $$emit$$"vmovdqu ymm0,(rax)\n\t"
       $$emit$$"vmovdqu ymm0,0x20(rax)\n\t"
       $$emit$$"add     0x40,rax\n\t"
       $$emit$$"# L_zero_64_bytes:\n\t"
       $$emit$$"sub     0x8,rcx\n\t"
       $$emit$$"jge     L_loop\n\t"
       $$emit$$"add     0x4,rcx\n\t"
       $$emit$$"jl      L_tail\n\t"
       $$emit$$"vmovdqu ymm0,(rax)\n\t"
       $$emit$$"add     0x20,rax\n\t"
       $$emit$$"sub     0x4,rcx\n\t"
       $$emit$$"# L_tail:\t# Clearing tail bytes\n\t"
       $$emit$$"add     0x4,rcx\n\t"
       $$emit$$"jle     L_end\n\t"
       $$emit$$"dec     rcx\n\t"
       $$emit$$"# L_sloop:\t# 8-byte short loop\n\t"
       $$emit$$"vmovq   xmm0,(rax)\n\t"
       $$emit$$"add     0x8,rax\n\t"
       $$emit$$"dec     rcx\n\t"
       $$emit$$"jge     L_sloop\n\t"
       $$emit$$"# L_end:\n\t"
    } else {
       $$emit$$"rep     stosq\t# Store rax to *rdi++ while rcx--\n\t"
    }
    $$emit$$"# DONE"
  %}
  ins_encode %{
    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,
                 $tmp$$XMMRegister, false, knoreg);
  %}
  ins_pipe(pipe_slow);
%}

// Small non-constant length ClearArray for AVX512 targets.
instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,
                       Universe dummy, rFlagsReg cr)
%{
  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2));
  match(Set dummy (ClearArray cnt base));
  ins_cost(125);
  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);

  format %{ $$template
    $$emit$$"xorq    rax, rax\t# ClearArray:\n\t"
    $$emit$$"cmp     InitArrayShortSize,rcx\n\t"
    $$emit$$"jg      LARGE\n\t"
    $$emit$$"dec     rcx\n\t"
    $$emit$$"js      DONE\t# Zero length\n\t"
    $$emit$$"mov     rax,(rdi,rcx,8)\t# LOOP\n\t"
    $$emit$$"dec     rcx\n\t"
    $$emit$$"jge     LOOP\n\t"
    $$emit$$"jmp     DONE\n\t"
    $$emit$$"# LARGE:\n\t"
    if (UseFastStosb) {
       $$emit$$"shlq    rcx,3\t# Convert doublewords to bytes\n\t"
       $$emit$$"rep     stosb\t# Store rax to *rdi++ while rcx--\n\t"
    } else if (UseXMMForObjInit) {
       $$emit$$"mov     rdi,rax\n\t"
       $$emit$$"vpxor   ymm0,ymm0,ymm0\n\t"
       $$emit$$"jmpq    L_zero_64_bytes\n\t"
       $$emit$$"# L_loop:\t# 64-byte LOOP\n\t"
       $$emit$$"vmovdqu ymm0,(rax)\n\t"
       $$emit$$"vmovdqu ymm0,0x20(rax)\n\t"
       $$emit$$"add     0x40,rax\n\t"
       $$emit$$"# L_zero_64_bytes:\n\t"
       $$emit$$"sub     0x8,rcx\n\t"
       $$emit$$"jge     L_loop\n\t"
       $$emit$$"add     0x4,rcx\n\t"
       $$emit$$"jl      L_tail\n\t"
       $$emit$$"vmovdqu ymm0,(rax)\n\t"
       $$emit$$"add     0x20,rax\n\t"
       $$emit$$"sub     0x4,rcx\n\t"
       $$emit$$"# L_tail:\t# Clearing tail bytes\n\t"
       $$emit$$"add     0x4,rcx\n\t"
       $$emit$$"jle     L_end\n\t"
       $$emit$$"dec     rcx\n\t"
       $$emit$$"# L_sloop:\t# 8-byte short loop\n\t"
       $$emit$$"vmovq   xmm0,(rax)\n\t"
       $$emit$$"add     0x8,rax\n\t"
       $$emit$$"dec     rcx\n\t"
       $$emit$$"jge     L_sloop\n\t"
       $$emit$$"# L_end:\n\t"
    } else {
       $$emit$$"rep     stosq\t# Store rax to *rdi++ while rcx--\n\t"
    }
    $$emit$$"# DONE"
  %}
  ins_encode %{
    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,
                 $tmp$$XMMRegister, false, $ktmp$$KRegister);
  %}
  ins_pipe(pipe_slow);
%}

// Large non-constant length ClearArray for non-AVX512 targets.
instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,
                        Universe dummy, rFlagsReg cr)
%{
  predicate((UseAVX <=2) && ((ClearArrayNode*)n)->is_large());
  match(Set dummy (ClearArray cnt base));
  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);

  format %{ $$template
    if (UseFastStosb) {
       $$emit$$"xorq    rax, rax\t# ClearArray:\n\t"
       $$emit$$"shlq    rcx,3\t# Convert doublewords to bytes\n\t"
       $$emit$$"rep     stosb\t# Store rax to *rdi++ while rcx--"
    } else if (UseXMMForObjInit) {
       $$emit$$"mov     rdi,rax\t# ClearArray:\n\t"
       $$emit$$"vpxor   ymm0,ymm0,ymm0\n\t"
       $$emit$$"jmpq    L_zero_64_bytes\n\t"
       $$emit$$"# L_loop:\t# 64-byte LOOP\n\t"
       $$emit$$"vmovdqu ymm0,(rax)\n\t"
       $$emit$$"vmovdqu ymm0,0x20(rax)\n\t"
       $$emit$$"add     0x40,rax\n\t"
       $$emit$$"# L_zero_64_bytes:\n\t"
       $$emit$$"sub     0x8,rcx\n\t"
       $$emit$$"jge     L_loop\n\t"
       $$emit$$"add     0x4,rcx\n\t"
       $$emit$$"jl      L_tail\n\t"
       $$emit$$"vmovdqu ymm0,(rax)\n\t"
       $$emit$$"add     0x20,rax\n\t"
       $$emit$$"sub     0x4,rcx\n\t"
       $$emit$$"# L_tail:\t# Clearing tail bytes\n\t"
       $$emit$$"add     0x4,rcx\n\t"
       $$emit$$"jle     L_end\n\t"
       $$emit$$"dec     rcx\n\t"
       $$emit$$"# L_sloop:\t# 8-byte short loop\n\t"
       $$emit$$"vmovq   xmm0,(rax)\n\t"
       $$emit$$"add     0x8,rax\n\t"
       $$emit$$"dec     rcx\n\t"
       $$emit$$"jge     L_sloop\n\t"
       $$emit$$"# L_end:\n\t"
    } else {
       $$emit$$"xorq    rax, rax\t# ClearArray:\n\t"
       $$emit$$"rep     stosq\t# Store rax to *rdi++ while rcx--"
    }
  %}
  ins_encode %{
    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,
                 $tmp$$XMMRegister, true, knoreg);
  %}
  ins_pipe(pipe_slow);
%}

// Large non-constant length ClearArray for AVX512 targets.
instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,
                             Universe dummy, rFlagsReg cr)
%{
  predicate((UseAVX > 2) && ((ClearArrayNode*)n)->is_large());
  match(Set dummy (ClearArray cnt base));
  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);

  format %{ $$template
    if (UseFastStosb) {
       $$emit$$"xorq    rax, rax\t# ClearArray:\n\t"
       $$emit$$"shlq    rcx,3\t# Convert doublewords to bytes\n\t"
       $$emit$$"rep     stosb\t# Store rax to *rdi++ while rcx--"
    } else if (UseXMMForObjInit) {
       $$emit$$"mov     rdi,rax\t# ClearArray:\n\t"
       $$emit$$"vpxor   ymm0,ymm0,ymm0\n\t"
       $$emit$$"jmpq    L_zero_64_bytes\n\t"
       $$emit$$"# L_loop:\t# 64-byte LOOP\n\t"
       $$emit$$"vmovdqu ymm0,(rax)\n\t"
       $$emit$$"vmovdqu ymm0,0x20(rax)\n\t"
       $$emit$$"add     0x40,rax\n\t"
       $$emit$$"# L_zero_64_bytes:\n\t"
       $$emit$$"sub     0x8,rcx\n\t"
       $$emit$$"jge     L_loop\n\t"
       $$emit$$"add     0x4,rcx\n\t"
       $$emit$$"jl      L_tail\n\t"
       $$emit$$"vmovdqu ymm0,(rax)\n\t"
       $$emit$$"add     0x20,rax\n\t"
       $$emit$$"sub     0x4,rcx\n\t"
       $$emit$$"# L_tail:\t# Clearing tail bytes\n\t"
       $$emit$$"add     0x4,rcx\n\t"
       $$emit$$"jle     L_end\n\t"
       $$emit$$"dec     rcx\n\t"
       $$emit$$"# L_sloop:\t# 8-byte short loop\n\t"
       $$emit$$"vmovq   xmm0,(rax)\n\t"
       $$emit$$"add     0x8,rax\n\t"
       $$emit$$"dec     rcx\n\t"
       $$emit$$"jge     L_sloop\n\t"
       $$emit$$"# L_end:\n\t"
    } else {
       $$emit$$"xorq    rax, rax\t# ClearArray:\n\t"
       $$emit$$"rep     stosq\t# Store rax to *rdi++ while rcx--"
    }
  %}
  ins_encode %{
    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,
                 $tmp$$XMMRegister, true, $ktmp$$KRegister);
  %}
  ins_pipe(pipe_slow);
%}

// Small constant length ClearArray for AVX512 targets.
instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, kReg ktmp, Universe dummy, rFlagsReg cr)
%{
  predicate(!((ClearArrayNode*)n)->is_large() && (MaxVectorSize >= 32) && VM_Version::supports_avx512vl());
  match(Set dummy (ClearArray cnt base));
  ins_cost(100);
  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);
  format %{ "clear_mem_imm $base , $cnt  \n\t" %}
  ins_encode %{
   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct string_compareL(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,
                         rax_RegI result, legRegD tmp1, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);
  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));
  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);

  format %{ "String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   // KILL $tmp1" %}
  ins_encode %{
    __ string_compare($str1$$Register, $str2$$Register,
                      $cnt1$$Register, $cnt2$$Register, $result$$Register,
                      $tmp1$$XMMRegister, StrIntrinsicNode::LL, knoreg);
  %}
  ins_pipe( pipe_slow );
%}

instruct string_compareL_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,
                              rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)
%{
  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);
  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));
  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);

  format %{ "String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   // KILL $tmp1" %}
  ins_encode %{
    __ string_compare($str1$$Register, $str2$$Register,
                      $cnt1$$Register, $cnt2$$Register, $result$$Register,
                      $tmp1$$XMMRegister, StrIntrinsicNode::LL, $ktmp$$KRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct string_compareU(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,
                         rax_RegI result, legRegD tmp1, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);
  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));
  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);

  format %{ "String Compare char[] $str1,$cnt1,$str2,$cnt2 -> $result   // KILL $tmp1" %}
  ins_encode %{
    __ string_compare($str1$$Register, $str2$$Register,
                      $cnt1$$Register, $cnt2$$Register, $result$$Register,
                      $tmp1$$XMMRegister, StrIntrinsicNode::UU, knoreg);
  %}
  ins_pipe( pipe_slow );
%}

instruct string_compareU_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,
                              rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)
%{
  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);
  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));
  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);

  format %{ "String Compare char[] $str1,$cnt1,$str2,$cnt2 -> $result   // KILL $tmp1" %}
  ins_encode %{
    __ string_compare($str1$$Register, $str2$$Register,
                      $cnt1$$Register, $cnt2$$Register, $result$$Register,
                      $tmp1$$XMMRegister, StrIntrinsicNode::UU, $ktmp$$KRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct string_compareLU(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,
                          rax_RegI result, legRegD tmp1, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);
  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));
  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);

  format %{ "String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   // KILL $tmp1" %}
  ins_encode %{
    __ string_compare($str1$$Register, $str2$$Register,
                      $cnt1$$Register, $cnt2$$Register, $result$$Register,
                      $tmp1$$XMMRegister, StrIntrinsicNode::LU, knoreg);
  %}
  ins_pipe( pipe_slow );
%}

instruct string_compareLU_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,
                               rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)
%{
  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);
  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));
  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);

  format %{ "String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   // KILL $tmp1" %}
  ins_encode %{
    __ string_compare($str1$$Register, $str2$$Register,
                      $cnt1$$Register, $cnt2$$Register, $result$$Register,
                      $tmp1$$XMMRegister, StrIntrinsicNode::LU, $ktmp$$KRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct string_compareUL(rsi_RegP str1, rdx_RegI cnt1, rdi_RegP str2, rcx_RegI cnt2,
                          rax_RegI result, legRegD tmp1, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);
  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));
  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);

  format %{ "String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   // KILL $tmp1" %}
  ins_encode %{
    __ string_compare($str2$$Register, $str1$$Register,
                      $cnt2$$Register, $cnt1$$Register, $result$$Register,
                      $tmp1$$XMMRegister, StrIntrinsicNode::UL, knoreg);
  %}
  ins_pipe( pipe_slow );
%}

instruct string_compareUL_evex(rsi_RegP str1, rdx_RegI cnt1, rdi_RegP str2, rcx_RegI cnt2,
                               rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)
%{
  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);
  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));
  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);

  format %{ "String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   // KILL $tmp1" %}
  ins_encode %{
    __ string_compare($str2$$Register, $str1$$Register,
                      $cnt2$$Register, $cnt1$$Register, $result$$Register,
                      $tmp1$$XMMRegister, StrIntrinsicNode::UL, $ktmp$$KRegister);
  %}
  ins_pipe( pipe_slow );
%}

// fast search of substring with known size.
instruct string_indexof_conL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, immI int_cnt2,
                             rbx_RegI result, legRegD tmp_vec, rax_RegI cnt2, rcx_RegI tmp, rFlagsReg cr)
%{
  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::LL));
  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));
  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);

  format %{ "String IndexOf byte[] $str1,$cnt1,$str2,$int_cnt2 -> $result   // KILL $tmp_vec, $cnt1, $cnt2, $tmp" %}
  ins_encode %{
    int icnt2 = (int)$int_cnt2$$constant;
    if (icnt2 >= 16) {
      // IndexOf for constant substrings with size >= 16 elements
      // which don't need to be loaded through stack.
      __ string_indexofC8($str1$$Register, $str2$$Register,
                          $cnt1$$Register, $cnt2$$Register,
                          icnt2, $result$$Register,
                          $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);
    } else {
      // Small strings are loaded through stack if they cross page boundary.
      __ string_indexof($str1$$Register, $str2$$Register,
                        $cnt1$$Register, $cnt2$$Register,
                        icnt2, $result$$Register,
                        $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);
    }
  %}
  ins_pipe( pipe_slow );
%}

// fast search of substring with known size.
instruct string_indexof_conU(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, immI int_cnt2,
                             rbx_RegI result, legRegD tmp_vec, rax_RegI cnt2, rcx_RegI tmp, rFlagsReg cr)
%{
  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UU));
  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));
  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);

  format %{ "String IndexOf char[] $str1,$cnt1,$str2,$int_cnt2 -> $result   // KILL $tmp_vec, $cnt1, $cnt2, $tmp" %}
  ins_encode %{
    int icnt2 = (int)$int_cnt2$$constant;
    if (icnt2 >= 8) {
      // IndexOf for constant substrings with size >= 8 elements
      // which don't need to be loaded through stack.
      __ string_indexofC8($str1$$Register, $str2$$Register,
                          $cnt1$$Register, $cnt2$$Register,
                          icnt2, $result$$Register,
                          $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);
    } else {
      // Small strings are loaded through stack if they cross page boundary.
      __ string_indexof($str1$$Register, $str2$$Register,
                        $cnt1$$Register, $cnt2$$Register,
                        icnt2, $result$$Register,
                        $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);
    }
  %}
  ins_pipe( pipe_slow );
%}

// fast search of substring with known size.
instruct string_indexof_conUL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, immI int_cnt2,
                              rbx_RegI result, legRegD tmp_vec, rax_RegI cnt2, rcx_RegI tmp, rFlagsReg cr)
%{
  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UL));
  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));
  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);

  format %{ "String IndexOf char[] $str1,$cnt1,$str2,$int_cnt2 -> $result   // KILL $tmp_vec, $cnt1, $cnt2, $tmp" %}
  ins_encode %{
    int icnt2 = (int)$int_cnt2$$constant;
    if (icnt2 >= 8) {
      // IndexOf for constant substrings with size >= 8 elements
      // which don't need to be loaded through stack.
      __ string_indexofC8($str1$$Register, $str2$$Register,
                          $cnt1$$Register, $cnt2$$Register,
                          icnt2, $result$$Register,
                          $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);
    } else {
      // Small strings are loaded through stack if they cross page boundary.
      __ string_indexof($str1$$Register, $str2$$Register,
                        $cnt1$$Register, $cnt2$$Register,
                        icnt2, $result$$Register,
                        $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct string_indexofL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, rax_RegI cnt2,
                         rbx_RegI result, legRegD tmp_vec, rcx_RegI tmp, rFlagsReg cr)
%{
  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::LL));
  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));
  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);

  format %{ "String IndexOf byte[] $str1,$cnt1,$str2,$cnt2 -> $result   // KILL all" %}
  ins_encode %{
    __ string_indexof($str1$$Register, $str2$$Register,
                      $cnt1$$Register, $cnt2$$Register,
                      (-1), $result$$Register,
                      $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);
  %}
  ins_pipe( pipe_slow );
%}

instruct string_indexofU(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, rax_RegI cnt2,
                         rbx_RegI result, legRegD tmp_vec, rcx_RegI tmp, rFlagsReg cr)
%{
  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UU));
  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));
  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);

  format %{ "String IndexOf char[] $str1,$cnt1,$str2,$cnt2 -> $result   // KILL all" %}
  ins_encode %{
    __ string_indexof($str1$$Register, $str2$$Register,
                      $cnt1$$Register, $cnt2$$Register,
                      (-1), $result$$Register,
                      $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);
  %}
  ins_pipe( pipe_slow );
%}

instruct string_indexofUL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, rax_RegI cnt2,
                          rbx_RegI result, legRegD tmp_vec, rcx_RegI tmp, rFlagsReg cr)
%{
  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UL));
  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));
  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);

  format %{ "String IndexOf char[] $str1,$cnt1,$str2,$cnt2 -> $result   // KILL all" %}
  ins_encode %{
    __ string_indexof($str1$$Register, $str2$$Register,
                      $cnt1$$Register, $cnt2$$Register,
                      (-1), $result$$Register,
                      $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);
  %}
  ins_pipe( pipe_slow );
%}

instruct string_indexof_char(rdi_RegP str1, rdx_RegI cnt1, rax_RegI ch,
                              rbx_RegI result, legRegD tmp_vec1, legRegD tmp_vec2, legRegD tmp_vec3, rcx_RegI tmp, rFlagsReg cr)
%{
  predicate(UseSSE42Intrinsics && (((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::U));
  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));
  effect(TEMP tmp_vec1, TEMP tmp_vec2, TEMP tmp_vec3, USE_KILL str1, USE_KILL cnt1, USE_KILL ch, TEMP tmp, KILL cr);
  format %{ "StringUTF16 IndexOf char[] $str1,$cnt1,$ch -> $result   // KILL all" %}
  ins_encode %{
    __ string_indexof_char($str1$$Register, $cnt1$$Register, $ch$$Register, $result$$Register,
                           $tmp_vec1$$XMMRegister, $tmp_vec2$$XMMRegister, $tmp_vec3$$XMMRegister, $tmp$$Register);
  %}
  ins_pipe( pipe_slow );
%}

instruct stringL_indexof_char(rdi_RegP str1, rdx_RegI cnt1, rax_RegI ch,
                              rbx_RegI result, legRegD tmp_vec1, legRegD tmp_vec2, legRegD tmp_vec3, rcx_RegI tmp, rFlagsReg cr)
%{
  predicate(UseSSE42Intrinsics && (((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::L));
  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));
  effect(TEMP tmp_vec1, TEMP tmp_vec2, TEMP tmp_vec3, USE_KILL str1, USE_KILL cnt1, USE_KILL ch, TEMP tmp, KILL cr);
  format %{ "StringLatin1 IndexOf char[] $str1,$cnt1,$ch -> $result   // KILL all" %}
  ins_encode %{
    __ stringL_indexof_char($str1$$Register, $cnt1$$Register, $ch$$Register, $result$$Register,
                           $tmp_vec1$$XMMRegister, $tmp_vec2$$XMMRegister, $tmp_vec3$$XMMRegister, $tmp$$Register);
  %}
  ins_pipe( pipe_slow );
%}

// fast string equals
instruct string_equals(rdi_RegP str1, rsi_RegP str2, rcx_RegI cnt, rax_RegI result,
                       legRegD tmp1, legRegD tmp2, rbx_RegI tmp3, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_avx512vlbw());
  match(Set result (StrEquals (Binary str1 str2) cnt));
  effect(TEMP tmp1, TEMP tmp2, USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL tmp3, KILL cr);

  format %{ "String Equals $str1,$str2,$cnt -> $result    // KILL $tmp1, $tmp2, $tmp3" %}
  ins_encode %{
    __ arrays_equals(false, $str1$$Register, $str2$$Register,
                     $cnt$$Register, $result$$Register, $tmp3$$Register,
                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false /* char */, knoreg);
  %}
  ins_pipe( pipe_slow );
%}

instruct string_equals_evex(rdi_RegP str1, rsi_RegP str2, rcx_RegI cnt, rax_RegI result,
                           legRegD tmp1, legRegD tmp2, kReg ktmp, rbx_RegI tmp3, rFlagsReg cr)
%{
  predicate(VM_Version::supports_avx512vlbw());
  match(Set result (StrEquals (Binary str1 str2) cnt));
  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL tmp3, KILL cr);

  format %{ "String Equals $str1,$str2,$cnt -> $result    // KILL $tmp1, $tmp2, $tmp3" %}
  ins_encode %{
    __ arrays_equals(false, $str1$$Register, $str2$$Register,
                     $cnt$$Register, $result$$Register, $tmp3$$Register,
                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false /* char */, $ktmp$$KRegister);
  %}
  ins_pipe( pipe_slow );
%}

// fast array equals
instruct array_equalsB(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,
                       legRegD tmp1, legRegD tmp2, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);
  match(Set result (AryEq ary1 ary2));
  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);

  format %{ "Array Equals byte[] $ary1,$ary2 -> $result   // KILL $tmp1, $tmp2, $tmp3, $tmp4" %}
  ins_encode %{
    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,
                     $tmp3$$Register, $result$$Register, $tmp4$$Register,
                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false /* char */, knoreg);
  %}
  ins_pipe( pipe_slow );
%}

instruct array_equalsB_evex(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,
                            legRegD tmp1, legRegD tmp2, kReg ktmp, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)
%{
  predicate(VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);
  match(Set result (AryEq ary1 ary2));
  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);

  format %{ "Array Equals byte[] $ary1,$ary2 -> $result   // KILL $tmp1, $tmp2, $tmp3, $tmp4" %}
  ins_encode %{
    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,
                     $tmp3$$Register, $result$$Register, $tmp4$$Register,
                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false /* char */, $ktmp$$KRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct array_equalsC(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,
                       legRegD tmp1, legRegD tmp2, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)
%{
  predicate(!VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);
  match(Set result (AryEq ary1 ary2));
  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);

  format %{ "Array Equals char[] $ary1,$ary2 -> $result   // KILL $tmp1, $tmp2, $tmp3, $tmp4" %}
  ins_encode %{
    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,
                     $tmp3$$Register, $result$$Register, $tmp4$$Register,
                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true /* char */, knoreg);
  %}
  ins_pipe( pipe_slow );
%}

instruct array_equalsC_evex(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,
                            legRegD tmp1, legRegD tmp2, kReg ktmp, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)
%{
  predicate(VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);
  match(Set result (AryEq ary1 ary2));
  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);

  format %{ "Array Equals char[] $ary1,$ary2 -> $result   // KILL $tmp1, $tmp2, $tmp3, $tmp4" %}
  ins_encode %{
    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,
                     $tmp3$$Register, $result$$Register, $tmp4$$Register,
                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true /* char */, $ktmp$$KRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct arrays_hashcode(rdi_RegP ary1, rdx_RegI cnt1, rbx_RegI result, immU8 basic_type,
                         legRegD tmp_vec1, legRegD tmp_vec2, legRegD tmp_vec3, legRegD tmp_vec4,
                         legRegD tmp_vec5, legRegD tmp_vec6, legRegD tmp_vec7, legRegD tmp_vec8,
                         legRegD tmp_vec9, legRegD tmp_vec10, legRegD tmp_vec11, legRegD tmp_vec12,
                         legRegD tmp_vec13, rRegI tmp1, rRegI tmp2, rRegI tmp3, rFlagsReg cr)
%{
  predicate(UseAVX >= 2);
  match(Set result (VectorizedHashCode (Binary ary1 cnt1) (Binary result basic_type)));
  effect(TEMP tmp_vec1, TEMP tmp_vec2, TEMP tmp_vec3, TEMP tmp_vec4, TEMP tmp_vec5, TEMP tmp_vec6,
         TEMP tmp_vec7, TEMP tmp_vec8, TEMP tmp_vec9, TEMP tmp_vec10, TEMP tmp_vec11, TEMP tmp_vec12,
         TEMP tmp_vec13, TEMP tmp1, TEMP tmp2, TEMP tmp3, USE_KILL ary1, USE_KILL cnt1,
         USE basic_type, KILL cr);

  format %{ "Array HashCode array[] $ary1,$cnt1,$result,$basic_type -> $result   // KILL all" %}
  ins_encode %{
    __ arrays_hashcode($ary1$$Register, $cnt1$$Register, $result$$Register,
                       $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,
                       $tmp_vec1$$XMMRegister, $tmp_vec2$$XMMRegister, $tmp_vec3$$XMMRegister,
                       $tmp_vec4$$XMMRegister, $tmp_vec5$$XMMRegister, $tmp_vec6$$XMMRegister,
                       $tmp_vec7$$XMMRegister, $tmp_vec8$$XMMRegister, $tmp_vec9$$XMMRegister,
                       $tmp_vec10$$XMMRegister, $tmp_vec11$$XMMRegister, $tmp_vec12$$XMMRegister,
                       $tmp_vec13$$XMMRegister, (BasicType)$basic_type$$constant);
  %}
  ins_pipe( pipe_slow );
%}

instruct count_positives(rsi_RegP ary1, rcx_RegI len, rax_RegI result,
                         legRegD tmp1, legRegD tmp2, rbx_RegI tmp3, rFlagsReg cr,)
%{
  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());
  match(Set result (CountPositives ary1 len));
  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL len, KILL tmp3, KILL cr);

  format %{ "countPositives byte[] $ary1,$len -> $result   // KILL $tmp1, $tmp2, $tmp3" %}
  ins_encode %{
    __ count_positives($ary1$$Register, $len$$Register,
                       $result$$Register, $tmp3$$Register,
                       $tmp1$$XMMRegister, $tmp2$$XMMRegister, knoreg, knoreg);
  %}
  ins_pipe( pipe_slow );
%}

instruct count_positives_evex(rsi_RegP ary1, rcx_RegI len, rax_RegI result,
                              legRegD tmp1, legRegD tmp2, kReg ktmp1, kReg ktmp2, rbx_RegI tmp3, rFlagsReg cr,)
%{
  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());
  match(Set result (CountPositives ary1 len));
  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp1, TEMP ktmp2, USE_KILL ary1, USE_KILL len, KILL tmp3, KILL cr);

  format %{ "countPositives byte[] $ary1,$len -> $result   // KILL $tmp1, $tmp2, $tmp3" %}
  ins_encode %{
    __ count_positives($ary1$$Register, $len$$Register,
                       $result$$Register, $tmp3$$Register,
                       $tmp1$$XMMRegister, $tmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister);
  %}
  ins_pipe( pipe_slow );
%}

// fast char[] to byte[] compression
instruct string_compress(rsi_RegP src, rdi_RegP dst, rdx_RegI len, legRegD tmp1, legRegD tmp2, legRegD tmp3,
                         legRegD tmp4, rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());
  match(Set result (StrCompressedCopy src (Binary dst len)));
  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst,
         USE_KILL len, KILL tmp5, KILL cr);

  format %{ "String Compress $src,$dst -> $result    // KILL RAX, RCX, RDX" %}
  ins_encode %{
    __ char_array_compress($src$$Register, $dst$$Register, $len$$Register,
                           $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,
                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register,
                           knoreg, knoreg);
  %}
  ins_pipe( pipe_slow );
%}

instruct string_compress_evex(rsi_RegP src, rdi_RegP dst, rdx_RegI len, legRegD tmp1, legRegD tmp2, legRegD tmp3,
                              legRegD tmp4, kReg ktmp1, kReg ktmp2, rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{
  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());
  match(Set result (StrCompressedCopy src (Binary dst len)));
  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP ktmp1, TEMP ktmp2, USE_KILL src, USE_KILL dst,
         USE_KILL len, KILL tmp5, KILL cr);

  format %{ "String Compress $src,$dst -> $result    // KILL RAX, RCX, RDX" %}
  ins_encode %{
    __ char_array_compress($src$$Register, $dst$$Register, $len$$Register,
                           $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,
                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register,
                           $ktmp1$$KRegister, $ktmp2$$KRegister);
  %}
  ins_pipe( pipe_slow );
%}
// fast byte[] to char[] inflation
instruct string_inflate(Universe dummy, rsi_RegP src, rdi_RegP dst, rdx_RegI len,
                        legRegD tmp1, rcx_RegI tmp2, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());
  match(Set dummy (StrInflatedCopy src (Binary dst len)));
  effect(TEMP tmp1, TEMP tmp2, USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);

  format %{ "String Inflate $src,$dst    // KILL $tmp1, $tmp2" %}
  ins_encode %{
    __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,
                          $tmp1$$XMMRegister, $tmp2$$Register, knoreg);
  %}
  ins_pipe( pipe_slow );
%}

instruct string_inflate_evex(Universe dummy, rsi_RegP src, rdi_RegP dst, rdx_RegI len,
                             legRegD tmp1, kReg ktmp, rcx_RegI tmp2, rFlagsReg cr) %{
  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());
  match(Set dummy (StrInflatedCopy src (Binary dst len)));
  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);

  format %{ "String Inflate $src,$dst    // KILL $tmp1, $tmp2" %}
  ins_encode %{
    __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,
                          $tmp1$$XMMRegister, $tmp2$$Register, $ktmp$$KRegister);
  %}
  ins_pipe( pipe_slow );
%}

// encode char[] to byte[] in ISO_8859_1
instruct encode_iso_array(rsi_RegP src, rdi_RegP dst, rdx_RegI len,
                          legRegD tmp1, legRegD tmp2, legRegD tmp3, legRegD tmp4,
                          rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{
  predicate(!((EncodeISOArrayNode*)n)->is_ascii());
  match(Set result (EncodeISOArray src (Binary dst len)));
  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst, USE_KILL len, KILL tmp5, KILL cr);

  format %{ "Encode iso array $src,$dst,$len -> $result    // KILL RCX, RDX, $tmp1, $tmp2, $tmp3, $tmp4, RSI, RDI " %}
  ins_encode %{
    __ encode_iso_array($src$$Register, $dst$$Register, $len$$Register,
                        $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,
                        $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register, false);
  %}
  ins_pipe( pipe_slow );
%}

// encode char[] to byte[] in ASCII
instruct encode_ascii_array(rsi_RegP src, rdi_RegP dst, rdx_RegI len,
                            legRegD tmp1, legRegD tmp2, legRegD tmp3, legRegD tmp4,
                            rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{
  predicate(((EncodeISOArrayNode*)n)->is_ascii());
  match(Set result (EncodeISOArray src (Binary dst len)));
  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst, USE_KILL len, KILL tmp5, KILL cr);

  format %{ "Encode ascii array $src,$dst,$len -> $result    // KILL RCX, RDX, $tmp1, $tmp2, $tmp3, $tmp4, RSI, RDI " %}
  ins_encode %{
    __ encode_iso_array($src$$Register, $dst$$Register, $len$$Register,
                        $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,
                        $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register, true);
  %}
  ins_pipe( pipe_slow );
%}

//----------Overflow Math Instructions-----------------------------------------

instruct overflowAddI_rReg(rFlagsReg cr, rax_RegI op1, rRegI op2)
%{
  match(Set cr (OverflowAddI op1 op2));
  effect(DEF cr, USE_KILL op1, USE op2);

  format %{ "addl    $op1, $op2\t# overflow check int" %}

  ins_encode %{
    __ addl($op1$$Register, $op2$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct overflowAddI_rReg_imm(rFlagsReg cr, rax_RegI op1, immI op2)
%{
  match(Set cr (OverflowAddI op1 op2));
  effect(DEF cr, USE_KILL op1, USE op2);

  format %{ "addl    $op1, $op2\t# overflow check int" %}

  ins_encode %{
    __ addl($op1$$Register, $op2$$constant);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct overflowAddL_rReg(rFlagsReg cr, rax_RegL op1, rRegL op2)
%{
  match(Set cr (OverflowAddL op1 op2));
  effect(DEF cr, USE_KILL op1, USE op2);

  format %{ "addq    $op1, $op2\t# overflow check long" %}
  ins_encode %{
    __ addq($op1$$Register, $op2$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct overflowAddL_rReg_imm(rFlagsReg cr, rax_RegL op1, immL32 op2)
%{
  match(Set cr (OverflowAddL op1 op2));
  effect(DEF cr, USE_KILL op1, USE op2);

  format %{ "addq    $op1, $op2\t# overflow check long" %}
  ins_encode %{
    __ addq($op1$$Register, $op2$$constant);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct overflowSubI_rReg(rFlagsReg cr, rRegI op1, rRegI op2)
%{
  match(Set cr (OverflowSubI op1 op2));

  format %{ "cmpl    $op1, $op2\t# overflow check int" %}
  ins_encode %{
    __ cmpl($op1$$Register, $op2$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct overflowSubI_rReg_imm(rFlagsReg cr, rRegI op1, immI op2)
%{
  match(Set cr (OverflowSubI op1 op2));

  format %{ "cmpl    $op1, $op2\t# overflow check int" %}
  ins_encode %{
    __ cmpl($op1$$Register, $op2$$constant);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct overflowSubL_rReg(rFlagsReg cr, rRegL op1, rRegL op2)
%{
  match(Set cr (OverflowSubL op1 op2));

  format %{ "cmpq    $op1, $op2\t# overflow check long" %}
  ins_encode %{
    __ cmpq($op1$$Register, $op2$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct overflowSubL_rReg_imm(rFlagsReg cr, rRegL op1, immL32 op2)
%{
  match(Set cr (OverflowSubL op1 op2));

  format %{ "cmpq    $op1, $op2\t# overflow check long" %}
  ins_encode %{
    __ cmpq($op1$$Register, $op2$$constant);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct overflowNegI_rReg(rFlagsReg cr, immI_0 zero, rax_RegI op2)
%{
  match(Set cr (OverflowSubI zero op2));
  effect(DEF cr, USE_KILL op2);

  format %{ "negl    $op2\t# overflow check int" %}
  ins_encode %{
    __ negl($op2$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct overflowNegL_rReg(rFlagsReg cr, immL0 zero, rax_RegL op2)
%{
  match(Set cr (OverflowSubL zero op2));
  effect(DEF cr, USE_KILL op2);

  format %{ "negq    $op2\t# overflow check long" %}
  ins_encode %{
    __ negq($op2$$Register);
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct overflowMulI_rReg(rFlagsReg cr, rax_RegI op1, rRegI op2)
%{
  match(Set cr (OverflowMulI op1 op2));
  effect(DEF cr, USE_KILL op1, USE op2);

  format %{ "imull    $op1, $op2\t# overflow check int" %}
  ins_encode %{
    __ imull($op1$$Register, $op2$$Register);
  %}
  ins_pipe(ialu_reg_reg_alu0);
%}

instruct overflowMulI_rReg_imm(rFlagsReg cr, rRegI op1, immI op2, rRegI tmp)
%{
  match(Set cr (OverflowMulI op1 op2));
  effect(DEF cr, TEMP tmp, USE op1, USE op2);

  format %{ "imull    $tmp, $op1, $op2\t# overflow check int" %}
  ins_encode %{
    __ imull($tmp$$Register, $op1$$Register, $op2$$constant);
  %}
  ins_pipe(ialu_reg_reg_alu0);
%}

instruct overflowMulL_rReg(rFlagsReg cr, rax_RegL op1, rRegL op2)
%{
  match(Set cr (OverflowMulL op1 op2));
  effect(DEF cr, USE_KILL op1, USE op2);

  format %{ "imulq    $op1, $op2\t# overflow check long" %}
  ins_encode %{
    __ imulq($op1$$Register, $op2$$Register);
  %}
  ins_pipe(ialu_reg_reg_alu0);
%}

instruct overflowMulL_rReg_imm(rFlagsReg cr, rRegL op1, immL32 op2, rRegL tmp)
%{
  match(Set cr (OverflowMulL op1 op2));
  effect(DEF cr, TEMP tmp, USE op1, USE op2);

  format %{ "imulq    $tmp, $op1, $op2\t# overflow check long" %}
  ins_encode %{
    __ imulq($tmp$$Register, $op1$$Register, $op2$$constant);
  %}
  ins_pipe(ialu_reg_reg_alu0);
%}


//----------Control Flow Instructions------------------------------------------
// Signed compare Instructions

// XXX more variants!!
instruct compI_rReg(rFlagsReg cr, rRegI op1, rRegI op2)
%{
  match(Set cr (CmpI op1 op2));
  effect(DEF cr, USE op1, USE op2);

  format %{ "cmpl    $op1, $op2" %}
  ins_encode %{
    __ cmpl($op1$$Register, $op2$$Register);
  %}
  ins_pipe(ialu_cr_reg_reg);
%}

instruct compI_rReg_imm(rFlagsReg cr, rRegI op1, immI op2)
%{
  match(Set cr (CmpI op1 op2));

  format %{ "cmpl    $op1, $op2" %}
  ins_encode %{
    __ cmpl($op1$$Register, $op2$$constant);
  %}
  ins_pipe(ialu_cr_reg_imm);
%}

instruct compI_rReg_mem(rFlagsReg cr, rRegI op1, memory op2)
%{
  match(Set cr (CmpI op1 (LoadI op2)));

  ins_cost(500); // XXX
  format %{ "cmpl    $op1, $op2" %}
  ins_encode %{
    __ cmpl($op1$$Register, $op2$$Address);
  %}
  ins_pipe(ialu_cr_reg_mem);
%}

instruct testI_reg(rFlagsReg cr, rRegI src, immI_0 zero)
%{
  match(Set cr (CmpI src zero));

  format %{ "testl   $src, $src" %}
  ins_encode %{
    __ testl($src$$Register, $src$$Register);
  %}
  ins_pipe(ialu_cr_reg_imm);
%}

instruct testI_reg_imm(rFlagsReg cr, rRegI src, immI con, immI_0 zero)
%{
  match(Set cr (CmpI (AndI src con) zero));

  format %{ "testl   $src, $con" %}
  ins_encode %{
    __ testl($src$$Register, $con$$constant);
  %}
  ins_pipe(ialu_cr_reg_imm);
%}

instruct testI_reg_reg(rFlagsReg cr, rRegI src1, rRegI src2, immI_0 zero)
%{
  match(Set cr (CmpI (AndI src1 src2) zero));

  format %{ "testl   $src1, $src2" %}
  ins_encode %{
    __ testl($src1$$Register, $src2$$Register);
  %}
  ins_pipe(ialu_cr_reg_imm);
%}

instruct testI_reg_mem(rFlagsReg cr, rRegI src, memory mem, immI_0 zero)
%{
  match(Set cr (CmpI (AndI src (LoadI mem)) zero));

  format %{ "testl   $src, $mem" %}
  ins_encode %{
    __ testl($src$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_cr_reg_mem);
%}

// Unsigned compare Instructions; really, same as signed except they
// produce an rFlagsRegU instead of rFlagsReg.
instruct compU_rReg(rFlagsRegU cr, rRegI op1, rRegI op2)
%{
  match(Set cr (CmpU op1 op2));

  format %{ "cmpl    $op1, $op2\t# unsigned" %}
  ins_encode %{
    __ cmpl($op1$$Register, $op2$$Register);
  %}
  ins_pipe(ialu_cr_reg_reg);
%}

instruct compU_rReg_imm(rFlagsRegU cr, rRegI op1, immI op2)
%{
  match(Set cr (CmpU op1 op2));

  format %{ "cmpl    $op1, $op2\t# unsigned" %}
  ins_encode %{
    __ cmpl($op1$$Register, $op2$$constant);
  %}
  ins_pipe(ialu_cr_reg_imm);
%}

instruct compU_rReg_mem(rFlagsRegU cr, rRegI op1, memory op2)
%{
  match(Set cr (CmpU op1 (LoadI op2)));

  ins_cost(500); // XXX
  format %{ "cmpl    $op1, $op2\t# unsigned" %}
  ins_encode %{
    __ cmpl($op1$$Register, $op2$$Address);
  %}
  ins_pipe(ialu_cr_reg_mem);
%}

instruct testU_reg(rFlagsRegU cr, rRegI src, immI_0 zero)
%{
  match(Set cr (CmpU src zero));

  format %{ "testl   $src, $src\t# unsigned" %}
  ins_encode %{
    __ testl($src$$Register, $src$$Register);
  %}
  ins_pipe(ialu_cr_reg_imm);
%}

instruct compP_rReg(rFlagsRegU cr, rRegP op1, rRegP op2)
%{
  match(Set cr (CmpP op1 op2));

  format %{ "cmpq    $op1, $op2\t# ptr" %}
  ins_encode %{
    __ cmpq($op1$$Register, $op2$$Register);
  %}
  ins_pipe(ialu_cr_reg_reg);
%}

instruct compP_rReg_mem(rFlagsRegU cr, rRegP op1, memory op2)
%{
  match(Set cr (CmpP op1 (LoadP op2)));
  predicate(n->in(2)->as_Load()->barrier_data() == 0);

  ins_cost(500); // XXX
  format %{ "cmpq    $op1, $op2\t# ptr" %}
  ins_encode %{
    __ cmpq($op1$$Register, $op2$$Address);
  %}
  ins_pipe(ialu_cr_reg_mem);
%}

// XXX this is generalized by compP_rReg_mem???
// Compare raw pointer (used in out-of-heap check).
// Only works because non-oop pointers must be raw pointers
// and raw pointers have no anti-dependencies.
instruct compP_mem_rReg(rFlagsRegU cr, rRegP op1, memory op2)
%{
  predicate(n->in(2)->in(2)->bottom_type()->reloc() == relocInfo::none &&
            n->in(2)->as_Load()->barrier_data() == 0);
  match(Set cr (CmpP op1 (LoadP op2)));

  format %{ "cmpq    $op1, $op2\t# raw ptr" %}
  ins_encode %{
    __ cmpq($op1$$Register, $op2$$Address);
  %}
  ins_pipe(ialu_cr_reg_mem);
%}

// This will generate a signed flags result. This should be OK since
// any compare to a zero should be eq/neq.
instruct testP_reg(rFlagsReg cr, rRegP src, immP0 zero)
%{
  match(Set cr (CmpP src zero));

  format %{ "testq   $src, $src\t# ptr" %}
  ins_encode %{
    __ testq($src$$Register, $src$$Register);
  %}
  ins_pipe(ialu_cr_reg_imm);
%}

// This will generate a signed flags result. This should be OK since
// any compare to a zero should be eq/neq.
instruct testP_mem(rFlagsReg cr, memory op, immP0 zero)
%{
  predicate((!UseCompressedOops || (CompressedOops::base() != nullptr)) &&
            n->in(1)->as_Load()->barrier_data() == 0);
  match(Set cr (CmpP (LoadP op) zero));

  ins_cost(500); // XXX
  format %{ "testq   $op, 0xffffffffffffffff\t# ptr" %}
  ins_encode %{
    __ testq($op$$Address, 0xFFFFFFFF);
  %}
  ins_pipe(ialu_cr_reg_imm);
%}

instruct testP_mem_reg0(rFlagsReg cr, memory mem, immP0 zero)
%{
  predicate(UseCompressedOops && (CompressedOops::base() == nullptr) &&
            n->in(1)->as_Load()->barrier_data() == 0);
  match(Set cr (CmpP (LoadP mem) zero));

  format %{ "cmpq    R12, $mem\t# ptr (R12_heapbase==0)" %}
  ins_encode %{
    __ cmpq(r12, $mem$$Address);
  %}
  ins_pipe(ialu_cr_reg_mem);
%}

instruct compN_rReg(rFlagsRegU cr, rRegN op1, rRegN op2)
%{
  match(Set cr (CmpN op1 op2));

  format %{ "cmpl    $op1, $op2\t# compressed ptr" %}
  ins_encode %{ __ cmpl($op1$$Register, $op2$$Register); %}
  ins_pipe(ialu_cr_reg_reg);
%}

instruct compN_rReg_mem(rFlagsRegU cr, rRegN src, memory mem)
%{
  predicate(n->in(2)->as_Load()->barrier_data() == 0);
  match(Set cr (CmpN src (LoadN mem)));

  format %{ "cmpl    $src, $mem\t# compressed ptr" %}
  ins_encode %{
    __ cmpl($src$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_cr_reg_mem);
%}

instruct compN_rReg_imm(rFlagsRegU cr, rRegN op1, immN op2) %{
  match(Set cr (CmpN op1 op2));

  format %{ "cmpl    $op1, $op2\t# compressed ptr" %}
  ins_encode %{
    __ cmp_narrow_oop($op1$$Register, (jobject)$op2$$constant);
  %}
  ins_pipe(ialu_cr_reg_imm);
%}

instruct compN_mem_imm(rFlagsRegU cr, memory mem, immN src)
%{
  predicate(n->in(2)->as_Load()->barrier_data() == 0);
  match(Set cr (CmpN src (LoadN mem)));

  format %{ "cmpl    $mem, $src\t# compressed ptr" %}
  ins_encode %{
    __ cmp_narrow_oop($mem$$Address, (jobject)$src$$constant);
  %}
  ins_pipe(ialu_cr_reg_mem);
%}

instruct compN_rReg_imm_klass(rFlagsRegU cr, rRegN op1, immNKlass op2) %{
  match(Set cr (CmpN op1 op2));

  format %{ "cmpl    $op1, $op2\t# compressed klass ptr" %}
  ins_encode %{
    __ cmp_narrow_klass($op1$$Register, (Klass*)$op2$$constant);
  %}
  ins_pipe(ialu_cr_reg_imm);
%}

instruct compN_mem_imm_klass(rFlagsRegU cr, memory mem, immNKlass src)
%{
  predicate(!UseCompactObjectHeaders);
  match(Set cr (CmpN src (LoadNKlass mem)));

  format %{ "cmpl    $mem, $src\t# compressed klass ptr" %}
  ins_encode %{
    __ cmp_narrow_klass($mem$$Address, (Klass*)$src$$constant);
  %}
  ins_pipe(ialu_cr_reg_mem);
%}

instruct testN_reg(rFlagsReg cr, rRegN src, immN0 zero) %{
  match(Set cr (CmpN src zero));

  format %{ "testl   $src, $src\t# compressed ptr" %}
  ins_encode %{ __ testl($src$$Register, $src$$Register); %}
  ins_pipe(ialu_cr_reg_imm);
%}

instruct testN_mem(rFlagsReg cr, memory mem, immN0 zero)
%{
  predicate(CompressedOops::base() != nullptr &&
            n->in(1)->as_Load()->barrier_data() == 0);
  match(Set cr (CmpN (LoadN mem) zero));

  ins_cost(500); // XXX
  format %{ "testl   $mem, 0xffffffff\t# compressed ptr" %}
  ins_encode %{
    __ cmpl($mem$$Address, (int)0xFFFFFFFF);
  %}
  ins_pipe(ialu_cr_reg_mem);
%}

instruct testN_mem_reg0(rFlagsReg cr, memory mem, immN0 zero)
%{
  predicate(CompressedOops::base() == nullptr &&
            n->in(1)->as_Load()->barrier_data() == 0);
  match(Set cr (CmpN (LoadN mem) zero));

  format %{ "cmpl    R12, $mem\t# compressed ptr (R12_heapbase==0)" %}
  ins_encode %{
    __ cmpl(r12, $mem$$Address);
  %}
  ins_pipe(ialu_cr_reg_mem);
%}

// Yanked all unsigned pointer compare operations.
// Pointer compares are done with CmpP which is already unsigned.

instruct compL_rReg(rFlagsReg cr, rRegL op1, rRegL op2)
%{
  match(Set cr (CmpL op1 op2));

  format %{ "cmpq    $op1, $op2" %}
  ins_encode %{
    __ cmpq($op1$$Register, $op2$$Register);
  %}
  ins_pipe(ialu_cr_reg_reg);
%}

instruct compL_rReg_imm(rFlagsReg cr, rRegL op1, immL32 op2)
%{
  match(Set cr (CmpL op1 op2));

  format %{ "cmpq    $op1, $op2" %}
  ins_encode %{
    __ cmpq($op1$$Register, $op2$$constant);
  %}
  ins_pipe(ialu_cr_reg_imm);
%}

instruct compL_rReg_mem(rFlagsReg cr, rRegL op1, memory op2)
%{
  match(Set cr (CmpL op1 (LoadL op2)));

  format %{ "cmpq    $op1, $op2" %}
  ins_encode %{
    __ cmpq($op1$$Register, $op2$$Address);
  %}
  ins_pipe(ialu_cr_reg_mem);
%}

instruct testL_reg(rFlagsReg cr, rRegL src, immL0 zero)
%{
  match(Set cr (CmpL src zero));

  format %{ "testq   $src, $src" %}
  ins_encode %{
    __ testq($src$$Register, $src$$Register);
  %}
  ins_pipe(ialu_cr_reg_imm);
%}

instruct testL_reg_imm(rFlagsReg cr, rRegL src, immL32 con, immL0 zero)
%{
  match(Set cr (CmpL (AndL src con) zero));

  format %{ "testq   $src, $con\t# long" %}
  ins_encode %{
    __ testq($src$$Register, $con$$constant);
  %}
  ins_pipe(ialu_cr_reg_imm);
%}

instruct testL_reg_reg(rFlagsReg cr, rRegL src1, rRegL src2, immL0 zero)
%{
  match(Set cr (CmpL (AndL src1 src2) zero));

  format %{ "testq   $src1, $src2\t# long" %}
  ins_encode %{
    __ testq($src1$$Register, $src2$$Register);
  %}
  ins_pipe(ialu_cr_reg_imm);
%}

instruct testL_reg_mem(rFlagsReg cr, rRegL src, memory mem, immL0 zero)
%{
  match(Set cr (CmpL (AndL src (LoadL mem)) zero));

  format %{ "testq   $src, $mem" %}
  ins_encode %{
    __ testq($src$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_cr_reg_mem);
%}

instruct testL_reg_mem2(rFlagsReg cr, rRegP src, memory mem, immL0 zero)
%{
  match(Set cr (CmpL (AndL (CastP2X src) (LoadL mem)) zero));

  format %{ "testq   $src, $mem" %}
  ins_encode %{
    __ testq($src$$Register, $mem$$Address);
  %}
  ins_pipe(ialu_cr_reg_mem);
%}

// Manifest a CmpU result in an integer register.  Very painful.
// This is the test to avoid.
instruct cmpU3_reg_reg(rRegI dst, rRegI src1, rRegI src2, rFlagsReg flags)
%{
  match(Set dst (CmpU3 src1 src2));
  effect(KILL flags);

  ins_cost(275); // XXX
  format %{ "cmpl    $src1, $src2\t# CmpL3\n\t"
            "movl    $dst, -1\n\t"
            "jb,u    done\n\t"
            "setcc   $dst \t# emits setne + movzbl or setzune for APX"
    "done:" %}
  ins_encode %{
    Label done;
    __ cmpl($src1$$Register, $src2$$Register);
    __ movl($dst$$Register, -1);
    __ jccb(Assembler::below, done);
    __ setcc(Assembler::notZero, $dst$$Register);
    __ bind(done);
  %}
  ins_pipe(pipe_slow);
%}

// Manifest a CmpL result in an integer register.  Very painful.
// This is the test to avoid.
instruct cmpL3_reg_reg(rRegI dst, rRegL src1, rRegL src2, rFlagsReg flags)
%{
  match(Set dst (CmpL3 src1 src2));
  effect(KILL flags);

  ins_cost(275); // XXX
  format %{ "cmpq    $src1, $src2\t# CmpL3\n\t"
            "movl    $dst, -1\n\t"
            "jl,s    done\n\t"
            "setcc   $dst \t# emits setne + movzbl or setzune for APX"
    "done:" %}
  ins_encode %{
    Label done;
    __ cmpq($src1$$Register, $src2$$Register);
    __ movl($dst$$Register, -1);
    __ jccb(Assembler::less, done);
    __ setcc(Assembler::notZero, $dst$$Register);
    __ bind(done);
  %}
  ins_pipe(pipe_slow);
%}

// Manifest a CmpUL result in an integer register.  Very painful.
// This is the test to avoid.
instruct cmpUL3_reg_reg(rRegI dst, rRegL src1, rRegL src2, rFlagsReg flags)
%{
  match(Set dst (CmpUL3 src1 src2));
  effect(KILL flags);

  ins_cost(275); // XXX
  format %{ "cmpq    $src1, $src2\t# CmpL3\n\t"
            "movl    $dst, -1\n\t"
            "jb,u    done\n\t"
            "setcc   $dst \t# emits setne + movzbl or setzune for APX"
    "done:" %}
  ins_encode %{
    Label done;
    __ cmpq($src1$$Register, $src2$$Register);
    __ movl($dst$$Register, -1);
    __ jccb(Assembler::below, done);
    __ setcc(Assembler::notZero, $dst$$Register);
    __ bind(done);
  %}
  ins_pipe(pipe_slow);
%}

// Unsigned long compare Instructions; really, same as signed long except they
// produce an rFlagsRegU instead of rFlagsReg.
instruct compUL_rReg(rFlagsRegU cr, rRegL op1, rRegL op2)
%{
  match(Set cr (CmpUL op1 op2));

  format %{ "cmpq    $op1, $op2\t# unsigned" %}
  ins_encode %{
    __ cmpq($op1$$Register, $op2$$Register);
  %}
  ins_pipe(ialu_cr_reg_reg);
%}

instruct compUL_rReg_imm(rFlagsRegU cr, rRegL op1, immL32 op2)
%{
  match(Set cr (CmpUL op1 op2));

  format %{ "cmpq    $op1, $op2\t# unsigned" %}
  ins_encode %{
    __ cmpq($op1$$Register, $op2$$constant);
  %}
  ins_pipe(ialu_cr_reg_imm);
%}

instruct compUL_rReg_mem(rFlagsRegU cr, rRegL op1, memory op2)
%{
  match(Set cr (CmpUL op1 (LoadL op2)));

  format %{ "cmpq    $op1, $op2\t# unsigned" %}
  ins_encode %{
    __ cmpq($op1$$Register, $op2$$Address);
  %}
  ins_pipe(ialu_cr_reg_mem);
%}

instruct testUL_reg(rFlagsRegU cr, rRegL src, immL0 zero)
%{
  match(Set cr (CmpUL src zero));

  format %{ "testq   $src, $src\t# unsigned" %}
  ins_encode %{
    __ testq($src$$Register, $src$$Register);
  %}
  ins_pipe(ialu_cr_reg_imm);
%}

instruct compB_mem_imm(rFlagsReg cr, memory mem, immI8 imm)
%{
  match(Set cr (CmpI (LoadB mem) imm));

  ins_cost(125);
  format %{ "cmpb    $mem, $imm" %}
  ins_encode %{ __ cmpb($mem$$Address, $imm$$constant); %}
  ins_pipe(ialu_cr_reg_mem);
%}

instruct testUB_mem_imm(rFlagsReg cr, memory mem, immU7 imm, immI_0 zero)
%{
  match(Set cr (CmpI (AndI (LoadUB mem) imm) zero));

  ins_cost(125);
  format %{ "testb   $mem, $imm\t# ubyte" %}
  ins_encode %{ __ testb($mem$$Address, $imm$$constant); %}
  ins_pipe(ialu_cr_reg_mem);
%}

instruct testB_mem_imm(rFlagsReg cr, memory mem, immI8 imm, immI_0 zero)
%{
  match(Set cr (CmpI (AndI (LoadB mem) imm) zero));

  ins_cost(125);
  format %{ "testb   $mem, $imm\t# byte" %}
  ins_encode %{ __ testb($mem$$Address, $imm$$constant); %}
  ins_pipe(ialu_cr_reg_mem);
%}

//----------Max and Min--------------------------------------------------------
// Min Instructions

instruct cmovI_reg_g(rRegI dst, rRegI src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  effect(USE_DEF dst, USE src, USE cr);

  format %{ "cmovlgt $dst, $src\t# min" %}
  ins_encode %{
    __ cmovl(Assembler::greater, $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovI_reg_g_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  effect(DEF dst, USE src1, USE src2, USE cr);

  format %{ "ecmovlgt $dst, $src1, $src2\t# min ndd" %}
  ins_encode %{
    __ ecmovl(Assembler::greater, $dst$$Register, $src1$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct minI_rReg(rRegI dst, rRegI src)
%{
  predicate(!UseAPX);
  match(Set dst (MinI dst src));

  ins_cost(200);
  expand %{
    rFlagsReg cr;
    compI_rReg(cr, dst, src);
    cmovI_reg_g(dst, src, cr);
  %}
%}

instruct minI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2)
%{
  predicate(UseAPX);
  match(Set dst (MinI src1 src2));
  effect(DEF dst, USE src1, USE src2);

  ins_cost(200);
  expand %{
    rFlagsReg cr;
    compI_rReg(cr, src1, src2);
    cmovI_reg_g_ndd(dst, src1, src2, cr);
  %}
%}

instruct cmovI_reg_l(rRegI dst, rRegI src, rFlagsReg cr)
%{
  predicate(!UseAPX);
  effect(USE_DEF dst, USE src, USE cr);

  format %{ "cmovllt $dst, $src\t# max" %}
  ins_encode %{
    __ cmovl(Assembler::less, $dst$$Register, $src$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct cmovI_reg_l_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)
%{
  predicate(UseAPX);
  effect(DEF dst, USE src1, USE src2, USE cr);

  format %{ "ecmovllt $dst, $src1, $src2\t# max ndd" %}
  ins_encode %{
    __ ecmovl(Assembler::less, $dst$$Register, $src1$$Register, $src2$$Register);
  %}
  ins_pipe(pipe_cmov_reg);
%}

instruct maxI_rReg(rRegI dst, rRegI src)
%{
  predicate(!UseAPX);
  match(Set dst (MaxI dst src));

  ins_cost(200);
  expand %{
    rFlagsReg cr;
    compI_rReg(cr, dst, src);
    cmovI_reg_l(dst, src, cr);
  %}
%}

instruct maxI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2)
%{
  predicate(UseAPX);
  match(Set dst (MaxI src1 src2));
  effect(DEF dst, USE src1, USE src2);

  ins_cost(200);
  expand %{
    rFlagsReg cr;
    compI_rReg(cr, src1, src2);
    cmovI_reg_l_ndd(dst, src1, src2, cr);
  %}
%}

// ============================================================================
// Branch Instructions

// Jump Direct - Label defines a relative address from JMP+1
instruct jmpDir(label labl)
%{
  match(Goto);
  effect(USE labl);

  ins_cost(300);
  format %{ "jmp     $labl" %}
  size(5);
  ins_encode %{
    Label* L = $labl$$label;
    __ jmp(*L, false); // Always long jump
  %}
  ins_pipe(pipe_jmp);
%}

// Jump Direct Conditional - Label defines a relative address from Jcc+1
instruct jmpCon(cmpOp cop, rFlagsReg cr, label labl)
%{
  match(If cop cr);
  effect(USE labl);

  ins_cost(300);
  format %{ "j$cop     $labl" %}
  size(6);
  ins_encode %{
    Label* L = $labl$$label;
    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); // Always long jump
  %}
  ins_pipe(pipe_jcc);
%}

// Jump Direct Conditional - Label defines a relative address from Jcc+1
instruct jmpLoopEnd(cmpOp cop, rFlagsReg cr, label labl)
%{
  match(CountedLoopEnd cop cr);
  effect(USE labl);

  ins_cost(300);
  format %{ "j$cop     $labl\t# loop end" %}
  size(6);
  ins_encode %{
    Label* L = $labl$$label;
    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); // Always long jump
  %}
  ins_pipe(pipe_jcc);
%}

// Jump Direct Conditional - using unsigned comparison
instruct jmpConU(cmpOpU cop, rFlagsRegU cmp, label labl) %{
  match(If cop cmp);
  effect(USE labl);

  ins_cost(300);
  format %{ "j$cop,u   $labl" %}
  size(6);
  ins_encode %{
    Label* L = $labl$$label;
    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); // Always long jump
  %}
  ins_pipe(pipe_jcc);
%}

instruct jmpConUCF(cmpOpUCF cop, rFlagsRegUCF cmp, label labl) %{
  match(If cop cmp);
  effect(USE labl);

  ins_cost(200);
  format %{ "j$cop,u   $labl" %}
  size(6);
  ins_encode %{
    Label* L = $labl$$label;
    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); // Always long jump
  %}
  ins_pipe(pipe_jcc);
%}

instruct jmpConUCF2(cmpOpUCF2 cop, rFlagsRegUCF cmp, label labl) %{
  match(If cop cmp);
  effect(USE labl);

  ins_cost(200);
  format %{ $$template
    if ($cop$$cmpcode == Assembler::notEqual) {
      $$emit$$"jp,u    $labl\n\t"
      $$emit$$"j$cop,u   $labl"
    } else {
      $$emit$$"jp,u    done\n\t"
      $$emit$$"j$cop,u   $labl\n\t"
      $$emit$$"done:"
    }
  %}
  ins_encode %{
    Label* l = $labl$$label;
    if ($cop$$cmpcode == Assembler::notEqual) {
      __ jcc(Assembler::parity, *l, false);
      __ jcc(Assembler::notEqual, *l, false);
    } else if ($cop$$cmpcode == Assembler::equal) {
      Label done;
      __ jccb(Assembler::parity, done);
      __ jcc(Assembler::equal, *l, false);
      __ bind(done);
    } else {
       ShouldNotReachHere();
    }
  %}
  ins_pipe(pipe_jcc);
%}

// ============================================================================
// The 2nd slow-half of a subtype check.  Scan the subklass's 2ndary
// superklass array for an instance of the superklass.  Set a hidden
// internal cache on a hit (cache is checked with exposed code in
// gen_subtype_check()).  Return NZ for a miss or zero for a hit.  The
// encoding ALSO sets flags.

instruct partialSubtypeCheck(rdi_RegP result,
                             rsi_RegP sub, rax_RegP super, rcx_RegI rcx,
                             rFlagsReg cr)
%{
  match(Set result (PartialSubtypeCheck sub super));
  predicate(!UseSecondarySupersTable);
  effect(KILL rcx, KILL cr);

  ins_cost(1100);  // slightly larger than the next version
  format %{ "movq    rdi, [$sub + in_bytes(Klass::secondary_supers_offset())]\n\t"
            "movl    rcx, [rdi + Array<Klass*>::length_offset_in_bytes()]\t# length to scan\n\t"
            "addq    rdi, Array<Klass*>::base_offset_in_bytes()\t# Skip to start of data; set NZ in case count is zero\n\t"
            "repne   scasq\t# Scan *rdi++ for a match with rax while rcx--\n\t"
            "jne,s   miss\t\t# Missed: rdi not-zero\n\t"
            "movq    [$sub + in_bytes(Klass::secondary_super_cache_offset())], $super\t# Hit: update cache\n\t"
            "xorq    $result, $result\t\t Hit: rdi zero\n\t"
    "miss:\t" %}

  ins_encode %{
    Label miss;
    // NB: Callers may assume that, when $result is a valid register,
    // check_klass_subtype_slow_path_linear sets it to a nonzero
    // value.
    __ check_klass_subtype_slow_path_linear($sub$$Register, $super$$Register,
                                            $rcx$$Register, $result$$Register,
                                            nullptr, &miss,
                                            /*set_cond_codes:*/ true);
    __ xorptr($result$$Register, $result$$Register);
    __ bind(miss);
  %}

  ins_pipe(pipe_slow);
%}

// ============================================================================
// Two versions of hashtable-based partialSubtypeCheck, both used when
// we need to search for a super class in the secondary supers array.
// The first is used when we don't know _a priori_ the class being
// searched for. The second, far more common, is used when we do know:
// this is used for instanceof, checkcast, and any case where C2 can
// determine it by constant propagation.

instruct partialSubtypeCheckVarSuper(rsi_RegP sub, rax_RegP super, rdi_RegP result,
                                       rdx_RegL temp1, rcx_RegL temp2, rbx_RegP temp3, r11_RegL temp4,
                                       rFlagsReg cr)
%{
  match(Set result (PartialSubtypeCheck sub super));
  predicate(UseSecondarySupersTable);
  effect(KILL cr, TEMP temp1, TEMP temp2, TEMP temp3, TEMP temp4);

  ins_cost(1000);
  format %{ "partialSubtypeCheck $result, $sub, $super" %}

  ins_encode %{
    __ lookup_secondary_supers_table_var($sub$$Register, $super$$Register, $temp1$$Register, $temp2$$Register,
					 $temp3$$Register, $temp4$$Register, $result$$Register);
  %}

  ins_pipe(pipe_slow);
%}

instruct partialSubtypeCheckConstSuper(rsi_RegP sub, rax_RegP super_reg, immP super_con, rdi_RegP result,
                                       rdx_RegL temp1, rcx_RegL temp2, rbx_RegP temp3, r11_RegL temp4,
                                       rFlagsReg cr)
%{
  match(Set result (PartialSubtypeCheck sub (Binary super_reg super_con)));
  predicate(UseSecondarySupersTable);
  effect(KILL cr, TEMP temp1, TEMP temp2, TEMP temp3, TEMP temp4);

  ins_cost(700);  // smaller than the next version
  format %{ "partialSubtypeCheck $result, $sub, $super_reg, $super_con" %}

  ins_encode %{
    u1 super_klass_slot = ((Klass*)$super_con$$constant)->hash_slot();
    if (InlineSecondarySupersTest) {
      __ lookup_secondary_supers_table_const($sub$$Register, $super_reg$$Register, $temp1$$Register, $temp2$$Register,
                                       $temp3$$Register, $temp4$$Register, $result$$Register,
                                       super_klass_slot);
    } else {
      __ call(RuntimeAddress(StubRoutines::lookup_secondary_supers_table_stub(super_klass_slot)));
    }
  %}

  ins_pipe(pipe_slow);
%}

// ============================================================================
// Branch Instructions -- short offset versions
//
// These instructions are used to replace jumps of a long offset (the default
// match) with jumps of a shorter offset.  These instructions are all tagged
// with the ins_short_branch attribute, which causes the ADLC to suppress the
// match rules in general matching.  Instead, the ADLC generates a conversion
// method in the MachNode which can be used to do in-place replacement of the
// long variant with the shorter variant.  The compiler will determine if a
// branch can be taken by the is_short_branch_offset() predicate in the machine
// specific code section of the file.

// Jump Direct - Label defines a relative address from JMP+1
instruct jmpDir_short(label labl) %{
  match(Goto);
  effect(USE labl);

  ins_cost(300);
  format %{ "jmp,s   $labl" %}
  size(2);
  ins_encode %{
    Label* L = $labl$$label;
    __ jmpb(*L);
  %}
  ins_pipe(pipe_jmp);
  ins_short_branch(1);
%}

// Jump Direct Conditional - Label defines a relative address from Jcc+1
instruct jmpCon_short(cmpOp cop, rFlagsReg cr, label labl) %{
  match(If cop cr);
  effect(USE labl);

  ins_cost(300);
  format %{ "j$cop,s   $labl" %}
  size(2);
  ins_encode %{
    Label* L = $labl$$label;
    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);
  %}
  ins_pipe(pipe_jcc);
  ins_short_branch(1);
%}

// Jump Direct Conditional - Label defines a relative address from Jcc+1
instruct jmpLoopEnd_short(cmpOp cop, rFlagsReg cr, label labl) %{
  match(CountedLoopEnd cop cr);
  effect(USE labl);

  ins_cost(300);
  format %{ "j$cop,s   $labl\t# loop end" %}
  size(2);
  ins_encode %{
    Label* L = $labl$$label;
    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);
  %}
  ins_pipe(pipe_jcc);
  ins_short_branch(1);
%}

// Jump Direct Conditional - using unsigned comparison
instruct jmpConU_short(cmpOpU cop, rFlagsRegU cmp, label labl) %{
  match(If cop cmp);
  effect(USE labl);

  ins_cost(300);
  format %{ "j$cop,us  $labl" %}
  size(2);
  ins_encode %{
    Label* L = $labl$$label;
    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);
  %}
  ins_pipe(pipe_jcc);
  ins_short_branch(1);
%}

instruct jmpConUCF_short(cmpOpUCF cop, rFlagsRegUCF cmp, label labl) %{
  match(If cop cmp);
  effect(USE labl);

  ins_cost(300);
  format %{ "j$cop,us  $labl" %}
  size(2);
  ins_encode %{
    Label* L = $labl$$label;
    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);
  %}
  ins_pipe(pipe_jcc);
  ins_short_branch(1);
%}

instruct jmpConUCF2_short(cmpOpUCF2 cop, rFlagsRegUCF cmp, label labl) %{
  match(If cop cmp);
  effect(USE labl);

  ins_cost(300);
  format %{ $$template
    if ($cop$$cmpcode == Assembler::notEqual) {
      $$emit$$"jp,u,s  $labl\n\t"
      $$emit$$"j$cop,u,s  $labl"
    } else {
      $$emit$$"jp,u,s  done\n\t"
      $$emit$$"j$cop,u,s  $labl\n\t"
      $$emit$$"done:"
    }
  %}
  size(4);
  ins_encode %{
    Label* l = $labl$$label;
    if ($cop$$cmpcode == Assembler::notEqual) {
      __ jccb(Assembler::parity, *l);
      __ jccb(Assembler::notEqual, *l);
    } else if ($cop$$cmpcode == Assembler::equal) {
      Label done;
      __ jccb(Assembler::parity, done);
      __ jccb(Assembler::equal, *l);
      __ bind(done);
    } else {
       ShouldNotReachHere();
    }
  %}
  ins_pipe(pipe_jcc);
  ins_short_branch(1);
%}

// ============================================================================
// inlined locking and unlocking

instruct cmpFastLockLightweight(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI rax_reg, rRegP tmp) %{
  match(Set cr (FastLock object box));
  effect(TEMP rax_reg, TEMP tmp, USE_KILL box);
  ins_cost(300);
  format %{ "fastlock $object,$box\t! kills $box,$rax_reg,$tmp" %}
  ins_encode %{
    __ fast_lock_lightweight($object$$Register, $box$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);
  %}
  ins_pipe(pipe_slow);
%}

instruct cmpFastUnlockLightweight(rFlagsReg cr, rRegP object, rax_RegP rax_reg, rRegP tmp) %{
  match(Set cr (FastUnlock object rax_reg));
  effect(TEMP tmp, USE_KILL rax_reg);
  ins_cost(300);
  format %{ "fastunlock $object,$rax_reg\t! kills $rax_reg,$tmp" %}
  ins_encode %{
    __ fast_unlock_lightweight($object$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);
  %}
  ins_pipe(pipe_slow);
%}


// ============================================================================
// Safepoint Instructions
instruct safePoint_poll_tls(rFlagsReg cr, rRegP poll)
%{
  match(SafePoint poll);
  effect(KILL cr, USE poll);

  format %{ "testl   rax, [$poll]\t"
            "# Safepoint: poll for GC" %}
  ins_cost(125);
  ins_encode %{
    __ relocate(relocInfo::poll_type);
    address pre_pc = __ pc();
    __ testl(rax, Address($poll$$Register, 0));
    assert(nativeInstruction_at(pre_pc)->is_safepoint_poll(), "must emit test %%eax [reg]");
  %}
  ins_pipe(ialu_reg_mem);
%}

instruct mask_all_evexL(kReg dst, rRegL src) %{
  match(Set dst (MaskAll src));
  format %{ "mask_all_evexL $dst, $src \t! mask all operation" %}
  ins_encode %{
    int mask_len = Matcher::vector_length(this);
    __ vector_maskall_operation($dst$$KRegister, $src$$Register, mask_len);
  %}
  ins_pipe( pipe_slow );
%}

instruct mask_all_evexI_GT32(kReg dst, rRegI src, rRegL tmp) %{
  predicate(Matcher::vector_length(n) > 32);
  match(Set dst (MaskAll src));
  effect(TEMP tmp);
  format %{ "mask_all_evexI_GT32 $dst, $src \t! using $tmp as TEMP" %}
  ins_encode %{
    int mask_len = Matcher::vector_length(this);
    __ movslq($tmp$$Register, $src$$Register);
    __ vector_maskall_operation($dst$$KRegister, $tmp$$Register, mask_len);
  %}
  ins_pipe( pipe_slow );
%}

// ============================================================================
// Procedure Call/Return Instructions
// Call Java Static Instruction
// Note: If this code changes, the corresponding ret_addr_offset() and
//       compute_padding() functions will have to be adjusted.
instruct CallStaticJavaDirect(method meth) %{
  match(CallStaticJava);
  effect(USE meth);

  ins_cost(300);
  format %{ "call,static " %}
  opcode(0xE8); /* E8 cd */
  ins_encode(clear_avx, Java_Static_Call(meth), call_epilog);
  ins_pipe(pipe_slow);
  ins_alignment(4);
%}

// Call Java Dynamic Instruction
// Note: If this code changes, the corresponding ret_addr_offset() and
//       compute_padding() functions will have to be adjusted.
instruct CallDynamicJavaDirect(method meth)
%{
  match(CallDynamicJava);
  effect(USE meth);

  ins_cost(300);
  format %{ "movq    rax, #Universe::non_oop_word()\n\t"
            "call,dynamic " %}
  ins_encode(clear_avx, Java_Dynamic_Call(meth), call_epilog);
  ins_pipe(pipe_slow);
  ins_alignment(4);
%}

// Call Runtime Instruction
instruct CallRuntimeDirect(method meth)
%{
  match(CallRuntime);
  effect(USE meth);

  ins_cost(300);
  format %{ "call,runtime " %}
  ins_encode(clear_avx, Java_To_Runtime(meth));
  ins_pipe(pipe_slow);
%}

// Call runtime without safepoint
instruct CallLeafDirect(method meth)
%{
  match(CallLeaf);
  effect(USE meth);

  ins_cost(300);
  format %{ "call_leaf,runtime " %}
  ins_encode(clear_avx, Java_To_Runtime(meth));
  ins_pipe(pipe_slow);
%}

// Call runtime without safepoint and with vector arguments
instruct CallLeafDirectVector(method meth)
%{
  match(CallLeafVector);
  effect(USE meth);

  ins_cost(300);
  format %{ "call_leaf,vector " %}
  ins_encode(Java_To_Runtime(meth));
  ins_pipe(pipe_slow);
%}

// Call runtime without safepoint
instruct CallLeafNoFPDirect(method meth)
%{
  match(CallLeafNoFP);
  effect(USE meth);

  ins_cost(300);
  format %{ "call_leaf_nofp,runtime " %}
  ins_encode(clear_avx, Java_To_Runtime(meth));
  ins_pipe(pipe_slow);
%}

// Return Instruction
// Remove the return address & jump to it.
// Notice: We always emit a nop after a ret to make sure there is room
// for safepoint patching
instruct Ret()
%{
  match(Return);

  format %{ "ret" %}
  ins_encode %{
    __ ret(0);
  %}
  ins_pipe(pipe_jmp);
%}

// Tail Call; Jump from runtime stub to Java code.
// Also known as an 'interprocedural jump'.
// Target of jump will eventually return to caller.
// TailJump below removes the return address.
// Don't use rbp for 'jump_target' because a MachEpilogNode has already been
// emitted just above the TailCall which has reset rbp to the caller state.
instruct TailCalljmpInd(no_rbp_RegP jump_target, rbx_RegP method_ptr)
%{
  match(TailCall jump_target method_ptr);

  ins_cost(300);
  format %{ "jmp     $jump_target\t# rbx holds method" %}
  ins_encode %{
    __ jmp($jump_target$$Register);
  %}
  ins_pipe(pipe_jmp);
%}

// Tail Jump; remove the return address; jump to target.
// TailCall above leaves the return address around.
instruct tailjmpInd(no_rbp_RegP jump_target, rax_RegP ex_oop)
%{
  match(TailJump jump_target ex_oop);

  ins_cost(300);
  format %{ "popq    rdx\t# pop return address\n\t"
            "jmp     $jump_target" %}
  ins_encode %{
    __ popq(as_Register(RDX_enc));
    __ jmp($jump_target$$Register);
  %}
  ins_pipe(pipe_jmp);
%}

// Forward exception.
instruct ForwardExceptionjmp()
%{
  match(ForwardException);

  format %{ "jmp     forward_exception_stub" %}
  ins_encode %{
    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()), noreg);
  %}
  ins_pipe(pipe_jmp);
%}

// Create exception oop: created by stack-crawling runtime code.
// Created exception is now available to this handler, and is setup
// just prior to jumping to this handler.  No code emitted.
instruct CreateException(rax_RegP ex_oop)
%{
  match(Set ex_oop (CreateEx));

  size(0);
  // use the following format syntax
  format %{ "# exception oop is in rax; no code emitted" %}
  ins_encode();
  ins_pipe(empty);
%}

// Rethrow exception:
// The exception oop will come in the first argument position.
// Then JUMP (not call) to the rethrow stub code.
instruct RethrowException()
%{
  match(Rethrow);

  // use the following format syntax
  format %{ "jmp     rethrow_stub" %}
  ins_encode %{
    __ jump(RuntimeAddress(OptoRuntime::rethrow_stub()), noreg);
  %}
  ins_pipe(pipe_jmp);
%}

// ============================================================================
// This name is KNOWN by the ADLC and cannot be changed.
// The ADLC forces a 'TypeRawPtr::BOTTOM' output type
// for this guy.
instruct tlsLoadP(r15_RegP dst) %{
  match(Set dst (ThreadLocal));
  effect(DEF dst);

  size(0);
  format %{ "# TLS is in R15" %}
  ins_encode( /*empty encoding*/ );
  ins_pipe(ialu_reg_reg);
%}

instruct addF_reg(regF dst, regF src) %{
  predicate(UseAVX == 0);
  match(Set dst (AddF dst src));

  format %{ "addss   $dst, $src" %}
  ins_cost(150);
  ins_encode %{
    __ addss($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct addF_mem(regF dst, memory src) %{
  predicate(UseAVX == 0);
  match(Set dst (AddF dst (LoadF src)));

  format %{ "addss   $dst, $src" %}
  ins_cost(150);
  ins_encode %{
    __ addss($dst$$XMMRegister, $src$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct addF_imm(regF dst, immF con) %{
  predicate(UseAVX == 0);
  match(Set dst (AddF dst con));
  format %{ "addss   $dst, [$constantaddress]\t# load from constant table: float=$con" %}
  ins_cost(150);
  ins_encode %{
    __ addss($dst$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct addF_reg_reg(regF dst, regF src1, regF src2) %{
  predicate(UseAVX > 0);
  match(Set dst (AddF src1 src2));

  format %{ "vaddss  $dst, $src1, $src2" %}
  ins_cost(150);
  ins_encode %{
    __ vaddss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct addF_reg_mem(regF dst, regF src1, memory src2) %{
  predicate(UseAVX > 0);
  match(Set dst (AddF src1 (LoadF src2)));

  format %{ "vaddss  $dst, $src1, $src2" %}
  ins_cost(150);
  ins_encode %{
    __ vaddss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct addF_reg_imm(regF dst, regF src, immF con) %{
  predicate(UseAVX > 0);
  match(Set dst (AddF src con));

  format %{ "vaddss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con" %}
  ins_cost(150);
  ins_encode %{
    __ vaddss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct addD_reg(regD dst, regD src) %{
  predicate(UseAVX == 0);
  match(Set dst (AddD dst src));

  format %{ "addsd   $dst, $src" %}
  ins_cost(150);
  ins_encode %{
    __ addsd($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct addD_mem(regD dst, memory src) %{
  predicate(UseAVX == 0);
  match(Set dst (AddD dst (LoadD src)));

  format %{ "addsd   $dst, $src" %}
  ins_cost(150);
  ins_encode %{
    __ addsd($dst$$XMMRegister, $src$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct addD_imm(regD dst, immD con) %{
  predicate(UseAVX == 0);
  match(Set dst (AddD dst con));
  format %{ "addsd   $dst, [$constantaddress]\t# load from constant table: double=$con" %}
  ins_cost(150);
  ins_encode %{
    __ addsd($dst$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct addD_reg_reg(regD dst, regD src1, regD src2) %{
  predicate(UseAVX > 0);
  match(Set dst (AddD src1 src2));

  format %{ "vaddsd  $dst, $src1, $src2" %}
  ins_cost(150);
  ins_encode %{
    __ vaddsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct addD_reg_mem(regD dst, regD src1, memory src2) %{
  predicate(UseAVX > 0);
  match(Set dst (AddD src1 (LoadD src2)));

  format %{ "vaddsd  $dst, $src1, $src2" %}
  ins_cost(150);
  ins_encode %{
    __ vaddsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct addD_reg_imm(regD dst, regD src, immD con) %{
  predicate(UseAVX > 0);
  match(Set dst (AddD src con));

  format %{ "vaddsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con" %}
  ins_cost(150);
  ins_encode %{
    __ vaddsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct subF_reg(regF dst, regF src) %{
  predicate(UseAVX == 0);
  match(Set dst (SubF dst src));

  format %{ "subss   $dst, $src" %}
  ins_cost(150);
  ins_encode %{
    __ subss($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct subF_mem(regF dst, memory src) %{
  predicate(UseAVX == 0);
  match(Set dst (SubF dst (LoadF src)));

  format %{ "subss   $dst, $src" %}
  ins_cost(150);
  ins_encode %{
    __ subss($dst$$XMMRegister, $src$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct subF_imm(regF dst, immF con) %{
  predicate(UseAVX == 0);
  match(Set dst (SubF dst con));
  format %{ "subss   $dst, [$constantaddress]\t# load from constant table: float=$con" %}
  ins_cost(150);
  ins_encode %{
    __ subss($dst$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct subF_reg_reg(regF dst, regF src1, regF src2) %{
  predicate(UseAVX > 0);
  match(Set dst (SubF src1 src2));

  format %{ "vsubss  $dst, $src1, $src2" %}
  ins_cost(150);
  ins_encode %{
    __ vsubss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct subF_reg_mem(regF dst, regF src1, memory src2) %{
  predicate(UseAVX > 0);
  match(Set dst (SubF src1 (LoadF src2)));

  format %{ "vsubss  $dst, $src1, $src2" %}
  ins_cost(150);
  ins_encode %{
    __ vsubss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct subF_reg_imm(regF dst, regF src, immF con) %{
  predicate(UseAVX > 0);
  match(Set dst (SubF src con));

  format %{ "vsubss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con" %}
  ins_cost(150);
  ins_encode %{
    __ vsubss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct subD_reg(regD dst, regD src) %{
  predicate(UseAVX == 0);
  match(Set dst (SubD dst src));

  format %{ "subsd   $dst, $src" %}
  ins_cost(150);
  ins_encode %{
    __ subsd($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct subD_mem(regD dst, memory src) %{
  predicate(UseAVX == 0);
  match(Set dst (SubD dst (LoadD src)));

  format %{ "subsd   $dst, $src" %}
  ins_cost(150);
  ins_encode %{
    __ subsd($dst$$XMMRegister, $src$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct subD_imm(regD dst, immD con) %{
  predicate(UseAVX == 0);
  match(Set dst (SubD dst con));
  format %{ "subsd   $dst, [$constantaddress]\t# load from constant table: double=$con" %}
  ins_cost(150);
  ins_encode %{
    __ subsd($dst$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct subD_reg_reg(regD dst, regD src1, regD src2) %{
  predicate(UseAVX > 0);
  match(Set dst (SubD src1 src2));

  format %{ "vsubsd  $dst, $src1, $src2" %}
  ins_cost(150);
  ins_encode %{
    __ vsubsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct subD_reg_mem(regD dst, regD src1, memory src2) %{
  predicate(UseAVX > 0);
  match(Set dst (SubD src1 (LoadD src2)));

  format %{ "vsubsd  $dst, $src1, $src2" %}
  ins_cost(150);
  ins_encode %{
    __ vsubsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct subD_reg_imm(regD dst, regD src, immD con) %{
  predicate(UseAVX > 0);
  match(Set dst (SubD src con));

  format %{ "vsubsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con" %}
  ins_cost(150);
  ins_encode %{
    __ vsubsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct mulF_reg(regF dst, regF src) %{
  predicate(UseAVX == 0);
  match(Set dst (MulF dst src));

  format %{ "mulss   $dst, $src" %}
  ins_cost(150);
  ins_encode %{
    __ mulss($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct mulF_mem(regF dst, memory src) %{
  predicate(UseAVX == 0);
  match(Set dst (MulF dst (LoadF src)));

  format %{ "mulss   $dst, $src" %}
  ins_cost(150);
  ins_encode %{
    __ mulss($dst$$XMMRegister, $src$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct mulF_imm(regF dst, immF con) %{
  predicate(UseAVX == 0);
  match(Set dst (MulF dst con));
  format %{ "mulss   $dst, [$constantaddress]\t# load from constant table: float=$con" %}
  ins_cost(150);
  ins_encode %{
    __ mulss($dst$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct mulF_reg_reg(regF dst, regF src1, regF src2) %{
  predicate(UseAVX > 0);
  match(Set dst (MulF src1 src2));

  format %{ "vmulss  $dst, $src1, $src2" %}
  ins_cost(150);
  ins_encode %{
    __ vmulss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct mulF_reg_mem(regF dst, regF src1, memory src2) %{
  predicate(UseAVX > 0);
  match(Set dst (MulF src1 (LoadF src2)));

  format %{ "vmulss  $dst, $src1, $src2" %}
  ins_cost(150);
  ins_encode %{
    __ vmulss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct mulF_reg_imm(regF dst, regF src, immF con) %{
  predicate(UseAVX > 0);
  match(Set dst (MulF src con));

  format %{ "vmulss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con" %}
  ins_cost(150);
  ins_encode %{
    __ vmulss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct mulD_reg(regD dst, regD src) %{
  predicate(UseAVX == 0);
  match(Set dst (MulD dst src));

  format %{ "mulsd   $dst, $src" %}
  ins_cost(150);
  ins_encode %{
    __ mulsd($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct mulD_mem(regD dst, memory src) %{
  predicate(UseAVX == 0);
  match(Set dst (MulD dst (LoadD src)));

  format %{ "mulsd   $dst, $src" %}
  ins_cost(150);
  ins_encode %{
    __ mulsd($dst$$XMMRegister, $src$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct mulD_imm(regD dst, immD con) %{
  predicate(UseAVX == 0);
  match(Set dst (MulD dst con));
  format %{ "mulsd   $dst, [$constantaddress]\t# load from constant table: double=$con" %}
  ins_cost(150);
  ins_encode %{
    __ mulsd($dst$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct mulD_reg_reg(regD dst, regD src1, regD src2) %{
  predicate(UseAVX > 0);
  match(Set dst (MulD src1 src2));

  format %{ "vmulsd  $dst, $src1, $src2" %}
  ins_cost(150);
  ins_encode %{
    __ vmulsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct mulD_reg_mem(regD dst, regD src1, memory src2) %{
  predicate(UseAVX > 0);
  match(Set dst (MulD src1 (LoadD src2)));

  format %{ "vmulsd  $dst, $src1, $src2" %}
  ins_cost(150);
  ins_encode %{
    __ vmulsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct mulD_reg_imm(regD dst, regD src, immD con) %{
  predicate(UseAVX > 0);
  match(Set dst (MulD src con));

  format %{ "vmulsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con" %}
  ins_cost(150);
  ins_encode %{
    __ vmulsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct divF_reg(regF dst, regF src) %{
  predicate(UseAVX == 0);
  match(Set dst (DivF dst src));

  format %{ "divss   $dst, $src" %}
  ins_cost(150);
  ins_encode %{
    __ divss($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct divF_mem(regF dst, memory src) %{
  predicate(UseAVX == 0);
  match(Set dst (DivF dst (LoadF src)));

  format %{ "divss   $dst, $src" %}
  ins_cost(150);
  ins_encode %{
    __ divss($dst$$XMMRegister, $src$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct divF_imm(regF dst, immF con) %{
  predicate(UseAVX == 0);
  match(Set dst (DivF dst con));
  format %{ "divss   $dst, [$constantaddress]\t# load from constant table: float=$con" %}
  ins_cost(150);
  ins_encode %{
    __ divss($dst$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct divF_reg_reg(regF dst, regF src1, regF src2) %{
  predicate(UseAVX > 0);
  match(Set dst (DivF src1 src2));

  format %{ "vdivss  $dst, $src1, $src2" %}
  ins_cost(150);
  ins_encode %{
    __ vdivss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct divF_reg_mem(regF dst, regF src1, memory src2) %{
  predicate(UseAVX > 0);
  match(Set dst (DivF src1 (LoadF src2)));

  format %{ "vdivss  $dst, $src1, $src2" %}
  ins_cost(150);
  ins_encode %{
    __ vdivss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct divF_reg_imm(regF dst, regF src, immF con) %{
  predicate(UseAVX > 0);
  match(Set dst (DivF src con));

  format %{ "vdivss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con" %}
  ins_cost(150);
  ins_encode %{
    __ vdivss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct divD_reg(regD dst, regD src) %{
  predicate(UseAVX == 0);
  match(Set dst (DivD dst src));

  format %{ "divsd   $dst, $src" %}
  ins_cost(150);
  ins_encode %{
    __ divsd($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct divD_mem(regD dst, memory src) %{
  predicate(UseAVX == 0);
  match(Set dst (DivD dst (LoadD src)));

  format %{ "divsd   $dst, $src" %}
  ins_cost(150);
  ins_encode %{
    __ divsd($dst$$XMMRegister, $src$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct divD_imm(regD dst, immD con) %{
  predicate(UseAVX == 0);
  match(Set dst (DivD dst con));
  format %{ "divsd   $dst, [$constantaddress]\t# load from constant table: double=$con" %}
  ins_cost(150);
  ins_encode %{
    __ divsd($dst$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct divD_reg_reg(regD dst, regD src1, regD src2) %{
  predicate(UseAVX > 0);
  match(Set dst (DivD src1 src2));

  format %{ "vdivsd  $dst, $src1, $src2" %}
  ins_cost(150);
  ins_encode %{
    __ vdivsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct divD_reg_mem(regD dst, regD src1, memory src2) %{
  predicate(UseAVX > 0);
  match(Set dst (DivD src1 (LoadD src2)));

  format %{ "vdivsd  $dst, $src1, $src2" %}
  ins_cost(150);
  ins_encode %{
    __ vdivsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
  %}
  ins_pipe(pipe_slow);
%}

instruct divD_reg_imm(regD dst, regD src, immD con) %{
  predicate(UseAVX > 0);
  match(Set dst (DivD src con));

  format %{ "vdivsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con" %}
  ins_cost(150);
  ins_encode %{
    __ vdivsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
  %}
  ins_pipe(pipe_slow);
%}

instruct absF_reg(regF dst) %{
  predicate(UseAVX == 0);
  match(Set dst (AbsF dst));
  ins_cost(150);
  format %{ "andps   $dst, [0x7fffffff]\t# abs float by sign masking" %}
  ins_encode %{
    __ andps($dst$$XMMRegister, ExternalAddress(float_signmask()));
  %}
  ins_pipe(pipe_slow);
%}

instruct absF_reg_reg(vlRegF dst, vlRegF src) %{
  predicate(UseAVX > 0);
  match(Set dst (AbsF src));
  ins_cost(150);
  format %{ "vandps  $dst, $src, [0x7fffffff]\t# abs float by sign masking" %}
  ins_encode %{
    int vlen_enc = Assembler::AVX_128bit;
    __ vandps($dst$$XMMRegister, $src$$XMMRegister,
              ExternalAddress(float_signmask()), vlen_enc);
  %}
  ins_pipe(pipe_slow);
%}

instruct absD_reg(regD dst) %{
  predicate(UseAVX == 0);
  match(Set dst (AbsD dst));
  ins_cost(150);
  format %{ "andpd   $dst, [0x7fffffffffffffff]\t"
            "# abs double by sign masking" %}
  ins_encode %{
    __ andpd($dst$$XMMRegister, ExternalAddress(double_signmask()));
  %}
  ins_pipe(pipe_slow);
%}

instruct absD_reg_reg(vlRegD dst, vlRegD src) %{
  predicate(UseAVX > 0);
  match(Set dst (AbsD src));
  ins_cost(150);
  format %{ "vandpd  $dst, $src, [0x7fffffffffffffff]\t"
            "# abs double by sign masking" %}
  ins_encode %{
    int vlen_enc = Assembler::AVX_128bit;
    __ vandpd($dst$$XMMRegister, $src$$XMMRegister,
              ExternalAddress(double_signmask()), vlen_enc);
  %}
  ins_pipe(pipe_slow);
%}

instruct negF_reg(regF dst) %{
  predicate(UseAVX == 0);
  match(Set dst (NegF dst));
  ins_cost(150);
  format %{ "xorps   $dst, [0x80000000]\t# neg float by sign flipping" %}
  ins_encode %{
    __ xorps($dst$$XMMRegister, ExternalAddress(float_signflip()));
  %}
  ins_pipe(pipe_slow);
%}

instruct negF_reg_reg(vlRegF dst, vlRegF src) %{
  predicate(UseAVX > 0);
  match(Set dst (NegF src));
  ins_cost(150);
  format %{ "vnegatess  $dst, $src, [0x80000000]\t# neg float by sign flipping" %}
  ins_encode %{
    __ vnegatess($dst$$XMMRegister, $src$$XMMRegister,
                 ExternalAddress(float_signflip()));
  %}
  ins_pipe(pipe_slow);
%}

instruct negD_reg(regD dst) %{
  predicate(UseAVX == 0);
  match(Set dst (NegD dst));
  ins_cost(150);
  format %{ "xorpd   $dst, [0x8000000000000000]\t"
            "# neg double by sign flipping" %}
  ins_encode %{
    __ xorpd($dst$$XMMRegister, ExternalAddress(double_signflip()));
  %}
  ins_pipe(pipe_slow);
%}

instruct negD_reg_reg(vlRegD dst, vlRegD src) %{
  predicate(UseAVX > 0);
  match(Set dst (NegD src));
  ins_cost(150);
  format %{ "vnegatesd  $dst, $src, [0x8000000000000000]\t"
            "# neg double by sign flipping" %}
  ins_encode %{
    __ vnegatesd($dst$$XMMRegister, $src$$XMMRegister,
                 ExternalAddress(double_signflip()));
  %}
  ins_pipe(pipe_slow);
%}

// sqrtss instruction needs destination register to be pre initialized for best performance
// Therefore only the instruct rule where the input is pre-loaded into dst register is defined below
instruct sqrtF_reg(regF dst) %{
  match(Set dst (SqrtF dst));
  format %{ "sqrtss  $dst, $dst" %}
  ins_encode %{
    __ sqrtss($dst$$XMMRegister, $dst$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

// sqrtsd instruction needs destination register to be pre initialized for best performance
// Therefore only the instruct rule where the input is pre-loaded into dst register is defined below
instruct sqrtD_reg(regD dst) %{
  match(Set dst (SqrtD dst));
  format %{ "sqrtsd  $dst, $dst" %}
  ins_encode %{
    __ sqrtsd($dst$$XMMRegister, $dst$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct convF2HF_reg_reg(rRegI dst, vlRegF src, vlRegF tmp) %{
  effect(TEMP tmp);
  match(Set dst (ConvF2HF src));
  ins_cost(125);
  format %{ "vcvtps2ph $dst,$src \t using $tmp as TEMP"%}
  ins_encode %{
    __ flt_to_flt16($dst$$Register, $src$$XMMRegister, $tmp$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct convF2HF_mem_reg(memory mem, regF src, kReg ktmp, rRegI rtmp) %{
  predicate((UseAVX > 2) && VM_Version::supports_avx512vl());
  effect(TEMP ktmp, TEMP rtmp);
  match(Set mem (StoreC mem (ConvF2HF src)));
  format %{ "evcvtps2ph $mem,$src \t using $ktmp and $rtmp as TEMP" %}
  ins_encode %{
    __ movl($rtmp$$Register, 0x1);
    __ kmovwl($ktmp$$KRegister, $rtmp$$Register);
    __ evcvtps2ph($mem$$Address, $ktmp$$KRegister, $src$$XMMRegister, 0x04, Assembler::AVX_128bit);
  %}
  ins_pipe( pipe_slow );
%}

instruct vconvF2HF(vec dst, vec src) %{
  match(Set dst (VectorCastF2HF src));
  format %{ "vector_conv_F2HF $dst $src" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    __ vcvtps2ph($dst$$XMMRegister, $src$$XMMRegister, 0x04, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vconvF2HF_mem_reg(memory mem, vec src) %{
  predicate(n->as_StoreVector()->memory_size() >= 16);
  match(Set mem (StoreVector mem (VectorCastF2HF src)));
  format %{ "vcvtps2ph $mem,$src" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    __ vcvtps2ph($mem$$Address, $src$$XMMRegister, 0x04, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct convHF2F_reg_reg(vlRegF dst, rRegI src) %{
  match(Set dst (ConvHF2F src));
  format %{ "vcvtph2ps $dst,$src" %}
  ins_encode %{
    __ flt16_to_flt($dst$$XMMRegister, $src$$Register);
  %}
  ins_pipe( pipe_slow );
%}

instruct vconvHF2F_reg_mem(vec dst, memory mem) %{
  match(Set dst (VectorCastHF2F (LoadVector mem)));
  format %{ "vcvtph2ps $dst,$mem" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vcvtph2ps($dst$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vconvHF2F(vec dst, vec src) %{
  match(Set dst (VectorCastHF2F src));
  ins_cost(125);
  format %{ "vector_conv_HF2F $dst,$src" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vcvtph2ps($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// ---------------------------------------- VectorReinterpret ------------------------------------
instruct reinterpret_mask(kReg dst) %{
  predicate(n->bottom_type()->isa_vectmask() &&
            Matcher::vector_length(n) == Matcher::vector_length(n->in(1))); // dst == src
  match(Set dst (VectorReinterpret dst));
  ins_cost(125);
  format %{ "vector_reinterpret $dst\t!" %}
  ins_encode %{
    // empty
  %}
  ins_pipe( pipe_slow );
%}

instruct reinterpret_mask_W2B(kReg dst, kReg src, vec xtmp) %{
  predicate(UseAVX > 2 && Matcher::vector_length(n) != Matcher::vector_length(n->in(1)) &&
            n->bottom_type()->isa_vectmask() &&
            n->in(1)->bottom_type()->isa_vectmask() &&
            n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_SHORT &&
            n->bottom_type()->is_vectmask()->element_basic_type() == T_BYTE); // dst == src
  match(Set dst (VectorReinterpret src));
  effect(TEMP xtmp);
  format %{ "vector_mask_reinterpret_W2B $dst $src\t!" %}
  ins_encode %{
     int src_sz = Matcher::vector_length(this, $src)*type2aelembytes(T_SHORT);
     int dst_sz = Matcher::vector_length(this)*type2aelembytes(T_BYTE);
     assert(src_sz == dst_sz , "src and dst size mismatch");
     int vlen_enc = vector_length_encoding(src_sz);
     __  evpmovm2w($xtmp$$XMMRegister, $src$$KRegister, vlen_enc);
     __  evpmovb2m($dst$$KRegister, $xtmp$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct reinterpret_mask_D2B(kReg dst, kReg src, vec xtmp) %{
  predicate(UseAVX > 2 && Matcher::vector_length(n) != Matcher::vector_length(n->in(1)) &&
            n->bottom_type()->isa_vectmask() &&
            n->in(1)->bottom_type()->isa_vectmask() &&
            (n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_INT ||
             n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_FLOAT) &&
            n->bottom_type()->is_vectmask()->element_basic_type() == T_BYTE); // dst == src
  match(Set dst (VectorReinterpret src));
  effect(TEMP xtmp);
  format %{ "vector_mask_reinterpret_D2B $dst $src\t!" %}
  ins_encode %{
     int src_sz = Matcher::vector_length(this, $src)*type2aelembytes(T_INT);
     int dst_sz = Matcher::vector_length(this)*type2aelembytes(T_BYTE);
     assert(src_sz == dst_sz , "src and dst size mismatch");
     int vlen_enc = vector_length_encoding(src_sz);
     __  evpmovm2d($xtmp$$XMMRegister, $src$$KRegister, vlen_enc);
     __  evpmovb2m($dst$$KRegister, $xtmp$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct reinterpret_mask_Q2B(kReg dst, kReg src, vec xtmp) %{
  predicate(UseAVX > 2 && Matcher::vector_length(n) != Matcher::vector_length(n->in(1)) &&
            n->bottom_type()->isa_vectmask() &&
            n->in(1)->bottom_type()->isa_vectmask() &&
            (n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_LONG ||
             n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_DOUBLE) &&
            n->bottom_type()->is_vectmask()->element_basic_type() == T_BYTE); // dst == src
  match(Set dst (VectorReinterpret src));
  effect(TEMP xtmp);
  format %{ "vector_mask_reinterpret_Q2B $dst $src\t!" %}
  ins_encode %{
     int src_sz = Matcher::vector_length(this, $src)*type2aelembytes(T_LONG);
     int dst_sz = Matcher::vector_length(this)*type2aelembytes(T_BYTE);
     assert(src_sz == dst_sz , "src and dst size mismatch");
     int vlen_enc = vector_length_encoding(src_sz);
     __  evpmovm2q($xtmp$$XMMRegister, $src$$KRegister, vlen_enc);
     __  evpmovb2m($dst$$KRegister, $xtmp$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct reinterpret(vec dst) %{
  predicate(!n->bottom_type()->isa_vectmask() &&
            Matcher::vector_length_in_bytes(n) == Matcher::vector_length_in_bytes(n->in(1))); // dst == src
  match(Set dst (VectorReinterpret dst));
  ins_cost(125);
  format %{ "vector_reinterpret $dst\t!" %}
  ins_encode %{
    // empty
  %}
  ins_pipe( pipe_slow );
%}

instruct reinterpret_expand(vec dst, vec src) %{
  predicate(UseAVX == 0 &&
            (Matcher::vector_length_in_bytes(n->in(1)) < Matcher::vector_length_in_bytes(n))); // src < dst
  match(Set dst (VectorReinterpret src));
  ins_cost(125);
  effect(TEMP dst);
  format %{ "vector_reinterpret_expand $dst,$src" %}
  ins_encode %{
    assert(Matcher::vector_length_in_bytes(this)       <= 16, "required");
    assert(Matcher::vector_length_in_bytes(this, $src) <=  8, "required");

    int src_vlen_in_bytes = Matcher::vector_length_in_bytes(this, $src);
    if (src_vlen_in_bytes == 4) {
      __ movdqu($dst$$XMMRegister, ExternalAddress(vector_32_bit_mask()), noreg);
    } else {
      assert(src_vlen_in_bytes == 8, "");
      __ movdqu($dst$$XMMRegister, ExternalAddress(vector_64_bit_mask()), noreg);
    }
    __ pand($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vreinterpret_expand4(legVec dst, vec src) %{
  predicate(UseAVX > 0 &&
            !n->bottom_type()->isa_vectmask() &&
            (Matcher::vector_length_in_bytes(n->in(1)) == 4) && // src
            (Matcher::vector_length_in_bytes(n->in(1)) < Matcher::vector_length_in_bytes(n))); // src < dst
  match(Set dst (VectorReinterpret src));
  ins_cost(125);
  format %{ "vector_reinterpret_expand $dst,$src" %}
  ins_encode %{
    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_32_bit_mask()), 0, noreg);
  %}
  ins_pipe( pipe_slow );
%}


instruct vreinterpret_expand(legVec dst, vec src) %{
  predicate(UseAVX > 0 &&
            !n->bottom_type()->isa_vectmask() &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 4) && // src
            (Matcher::vector_length_in_bytes(n->in(1)) < Matcher::vector_length_in_bytes(n))); // src < dst
  match(Set dst (VectorReinterpret src));
  ins_cost(125);
  format %{ "vector_reinterpret_expand $dst,$src\t!" %}
  ins_encode %{
    switch (Matcher::vector_length_in_bytes(this, $src)) {
      case  8: __ movq   ($dst$$XMMRegister, $src$$XMMRegister); break;
      case 16: __ movdqu ($dst$$XMMRegister, $src$$XMMRegister); break;
      case 32: __ vmovdqu($dst$$XMMRegister, $src$$XMMRegister); break;
      default: ShouldNotReachHere();
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct reinterpret_shrink(vec dst, legVec src) %{
  predicate(!n->bottom_type()->isa_vectmask() &&
            Matcher::vector_length_in_bytes(n->in(1)) > Matcher::vector_length_in_bytes(n)); // src > dst
  match(Set dst (VectorReinterpret src));
  ins_cost(125);
  format %{ "vector_reinterpret_shrink $dst,$src\t!" %}
  ins_encode %{
    switch (Matcher::vector_length_in_bytes(this)) {
      case  4: __ movfltz($dst$$XMMRegister, $src$$XMMRegister); break;
      case  8: __ movq   ($dst$$XMMRegister, $src$$XMMRegister); break;
      case 16: __ movdqu ($dst$$XMMRegister, $src$$XMMRegister); break;
      case 32: __ vmovdqu($dst$$XMMRegister, $src$$XMMRegister); break;
      default: ShouldNotReachHere();
    }
  %}
  ins_pipe( pipe_slow );
%}

// ----------------------------------------------------------------------------------------------------

instruct roundD_reg(legRegD dst, legRegD src, immU8 rmode) %{
  match(Set dst (RoundDoubleMode src rmode));
  format %{ "roundsd $dst,$src" %}
  ins_cost(150);
  ins_encode %{
    assert(UseSSE >= 4, "required");
    if ((UseAVX == 0) && ($dst$$XMMRegister != $src$$XMMRegister)) {
      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
    }
    __ roundsd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant);
  %}
  ins_pipe(pipe_slow);
%}

instruct roundD_imm(legRegD dst, immD con, immU8 rmode) %{
  match(Set dst (RoundDoubleMode con rmode));
  format %{ "roundsd $dst,[$constantaddress]\t# load from constant table: double=$con" %}
  ins_cost(150);
  ins_encode %{
    assert(UseSSE >= 4, "required");
    __ roundsd($dst$$XMMRegister, $constantaddress($con), $rmode$$constant, noreg);
  %}
  ins_pipe(pipe_slow);
%}

instruct vroundD_reg(legVec dst, legVec src, immU8 rmode) %{
  predicate(Matcher::vector_length(n) < 8);
  match(Set dst (RoundDoubleModeV src rmode));
  format %{ "vroundpd $dst,$src,$rmode\t! round packedD" %}
  ins_encode %{
    assert(UseAVX > 0, "required");
    int vlen_enc = vector_length_encoding(this);
    __ vroundpd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vround8D_reg(vec dst, vec src, immU8 rmode) %{
  predicate(Matcher::vector_length(n) == 8);
  match(Set dst (RoundDoubleModeV src rmode));
  format %{ "vrndscalepd $dst,$src,$rmode\t! round packed8D" %}
  ins_encode %{
    assert(UseAVX > 2, "required");
    __ vrndscalepd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant, Assembler::AVX_512bit);
  %}
  ins_pipe( pipe_slow );
%}

instruct vroundD_mem(legVec dst, memory mem, immU8 rmode) %{
  predicate(Matcher::vector_length(n) < 8);
  match(Set dst (RoundDoubleModeV (LoadVector mem) rmode));
  format %{ "vroundpd $dst, $mem, $rmode\t! round packedD" %}
  ins_encode %{
    assert(UseAVX > 0, "required");
    int vlen_enc = vector_length_encoding(this);
    __ vroundpd($dst$$XMMRegister, $mem$$Address, $rmode$$constant, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vround8D_mem(vec dst, memory mem, immU8 rmode) %{
  predicate(Matcher::vector_length(n) == 8);
  match(Set dst (RoundDoubleModeV (LoadVector mem) rmode));
  format %{ "vrndscalepd $dst,$mem,$rmode\t! round packed8D" %}
  ins_encode %{
    assert(UseAVX > 2, "required");
    __ vrndscalepd($dst$$XMMRegister, $mem$$Address, $rmode$$constant, Assembler::AVX_512bit);
  %}
  ins_pipe( pipe_slow );
%}

instruct onspinwait() %{
  match(OnSpinWait);
  ins_cost(200);

  format %{
    $$template
    $$emit$$"pause\t! membar_onspinwait"
  %}
  ins_encode %{
    __ pause();
  %}
  ins_pipe(pipe_slow);
%}

// a * b + c
instruct fmaD_reg(regD a, regD b, regD c) %{
  match(Set c (FmaD  c (Binary a b)));
  format %{ "fmasd $a,$b,$c\t# $c = $a * $b + $c" %}
  ins_cost(150);
  ins_encode %{
    assert(UseFMA, "Needs FMA instructions support.");
    __ fmad($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

// a * b + c
instruct fmaF_reg(regF a, regF b, regF c) %{
  match(Set c (FmaF  c (Binary a b)));
  format %{ "fmass $a,$b,$c\t# $c = $a * $b + $c" %}
  ins_cost(150);
  ins_encode %{
    assert(UseFMA, "Needs FMA instructions support.");
    __ fmaf($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

// ====================VECTOR INSTRUCTIONS=====================================

// Dummy reg-to-reg vector moves. Removed during post-selection cleanup.
instruct MoveVec2Leg(legVec dst, vec src) %{
  match(Set dst src);
  format %{ "" %}
  ins_encode %{
    ShouldNotReachHere();
  %}
  ins_pipe( fpu_reg_reg );
%}

instruct MoveLeg2Vec(vec dst, legVec src) %{
  match(Set dst src);
  format %{ "" %}
  ins_encode %{
    ShouldNotReachHere();
  %}
  ins_pipe( fpu_reg_reg );
%}

// ============================================================================

// Load vectors generic operand pattern
instruct loadV(vec dst, memory mem) %{
  match(Set dst (LoadVector mem));
  ins_cost(125);
  format %{ "load_vector $dst,$mem" %}
  ins_encode %{
    BasicType bt = Matcher::vector_element_basic_type(this);
    __ load_vector(bt, $dst$$XMMRegister, $mem$$Address, Matcher::vector_length_in_bytes(this));
  %}
  ins_pipe( pipe_slow );
%}

// Store vectors generic operand pattern.
instruct storeV(memory mem, vec src) %{
  match(Set mem (StoreVector mem src));
  ins_cost(145);
  format %{ "store_vector $mem,$src\n\t" %}
  ins_encode %{
    switch (Matcher::vector_length_in_bytes(this, $src)) {
      case  4: __ movdl    ($mem$$Address, $src$$XMMRegister); break;
      case  8: __ movq     ($mem$$Address, $src$$XMMRegister); break;
      case 16: __ movdqu   ($mem$$Address, $src$$XMMRegister); break;
      case 32: __ vmovdqu  ($mem$$Address, $src$$XMMRegister); break;
      case 64: __ evmovdqul($mem$$Address, $src$$XMMRegister, Assembler::AVX_512bit); break;
      default: ShouldNotReachHere();
    }
  %}
  ins_pipe( pipe_slow );
%}

// ---------------------------------------- Gather ------------------------------------

// Gather BYTE, SHORT, INT, LONG, FLOAT, DOUBLE

instruct gather(legVec dst, memory mem, legVec idx, rRegP tmp, legVec mask) %{
  predicate(!VM_Version::supports_avx512vl() && !is_subword_type(Matcher::vector_element_basic_type(n)) &&
            Matcher::vector_length_in_bytes(n) <= 32);
  match(Set dst (LoadVectorGather mem idx));
  effect(TEMP dst, TEMP tmp, TEMP mask);
  format %{ "load_vector_gather $dst, $mem, $idx\t! using $tmp and $mask as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    assert(!is_subword_type(elem_bt), "sanity"); // T_INT, T_LONG, T_FLOAT, T_DOUBLE
    __ vpcmpeqd($mask$$XMMRegister, $mask$$XMMRegister, $mask$$XMMRegister, vlen_enc);
    __ lea($tmp$$Register, $mem$$Address);
    __ vgather(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$XMMRegister, $mask$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}


instruct evgather(vec dst, memory mem, vec idx, rRegP tmp, kReg ktmp) %{
  predicate((VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64) &&
            !is_subword_type(Matcher::vector_element_basic_type(n)));
  match(Set dst (LoadVectorGather mem idx));
  effect(TEMP dst, TEMP tmp, TEMP ktmp);
  format %{ "load_vector_gather $dst, $mem, $idx\t! using $tmp and ktmp as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ kxnorwl($ktmp$$KRegister, $ktmp$$KRegister, $ktmp$$KRegister);
    __ lea($tmp$$Register, $mem$$Address);
    __ evgather(elem_bt, $dst$$XMMRegister, $ktmp$$KRegister, $tmp$$Register, $idx$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct evgather_masked(vec dst, memory mem, vec idx, kReg mask, kReg ktmp, rRegP tmp) %{
  predicate((VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64) &&
            !is_subword_type(Matcher::vector_element_basic_type(n)));
  match(Set dst (LoadVectorGatherMasked mem (Binary idx mask)));
  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp);
  format %{ "load_vector_gather_masked $dst, $mem, $idx, $mask\t! using $tmp and ktmp as TEMP" %}
  ins_encode %{
    assert(UseAVX > 2, "sanity");
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    assert(!is_subword_type(elem_bt), "sanity"); // T_INT, T_LONG, T_FLOAT, T_DOUBLE
    // Note: Since gather instruction partially updates the opmask register used
    // for predication hense moving mask operand to a temporary.
    __ kmovwl($ktmp$$KRegister, $mask$$KRegister);
    __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
    __ lea($tmp$$Register, $mem$$Address);
    __ evgather(elem_bt, $dst$$XMMRegister, $ktmp$$KRegister, $tmp$$Register, $idx$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vgather_subwordLE8B(vec dst, memory mem, rRegP idx_base, rRegP tmp, rRegI rtmp) %{
  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);
  match(Set dst (LoadVectorGather mem idx_base));
  effect(TEMP tmp, TEMP rtmp);
  format %{ "vector_gatherLE8 $dst, $mem, $idx_base\t! using $tmp and $rtmp as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ lea($tmp$$Register, $mem$$Address);
    __ vgather8b(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, $rtmp$$Register, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vgather_subwordGT8B(vec dst, memory mem, rRegP idx_base, rRegP tmp, rRegP idx_base_temp,
                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length, rFlagsReg cr) %{
  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);
  match(Set dst (LoadVectorGather mem idx_base));
  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length, KILL cr);
  format %{ "vector_gatherGT8 $dst, $mem, $idx_base\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    int vector_len = Matcher::vector_length(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ lea($tmp$$Register, $mem$$Address);
    __ movptr($idx_base_temp$$Register, $idx_base$$Register);
    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, $xtmp1$$XMMRegister,
                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vgather_masked_subwordLE8B_avx3(vec dst, memory mem, rRegP idx_base, kReg mask, rRegL mask_idx, rRegP tmp, rRegI rtmp, rRegL rtmp2, rFlagsReg cr) %{
  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);
  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base mask)));
  effect(TEMP mask_idx, TEMP tmp, TEMP rtmp, TEMP rtmp2, KILL cr);
  format %{ "vector_masked_gatherLE8 $dst, $mem, $idx_base, $mask\t! using $mask_idx, $tmp, $rtmp and $rtmp2 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ xorq($mask_idx$$Register, $mask_idx$$Register);
    __ lea($tmp$$Register, $mem$$Address);
    __ kmovql($rtmp2$$Register, $mask$$KRegister);
    __ vgather8b_masked(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, $rtmp2$$Register, $mask_idx$$Register, $rtmp$$Register, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vgather_masked_subwordGT8B_avx3(vec dst, memory mem, rRegP idx_base, kReg mask, rRegP tmp, rRegP idx_base_temp,
                                         vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegL rtmp2, rRegL mask_idx, rRegI length, rFlagsReg cr) %{
  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);
  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base mask)));
  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP mask_idx, TEMP length, KILL cr);
  format %{ "vector_gatherGT8_masked $dst, $mem, $idx_base, $mask\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $mask_idx and $length as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    int vector_len = Matcher::vector_length(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ xorq($mask_idx$$Register, $mask_idx$$Register);
    __ lea($tmp$$Register, $mem$$Address);
    __ movptr($idx_base_temp$$Register, $idx_base$$Register);
    __ kmovql($rtmp2$$Register, $mask$$KRegister);
    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $rtmp2$$Register, $xtmp1$$XMMRegister,
                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $mask_idx$$Register, $length$$Register, vector_len, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vgather_masked_subwordLE8B_avx2(vec dst, memory mem, rRegP idx_base, vec mask, rRegI mask_idx, rRegP tmp, rRegI rtmp, rRegI rtmp2, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);
  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base mask)));
  effect(TEMP mask_idx, TEMP tmp, TEMP rtmp, TEMP rtmp2, KILL cr);
  format %{ "vector_masked_gatherLE8 $dst, $mem, $idx_base, $mask\t! using $mask_idx, $tmp, $rtmp and $rtmp2 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ lea($tmp$$Register, $mem$$Address);
    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);
    if (elem_bt == T_SHORT) {
      __ movl($mask_idx$$Register, 0x55555555);
      __ pextl($rtmp2$$Register, $rtmp2$$Register, $mask_idx$$Register);
    }
    __ xorl($mask_idx$$Register, $mask_idx$$Register);
    __ vgather8b_masked(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, $rtmp2$$Register, $mask_idx$$Register, $rtmp$$Register, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vgather_masked_subwordGT8B_avx2(vec dst, memory mem, rRegP idx_base, vec mask, rRegP tmp, rRegP idx_base_temp,
                                         vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI rtmp2, rRegI mask_idx, rRegI length, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);
  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base mask)));
  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP mask_idx, TEMP length, KILL cr);
  format %{ "vector_gatherGT8_masked $dst, $mem, $idx_base, $mask\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $mask_idx and $length as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    int vector_len = Matcher::vector_length(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ lea($tmp$$Register, $mem$$Address);
    __ movptr($idx_base_temp$$Register, $idx_base$$Register);
    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);
    if (elem_bt == T_SHORT) {
      __ movl($mask_idx$$Register, 0x55555555);
      __ pextl($rtmp2$$Register, $rtmp2$$Register, $mask_idx$$Register);
    }
    __ xorl($mask_idx$$Register, $mask_idx$$Register);
    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $rtmp2$$Register, $xtmp1$$XMMRegister,
                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $mask_idx$$Register, $length$$Register, vector_len, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// ====================Scatter=======================================

// Scatter INT, LONG, FLOAT, DOUBLE

instruct scatter(memory mem, vec src, vec idx, rRegP tmp, kReg ktmp) %{
  predicate(UseAVX > 2);
  match(Set mem (StoreVectorScatter mem (Binary src idx)));
  effect(TEMP tmp, TEMP ktmp);
  format %{ "store_vector_scatter $mem, $idx, $src\t! using k2 and $tmp as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType elem_bt = Matcher::vector_element_basic_type(this, $src);

    assert(Matcher::vector_length_in_bytes(this, $src) >= 16, "sanity");
    assert(!is_subword_type(elem_bt), "sanity"); // T_INT, T_LONG, T_FLOAT, T_DOUBLE

    __ kmovwl($ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), noreg);
    __ lea($tmp$$Register, $mem$$Address);
    __ evscatter(elem_bt, $tmp$$Register, $idx$$XMMRegister, $ktmp$$KRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct scatter_masked(memory mem, vec src, vec idx, kReg mask, kReg ktmp, rRegP tmp) %{
  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx mask))));
  effect(TEMP tmp, TEMP ktmp);
  format %{ "store_vector_scatter_masked $mem, $idx, $src, $mask\t!" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType elem_bt = Matcher::vector_element_basic_type(this, $src);
    assert(Matcher::vector_length_in_bytes(this, $src) >= 16, "sanity");
    assert(!is_subword_type(elem_bt), "sanity"); // T_INT, T_LONG, T_FLOAT, T_DOUBLE
    // Note: Since scatter instruction partially updates the opmask register used
    // for predication hense moving mask operand to a temporary.
    __ kmovwl($ktmp$$KRegister, $mask$$KRegister);
    __ lea($tmp$$Register, $mem$$Address);
    __ evscatter(elem_bt, $tmp$$Register, $idx$$XMMRegister, $ktmp$$KRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// ====================REPLICATE=======================================

// Replicate byte scalar to be vector
instruct vReplB_reg(vec dst, rRegI src) %{
  predicate(Matcher::vector_element_basic_type(n) == T_BYTE);
  match(Set dst (Replicate src));
  format %{ "replicateB $dst,$src" %}
  ins_encode %{
    uint vlen = Matcher::vector_length(this);
    if (UseAVX >= 2) {
      int vlen_enc = vector_length_encoding(this);
      if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for <512bit operands
        assert(VM_Version::supports_avx512bw(), "required"); // 512-bit byte vectors assume AVX512BW
        __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);
      } else {
        __ movdl($dst$$XMMRegister, $src$$Register);
        __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
      }
    } else {
       assert(UseAVX < 2, "");
      __ movdl($dst$$XMMRegister, $src$$Register);
      __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
      __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
      if (vlen >= 16) {
        assert(vlen == 16, "");
        __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
      }
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct ReplB_mem(vec dst, memory mem) %{
  predicate(UseAVX >= 2 && Matcher::vector_element_basic_type(n) == T_BYTE);
  match(Set dst (Replicate (LoadB mem)));
  format %{ "replicateB $dst,$mem" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// ====================ReplicateS=======================================

instruct vReplS_reg(vec dst, rRegI src) %{
  predicate(Matcher::vector_element_basic_type(n) == T_SHORT);
  match(Set dst (Replicate src));
  format %{ "replicateS $dst,$src" %}
  ins_encode %{
    uint vlen = Matcher::vector_length(this);
    int vlen_enc = vector_length_encoding(this);
    if (UseAVX >= 2) {
      if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for <512bit operands
        assert(VM_Version::supports_avx512bw(), "required"); // 512-bit short vectors assume AVX512BW
        __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);
      } else {
        __ movdl($dst$$XMMRegister, $src$$Register);
        __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
      }
    } else {
      assert(UseAVX < 2, "");
      __ movdl($dst$$XMMRegister, $src$$Register);
      __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
      if (vlen >= 8) {
        assert(vlen == 8, "");
        __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
      }
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct ReplHF_imm(vec dst, immH con, rRegI rtmp) %{
  match(Set dst (Replicate con));
  effect(TEMP rtmp);
  format %{ "replicateHF $dst, $con \t! using $rtmp as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    assert(VM_Version::supports_avx512_fp16() && bt == T_SHORT, "");
    __ movl($rtmp$$Register, $con$$constant);
    __ evpbroadcastw($dst$$XMMRegister, $rtmp$$Register, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct ReplHF_reg(vec dst, regF src, rRegI rtmp) %{
  predicate(VM_Version::supports_avx512_fp16() && Matcher::vector_element_basic_type(n) == T_SHORT);
  match(Set dst (Replicate src));
  effect(TEMP rtmp);
  format %{ "replicateHF $dst, $src \t! using $rtmp as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vmovw($rtmp$$Register, $src$$XMMRegister);
    __ evpbroadcastw($dst$$XMMRegister, $rtmp$$Register, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct ReplS_mem(vec dst, memory mem) %{
  predicate(UseAVX >= 2 && Matcher::vector_element_basic_type(n) == T_SHORT);
  match(Set dst (Replicate (LoadS mem)));
  format %{ "replicateS $dst,$mem" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// ====================ReplicateI=======================================

instruct ReplI_reg(vec dst, rRegI src) %{
  predicate(Matcher::vector_element_basic_type(n) == T_INT);
  match(Set dst (Replicate src));
  format %{ "replicateI $dst,$src" %}
  ins_encode %{
    uint vlen = Matcher::vector_length(this);
    int vlen_enc = vector_length_encoding(this);
    if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
      __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vlen_enc);
    } else if (VM_Version::supports_avx2()) {
      __ movdl($dst$$XMMRegister, $src$$Register);
      __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
    } else {
      __ movdl($dst$$XMMRegister, $src$$Register);
      __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct ReplI_mem(vec dst, memory mem) %{
  predicate(Matcher::vector_element_basic_type(n) == T_INT);
  match(Set dst (Replicate (LoadI mem)));
  format %{ "replicateI $dst,$mem" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    if (VM_Version::supports_avx2()) {
      __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vlen_enc);
    } else if (VM_Version::supports_avx()) {
      __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vlen_enc);
    } else {
      __ movdl($dst$$XMMRegister, $mem$$Address);
      __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct ReplI_imm(vec dst, immI con) %{
  predicate(Matcher::is_non_long_integral_vector(n));
  match(Set dst (Replicate con));
  format %{ "replicateI $dst,$con" %}
  ins_encode %{
    InternalAddress addr = $constantaddress(vreplicate_imm(Matcher::vector_element_basic_type(this), $con$$constant,
                                                           (VM_Version::supports_sse3() ? (VM_Version::supports_avx() ? 4 : 8) : 16) /
                                                                   type2aelembytes(Matcher::vector_element_basic_type(this))));
    BasicType bt = Matcher::vector_element_basic_type(this);
    int vlen = Matcher::vector_length_in_bytes(this);
    __ load_constant_vector(bt, $dst$$XMMRegister, addr, vlen);
  %}
  ins_pipe( pipe_slow );
%}

// Replicate scalar zero to be vector
instruct ReplI_zero(vec dst, immI_0 zero) %{
  predicate(Matcher::is_non_long_integral_vector(n));
  match(Set dst (Replicate zero));
  format %{ "replicateI $dst,$zero" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    if (VM_Version::supports_evex() && !VM_Version::supports_avx512vl()) {
      __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
    } else {
      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
    }
  %}
  ins_pipe( fpu_reg_reg );
%}

instruct ReplI_M1(vec dst, immI_M1 con) %{
  predicate(Matcher::is_non_long_integral_vector(n));
  match(Set dst (Replicate con));
  format %{ "vallones $dst" %}
  ins_encode %{
    int vector_len = vector_length_encoding(this);
    __ vallones($dst$$XMMRegister, vector_len);
  %}
  ins_pipe( pipe_slow );
%}

// ====================ReplicateL=======================================

// Replicate long (8 byte) scalar to be vector
instruct ReplL_reg(vec dst, rRegL src) %{
  predicate(Matcher::vector_element_basic_type(n) == T_LONG);
  match(Set dst (Replicate src));
  format %{ "replicateL $dst,$src" %}
  ins_encode %{
    int vlen = Matcher::vector_length(this);
    int vlen_enc = vector_length_encoding(this);
    if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
      __ evpbroadcastq($dst$$XMMRegister, $src$$Register, vlen_enc);
    } else if (VM_Version::supports_avx2()) {
      __ movdq($dst$$XMMRegister, $src$$Register);
      __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
    } else {
      __ movdq($dst$$XMMRegister, $src$$Register);
      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct ReplL_mem(vec dst, memory mem) %{
  predicate(Matcher::vector_element_basic_type(n) == T_LONG);
  match(Set dst (Replicate (LoadL mem)));
  format %{ "replicateL $dst,$mem" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    if (VM_Version::supports_avx2()) {
      __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);
    } else if (VM_Version::supports_sse3()) {
      __ movddup($dst$$XMMRegister, $mem$$Address);
    } else {
      __ movq($dst$$XMMRegister, $mem$$Address);
      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
    }
  %}
  ins_pipe( pipe_slow );
%}

// Replicate long (8 byte) scalar immediate to be vector by loading from const table.
instruct ReplL_imm(vec dst, immL con) %{
  predicate(Matcher::vector_element_basic_type(n) == T_LONG);
  match(Set dst (Replicate con));
  format %{ "replicateL $dst,$con" %}
  ins_encode %{
    InternalAddress addr = $constantaddress(vreplicate_imm(T_LONG, $con$$constant, VM_Version::supports_sse3() ? 1 : 2));
    int vlen = Matcher::vector_length_in_bytes(this);
    __ load_constant_vector(T_LONG, $dst$$XMMRegister, addr, vlen);
  %}
  ins_pipe( pipe_slow );
%}

instruct ReplL_zero(vec dst, immL0 zero) %{
  predicate(Matcher::vector_element_basic_type(n) == T_LONG);
  match(Set dst (Replicate zero));
  format %{ "replicateL $dst,$zero" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    if (VM_Version::supports_evex() && !VM_Version::supports_avx512vl()) {
      __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
    } else {
      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
    }
  %}
  ins_pipe( fpu_reg_reg );
%}

instruct ReplL_M1(vec dst, immL_M1 con) %{
  predicate(Matcher::vector_element_basic_type(n) == T_LONG);
  match(Set dst (Replicate con));
  format %{ "vallones $dst" %}
  ins_encode %{
    int vector_len = vector_length_encoding(this);
    __ vallones($dst$$XMMRegister, vector_len);
  %}
  ins_pipe( pipe_slow );
%}

// ====================ReplicateF=======================================

instruct vReplF_reg(vec dst, vlRegF src) %{
  predicate(UseAVX > 0 && Matcher::vector_element_basic_type(n) == T_FLOAT);
  match(Set dst (Replicate src));
  format %{ "replicateF $dst,$src" %}
  ins_encode %{
    uint vlen = Matcher::vector_length(this);
    int vlen_enc = vector_length_encoding(this);
    if (vlen <= 4) {
      __ vpermilps($dst$$XMMRegister, $src$$XMMRegister, 0x00, Assembler::AVX_128bit);
    } else if (VM_Version::supports_avx2()) {
      __ vbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vlen_enc); // reg-to-reg variant requires AVX2
    } else {
      assert(vlen == 8, "sanity");
      __ vpermilps($dst$$XMMRegister, $src$$XMMRegister, 0x00, Assembler::AVX_128bit);
      __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct ReplF_reg(vec dst, vlRegF src) %{
  predicate(UseAVX == 0 && Matcher::vector_element_basic_type(n) == T_FLOAT);
  match(Set dst (Replicate src));
  format %{ "replicateF $dst,$src" %}
  ins_encode %{
    __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
  %}
  ins_pipe( pipe_slow );
%}

instruct ReplF_mem(vec dst, memory mem) %{
  predicate(UseAVX > 0 && Matcher::vector_element_basic_type(n) == T_FLOAT);
  match(Set dst (Replicate (LoadF mem)));
  format %{ "replicateF $dst,$mem" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Replicate float scalar immediate to be vector by loading from const table.
instruct ReplF_imm(vec dst, immF con) %{
  predicate(Matcher::vector_element_basic_type(n) == T_FLOAT);
  match(Set dst (Replicate con));
  format %{ "replicateF $dst,$con" %}
  ins_encode %{
    InternalAddress addr = $constantaddress(vreplicate_imm(T_FLOAT, $con$$constant,
                                                           VM_Version::supports_sse3() ? (VM_Version::supports_avx() ? 1 : 2) : 4));
    int vlen = Matcher::vector_length_in_bytes(this);
    __ load_constant_vector(T_FLOAT, $dst$$XMMRegister, addr, vlen);
  %}
  ins_pipe( pipe_slow );
%}

instruct ReplF_zero(vec dst, immF0 zero) %{
  predicate(Matcher::vector_element_basic_type(n) == T_FLOAT);
  match(Set dst (Replicate zero));
  format %{ "replicateF $dst,$zero" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    if (VM_Version::supports_evex() && !VM_Version::supports_avx512vldq()) {
      __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
    } else {
      __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
    }
  %}
  ins_pipe( fpu_reg_reg );
%}

// ====================ReplicateD=======================================

// Replicate double (8 bytes) scalar to be vector
instruct vReplD_reg(vec dst, vlRegD src) %{
  predicate(UseSSE >= 3 && Matcher::vector_element_basic_type(n) == T_DOUBLE);
  match(Set dst (Replicate src));
  format %{ "replicateD $dst,$src" %}
  ins_encode %{
    uint vlen = Matcher::vector_length(this);
    int vlen_enc = vector_length_encoding(this);
    if (vlen <= 2) {
      __ movddup($dst$$XMMRegister, $src$$XMMRegister);
    } else if (VM_Version::supports_avx2()) {
      __ vbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc); // reg-to-reg variant requires AVX2
    } else {
      assert(vlen == 4, "sanity");
      __ movddup($dst$$XMMRegister, $src$$XMMRegister);
      __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct ReplD_reg(vec dst, vlRegD src) %{
  predicate(UseSSE < 3 && Matcher::vector_element_basic_type(n) == T_DOUBLE);
  match(Set dst (Replicate src));
  format %{ "replicateD $dst,$src" %}
  ins_encode %{
    __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
  %}
  ins_pipe( pipe_slow );
%}

instruct ReplD_mem(vec dst, memory mem) %{
  predicate(UseSSE >= 3 && Matcher::vector_element_basic_type(n) == T_DOUBLE);
  match(Set dst (Replicate (LoadD mem)));
  format %{ "replicateD $dst,$mem" %}
  ins_encode %{
    if (Matcher::vector_length(this) >= 4) {
      int vlen_enc = vector_length_encoding(this);
      __ vbroadcastsd($dst$$XMMRegister, $mem$$Address, vlen_enc);
    } else {
      __ movddup($dst$$XMMRegister, $mem$$Address);
    }
  %}
  ins_pipe( pipe_slow );
%}

// Replicate double (8 byte) scalar immediate to be vector by loading from const table.
instruct ReplD_imm(vec dst, immD con) %{
  predicate(Matcher::vector_element_basic_type(n) == T_DOUBLE);
  match(Set dst (Replicate con));
  format %{ "replicateD $dst,$con" %}
  ins_encode %{
    InternalAddress addr = $constantaddress(vreplicate_imm(T_DOUBLE, $con$$constant, VM_Version::supports_sse3() ? 1 : 2));
    int vlen = Matcher::vector_length_in_bytes(this);
    __ load_constant_vector(T_DOUBLE, $dst$$XMMRegister, addr, vlen);
  %}
  ins_pipe( pipe_slow );
%}

instruct ReplD_zero(vec dst, immD0 zero) %{
  predicate(Matcher::vector_element_basic_type(n) == T_DOUBLE);
  match(Set dst (Replicate zero));
  format %{ "replicateD $dst,$zero" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    if (VM_Version::supports_evex() && !VM_Version::supports_avx512vldq()) {
      __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
    } else {
      __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
    }
  %}
  ins_pipe( fpu_reg_reg );
%}

// ====================VECTOR INSERT=======================================

instruct insert(vec dst, rRegI val, immU8 idx) %{
  predicate(Matcher::vector_length_in_bytes(n) < 32);
  match(Set dst (VectorInsert (Binary dst val) idx));
  format %{ "vector_insert $dst,$val,$idx" %}
  ins_encode %{
    assert(UseSSE >= 4, "required");
    assert(Matcher::vector_length_in_bytes(this) >= 8, "required");

    BasicType elem_bt = Matcher::vector_element_basic_type(this);

    assert(is_integral_type(elem_bt), "");
    assert($idx$$constant < (int)Matcher::vector_length(this), "out of bounds");

    __ insert(elem_bt, $dst$$XMMRegister, $val$$Register, $idx$$constant);
  %}
  ins_pipe( pipe_slow );
%}

instruct insert32(vec dst, vec src, rRegI val, immU8 idx, vec vtmp) %{
  predicate(Matcher::vector_length_in_bytes(n) == 32);
  match(Set dst (VectorInsert (Binary src val) idx));
  effect(TEMP vtmp);
  format %{ "vector_insert $dst,$src,$val,$idx\t!using $vtmp as TEMP" %}
  ins_encode %{
    int vlen_enc = Assembler::AVX_256bit;
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    int elem_per_lane = 16/type2aelembytes(elem_bt);
    int log2epr = log2(elem_per_lane);

    assert(is_integral_type(elem_bt), "sanity");
    assert($idx$$constant < (int)Matcher::vector_length(this), "out of bounds");

    uint x_idx = $idx$$constant & right_n_bits(log2epr);
    uint y_idx = ($idx$$constant >> log2epr) & 1;
    __ vextracti128($vtmp$$XMMRegister, $src$$XMMRegister, y_idx);
    __ vinsert(elem_bt, $vtmp$$XMMRegister, $vtmp$$XMMRegister, $val$$Register, x_idx);
    __ vinserti128($dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister, y_idx);
  %}
  ins_pipe( pipe_slow );
%}

instruct insert64(vec dst, vec src, rRegI val, immU8 idx, legVec vtmp) %{
  predicate(Matcher::vector_length_in_bytes(n) == 64);
  match(Set dst (VectorInsert (Binary src val) idx));
  effect(TEMP vtmp);
  format %{ "vector_insert $dst,$src,$val,$idx\t!using $vtmp as TEMP" %}
  ins_encode %{
    assert(UseAVX > 2, "sanity");

    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    int elem_per_lane = 16/type2aelembytes(elem_bt);
    int log2epr = log2(elem_per_lane);

    assert(is_integral_type(elem_bt), "");
    assert($idx$$constant < (int)Matcher::vector_length(this), "out of bounds");

    uint x_idx = $idx$$constant & right_n_bits(log2epr);
    uint y_idx = ($idx$$constant >> log2epr) & 3;
    __ vextracti32x4($vtmp$$XMMRegister, $src$$XMMRegister, y_idx);
    __ vinsert(elem_bt, $vtmp$$XMMRegister, $vtmp$$XMMRegister, $val$$Register, x_idx);
    __ vinserti32x4($dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister, y_idx);
  %}
  ins_pipe( pipe_slow );
%}

instruct insert2L(vec dst, rRegL val, immU8 idx) %{
  predicate(Matcher::vector_length(n) == 2);
  match(Set dst (VectorInsert (Binary dst val) idx));
  format %{ "vector_insert $dst,$val,$idx" %}
  ins_encode %{
    assert(UseSSE >= 4, "required");
    assert(Matcher::vector_element_basic_type(this) == T_LONG, "");
    assert($idx$$constant < (int)Matcher::vector_length(this), "out of bounds");

    __ pinsrq($dst$$XMMRegister, $val$$Register, $idx$$constant);
  %}
  ins_pipe( pipe_slow );
%}

instruct insert4L(vec dst, vec src, rRegL val, immU8 idx, vec vtmp) %{
  predicate(Matcher::vector_length(n) == 4);
  match(Set dst (VectorInsert (Binary src val) idx));
  effect(TEMP vtmp);
  format %{ "vector_insert $dst,$src,$val,$idx\t!using $vtmp as TEMP" %}
  ins_encode %{
    assert(Matcher::vector_element_basic_type(this) == T_LONG, "");
    assert($idx$$constant < (int)Matcher::vector_length(this), "out of bounds");

    uint x_idx = $idx$$constant & right_n_bits(1);
    uint y_idx = ($idx$$constant >> 1) & 1;
    int vlen_enc = Assembler::AVX_256bit;
    __ vextracti128($vtmp$$XMMRegister, $src$$XMMRegister, y_idx);
    __ vpinsrq($vtmp$$XMMRegister, $vtmp$$XMMRegister, $val$$Register, x_idx);
    __ vinserti128($dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister, y_idx);
  %}
  ins_pipe( pipe_slow );
%}

instruct insert8L(vec dst, vec src, rRegL val, immU8 idx, legVec vtmp) %{
  predicate(Matcher::vector_length(n) == 8);
  match(Set dst (VectorInsert (Binary src val) idx));
  effect(TEMP vtmp);
  format %{ "vector_insert $dst,$src,$val,$idx\t!using $vtmp as TEMP" %}
  ins_encode %{
    assert(Matcher::vector_element_basic_type(this) == T_LONG, "sanity");
    assert($idx$$constant < (int)Matcher::vector_length(this), "out of bounds");

    uint x_idx = $idx$$constant & right_n_bits(1);
    uint y_idx = ($idx$$constant >> 1) & 3;
    __ vextracti32x4($vtmp$$XMMRegister, $src$$XMMRegister, y_idx);
    __ vpinsrq($vtmp$$XMMRegister, $vtmp$$XMMRegister, $val$$Register, x_idx);
    __ vinserti32x4($dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister, y_idx);
  %}
  ins_pipe( pipe_slow );
%}

instruct insertF(vec dst, regF val, immU8 idx) %{
  predicate(Matcher::vector_length(n) < 8);
  match(Set dst (VectorInsert (Binary dst val) idx));
  format %{ "vector_insert $dst,$val,$idx" %}
  ins_encode %{
    assert(UseSSE >= 4, "sanity");

    assert(Matcher::vector_element_basic_type(this) == T_FLOAT, "sanity");
    assert($idx$$constant < (int)Matcher::vector_length(this), "out of bounds");

    uint x_idx = $idx$$constant & right_n_bits(2);
    __ insertps($dst$$XMMRegister, $val$$XMMRegister, x_idx << 4);
  %}
  ins_pipe( pipe_slow );
%}

instruct vinsertF(vec dst, vec src, regF val, immU8 idx, vec vtmp) %{
  predicate(Matcher::vector_length(n) >= 8);
  match(Set dst (VectorInsert (Binary src val) idx));
  effect(TEMP vtmp);
  format %{ "vector_insert $dst,$src,$val,$idx\t!using $vtmp as TEMP" %}
  ins_encode %{
    assert(Matcher::vector_element_basic_type(this) == T_FLOAT, "sanity");
    assert($idx$$constant < (int)Matcher::vector_length(this), "out of bounds");

    int vlen = Matcher::vector_length(this);
    uint x_idx = $idx$$constant & right_n_bits(2);
    if (vlen == 8) {
      uint y_idx = ($idx$$constant >> 2) & 1;
      int vlen_enc = Assembler::AVX_256bit;
      __ vextracti128($vtmp$$XMMRegister, $src$$XMMRegister, y_idx);
      __ vinsertps($vtmp$$XMMRegister, $vtmp$$XMMRegister, $val$$XMMRegister, x_idx << 4);
      __ vinserti128($dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister, y_idx);
    } else {
      assert(vlen == 16, "sanity");
      uint y_idx = ($idx$$constant >> 2) & 3;
      __ vextracti32x4($vtmp$$XMMRegister, $src$$XMMRegister, y_idx);
      __ vinsertps($vtmp$$XMMRegister, $vtmp$$XMMRegister, $val$$XMMRegister, x_idx << 4);
      __ vinserti32x4($dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister, y_idx);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct insert2D(vec dst, regD val, immU8 idx, rRegL tmp) %{
  predicate(Matcher::vector_length(n) == 2);
  match(Set dst (VectorInsert (Binary dst val) idx));
  effect(TEMP tmp);
  format %{ "vector_insert $dst,$val,$idx\t!using $tmp as TEMP" %}
  ins_encode %{
    assert(UseSSE >= 4, "sanity");
    assert(Matcher::vector_element_basic_type(this) == T_DOUBLE, "sanity");
    assert($idx$$constant < (int)Matcher::vector_length(this), "out of bounds");

    __ movq($tmp$$Register, $val$$XMMRegister);
    __ pinsrq($dst$$XMMRegister, $tmp$$Register, $idx$$constant);
  %}
  ins_pipe( pipe_slow );
%}

instruct insert4D(vec dst, vec src, regD val, immU8 idx, rRegL tmp, vec vtmp) %{
  predicate(Matcher::vector_length(n) == 4);
  match(Set dst (VectorInsert (Binary src val) idx));
  effect(TEMP vtmp, TEMP tmp);
  format %{ "vector_insert $dst,$src,$val,$idx\t!using $tmp, $vtmp as TEMP" %}
  ins_encode %{
    assert(Matcher::vector_element_basic_type(this) == T_DOUBLE, "sanity");
    assert($idx$$constant < (int)Matcher::vector_length(this), "out of bounds");

    uint x_idx = $idx$$constant & right_n_bits(1);
    uint y_idx = ($idx$$constant >> 1) & 1;
    int vlen_enc = Assembler::AVX_256bit;
    __ movq($tmp$$Register, $val$$XMMRegister);
    __ vextracti128($vtmp$$XMMRegister, $src$$XMMRegister, y_idx);
    __ vpinsrq($vtmp$$XMMRegister, $vtmp$$XMMRegister, $tmp$$Register, x_idx);
    __ vinserti128($dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister, y_idx);
  %}
  ins_pipe( pipe_slow );
%}

instruct insert8D(vec dst, vec src, regD val, immI idx, rRegL tmp, legVec vtmp) %{
  predicate(Matcher::vector_length(n) == 8);
  match(Set dst (VectorInsert (Binary src val) idx));
  effect(TEMP tmp, TEMP vtmp);
  format %{ "vector_insert $dst,$src,$val,$idx\t!using $vtmp as TEMP" %}
  ins_encode %{
    assert(Matcher::vector_element_basic_type(this) == T_DOUBLE, "sanity");
    assert($idx$$constant < (int)Matcher::vector_length(this), "out of bounds");

    uint x_idx = $idx$$constant & right_n_bits(1);
    uint y_idx = ($idx$$constant >> 1) & 3;
    __ movq($tmp$$Register, $val$$XMMRegister);
    __ vextracti32x4($vtmp$$XMMRegister, $src$$XMMRegister, y_idx);
    __ vpinsrq($vtmp$$XMMRegister, $vtmp$$XMMRegister, $tmp$$Register, x_idx);
    __ vinserti32x4($dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister, y_idx);
  %}
  ins_pipe( pipe_slow );
%}

// ====================REDUCTION ARITHMETIC=======================================

// =======================Int Reduction==========================================

instruct reductionI(rRegI dst, rRegI src1, legVec src2, legVec vtmp1, legVec vtmp2) %{
  predicate(Matcher::vector_element_basic_type(n->in(2)) == T_INT); // src2
  match(Set dst (AddReductionVI src1 src2));
  match(Set dst (MulReductionVI src1 src2));
  match(Set dst (AndReductionV  src1 src2));
  match(Set dst ( OrReductionV  src1 src2));
  match(Set dst (XorReductionV  src1 src2));
  match(Set dst (MinReductionV  src1 src2));
  match(Set dst (MaxReductionV  src1 src2));
  effect(TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_reduction_int $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ reduceI(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

// =======================Long Reduction==========================================

instruct reductionL(rRegL dst, rRegL src1, legVec src2, legVec vtmp1, legVec vtmp2) %{
  predicate(Matcher::vector_element_basic_type(n->in(2)) == T_LONG && !VM_Version::supports_avx512dq());
  match(Set dst (AddReductionVL src1 src2));
  match(Set dst (MulReductionVL src1 src2));
  match(Set dst (AndReductionV  src1 src2));
  match(Set dst ( OrReductionV  src1 src2));
  match(Set dst (XorReductionV  src1 src2));
  match(Set dst (MinReductionV  src1 src2));
  match(Set dst (MaxReductionV  src1 src2));
  effect(TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_reduction_long $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ reduceL(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct reductionL_avx512dq(rRegL dst, rRegL src1, vec src2, vec vtmp1, vec vtmp2) %{
  predicate(Matcher::vector_element_basic_type(n->in(2)) == T_LONG && VM_Version::supports_avx512dq());
  match(Set dst (AddReductionVL src1 src2));
  match(Set dst (MulReductionVL src1 src2));
  match(Set dst (AndReductionV  src1 src2));
  match(Set dst ( OrReductionV  src1 src2));
  match(Set dst (XorReductionV  src1 src2));
  match(Set dst (MinReductionV  src1 src2));
  match(Set dst (MaxReductionV  src1 src2));
  effect(TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_reduction_long $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ reduceL(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

// =======================Float Reduction==========================================

instruct reductionF128(regF dst, vec src, vec vtmp) %{
  predicate(n->as_Reduction()->requires_strict_order() && Matcher::vector_length(n->in(2)) <= 4); // src
  match(Set dst (AddReductionVF dst src));
  match(Set dst (MulReductionVF dst src));
  effect(TEMP dst, TEMP vtmp);
  format %{ "vector_reduction_float  $dst,$src ; using $vtmp as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src);
    __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct reduction8F(regF dst, vec src, vec vtmp1, vec vtmp2) %{
  predicate(n->as_Reduction()->requires_strict_order() && Matcher::vector_length(n->in(2)) == 8); // src
  match(Set dst (AddReductionVF dst src));
  match(Set dst (MulReductionVF dst src));
  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_reduction_float $dst,$src ; using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src);
    __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct reduction16F(regF dst, legVec src, legVec vtmp1, legVec vtmp2) %{
  predicate(n->as_Reduction()->requires_strict_order() && Matcher::vector_length(n->in(2)) == 16); // src
  match(Set dst (AddReductionVF dst src));
  match(Set dst (MulReductionVF dst src));
  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_reduction_float $dst,$src ; using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src);
    __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}


instruct unordered_reduction2F(regF dst, regF src1, vec src2) %{
  // Non-strictly ordered floating-point add/mul reduction for floats. This rule is
  // intended for the VectorAPI (which allows for non-strictly ordered add/mul reduction).
  // src1 contains reduction identity
  predicate(!n->as_Reduction()->requires_strict_order() && Matcher::vector_length(n->in(2)) == 2); // src2
  match(Set dst (AddReductionVF src1 src2));
  match(Set dst (MulReductionVF src1 src2));
  effect(TEMP dst);
  format %{ "vector_reduction_float  $dst,$src1,$src2 ;" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ unordered_reduce_fp(opcode, vlen, $dst$$XMMRegister, $src2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct unordered_reduction4F(regF dst, regF src1, vec src2, vec vtmp) %{
  // Non-strictly ordered floating-point add/mul reduction for floats. This rule is
  // intended for the VectorAPI (which allows for non-strictly ordered add/mul reduction).
  // src1 contains reduction identity
  predicate(!n->as_Reduction()->requires_strict_order() && Matcher::vector_length(n->in(2)) == 4); // src2
  match(Set dst (AddReductionVF src1 src2));
  match(Set dst (MulReductionVF src1 src2));
  effect(TEMP dst, TEMP vtmp);
  format %{ "vector_reduction_float  $dst,$src1,$src2 ; using $vtmp as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ unordered_reduce_fp(opcode, vlen, $dst$$XMMRegister, $src2$$XMMRegister, $vtmp$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct unordered_reduction8F(regF dst, regF src1, vec src2, vec vtmp1, vec vtmp2) %{
  // Non-strictly ordered floating-point add/mul reduction for floats. This rule is
  // intended for the VectorAPI (which allows for non-strictly ordered add/mul reduction).
  // src1 contains reduction identity
  predicate(!n->as_Reduction()->requires_strict_order() && Matcher::vector_length(n->in(2)) == 8); // src2
  match(Set dst (AddReductionVF src1 src2));
  match(Set dst (MulReductionVF src1 src2));
  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_reduction_float $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ unordered_reduce_fp(opcode, vlen, $dst$$XMMRegister, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct unordered_reduction16F(regF dst, regF src1, legVec src2, legVec vtmp1, legVec vtmp2) %{
  // Non-strictly ordered floating-point add/mul reduction for floats. This rule is
  // intended for the VectorAPI (which allows for non-strictly ordered add/mul reduction).
  // src1 contains reduction identity
  predicate(!n->as_Reduction()->requires_strict_order() && Matcher::vector_length(n->in(2)) == 16); // src2
  match(Set dst (AddReductionVF src1 src2));
  match(Set dst (MulReductionVF src1 src2));
  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_reduction_float $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ unordered_reduce_fp(opcode, vlen, $dst$$XMMRegister, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

// =======================Double Reduction==========================================

instruct reduction2D(regD dst, vec src, vec vtmp) %{
  predicate(n->as_Reduction()->requires_strict_order() && Matcher::vector_length(n->in(2)) == 2); // src
  match(Set dst (AddReductionVD dst src));
  match(Set dst (MulReductionVD dst src));
  effect(TEMP dst, TEMP vtmp);
  format %{ "vector_reduction_double $dst,$src ; using $vtmp as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src);
    __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister);
%}
  ins_pipe( pipe_slow );
%}

instruct reduction4D(regD dst, vec src, vec vtmp1, vec vtmp2) %{
  predicate(n->as_Reduction()->requires_strict_order() && Matcher::vector_length(n->in(2)) == 4); // src
  match(Set dst (AddReductionVD dst src));
  match(Set dst (MulReductionVD dst src));
  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_reduction_double $dst,$src ; using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src);
    __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct reduction8D(regD dst, legVec src, legVec vtmp1, legVec vtmp2) %{
  predicate(n->as_Reduction()->requires_strict_order() && Matcher::vector_length(n->in(2)) == 8); // src
  match(Set dst (AddReductionVD dst src));
  match(Set dst (MulReductionVD dst src));
  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_reduction_double $dst,$src ; using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src);
    __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct unordered_reduction2D(regD dst, regD src1, vec src2) %{
  // Non-strictly ordered floating-point add/mul reduction for doubles. This rule is
  // intended for the VectorAPI (which allows for non-strictly ordered add/mul reduction).
  // src1 contains reduction identity
  predicate(!n->as_Reduction()->requires_strict_order() && Matcher::vector_length(n->in(2)) == 2); // src2
  match(Set dst (AddReductionVD src1 src2));
  match(Set dst (MulReductionVD src1 src2));
  effect(TEMP dst);
  format %{ "vector_reduction_double $dst,$src1,$src2 ;" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ unordered_reduce_fp(opcode, vlen, $dst$$XMMRegister, $src2$$XMMRegister);
%}
  ins_pipe( pipe_slow );
%}

instruct unordered_reduction4D(regD dst, regD src1, vec src2, vec vtmp) %{
  // Non-strictly ordered floating-point add/mul reduction for doubles. This rule is
  // intended for the VectorAPI (which allows for non-strictly ordered add/mul reduction).
  // src1 contains reduction identity
  predicate(!n->as_Reduction()->requires_strict_order() && Matcher::vector_length(n->in(2)) == 4); // src2
  match(Set dst (AddReductionVD src1 src2));
  match(Set dst (MulReductionVD src1 src2));
  effect(TEMP dst, TEMP vtmp);
  format %{ "vector_reduction_double $dst,$src1,$src2 ; using $vtmp as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ unordered_reduce_fp(opcode, vlen, $dst$$XMMRegister, $src2$$XMMRegister, $vtmp$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct unordered_reduction8D(regD dst, regD src1, legVec src2, legVec vtmp1, legVec vtmp2) %{
  // Non-strictly ordered floating-point add/mul reduction for doubles. This rule is
  // intended for the VectorAPI (which allows for non-strictly ordered add/mul reduction).
  // src1 contains reduction identity
  predicate(!n->as_Reduction()->requires_strict_order() && Matcher::vector_length(n->in(2)) == 8); // src2
  match(Set dst (AddReductionVD src1 src2));
  match(Set dst (MulReductionVD src1 src2));
  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_reduction_double $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ unordered_reduce_fp(opcode, vlen, $dst$$XMMRegister, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

// =======================Byte Reduction==========================================

instruct reductionB(rRegI dst, rRegI src1, legVec src2, legVec vtmp1, legVec vtmp2) %{
  predicate(Matcher::vector_element_basic_type(n->in(2)) == T_BYTE && !VM_Version::supports_avx512bw());
  match(Set dst (AddReductionVI src1 src2));
  match(Set dst (AndReductionV  src1 src2));
  match(Set dst ( OrReductionV  src1 src2));
  match(Set dst (XorReductionV  src1 src2));
  match(Set dst (MinReductionV  src1 src2));
  match(Set dst (MaxReductionV  src1 src2));
  effect(TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_reduction_byte $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ reduceB(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct reductionB_avx512bw(rRegI dst, rRegI src1, vec src2, vec vtmp1, vec vtmp2) %{
  predicate(Matcher::vector_element_basic_type(n->in(2)) == T_BYTE && VM_Version::supports_avx512bw());
  match(Set dst (AddReductionVI src1 src2));
  match(Set dst (AndReductionV  src1 src2));
  match(Set dst ( OrReductionV  src1 src2));
  match(Set dst (XorReductionV  src1 src2));
  match(Set dst (MinReductionV  src1 src2));
  match(Set dst (MaxReductionV  src1 src2));
  effect(TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_reduction_byte $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ reduceB(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

// =======================Short Reduction==========================================

instruct reductionS(rRegI dst, rRegI src1, legVec src2, legVec vtmp1, legVec vtmp2) %{
  predicate(Matcher::vector_element_basic_type(n->in(2)) == T_SHORT); // src2
  match(Set dst (AddReductionVI src1 src2));
  match(Set dst (MulReductionVI src1 src2));
  match(Set dst (AndReductionV  src1 src2));
  match(Set dst ( OrReductionV  src1 src2));
  match(Set dst (XorReductionV  src1 src2));
  match(Set dst (MinReductionV  src1 src2));
  match(Set dst (MaxReductionV  src1 src2));
  effect(TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_reduction_short $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ reduceS(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

// =======================Mul Reduction==========================================

instruct mul_reductionB(rRegI dst, rRegI src1, vec src2, vec vtmp1, vec vtmp2) %{
  predicate(Matcher::vector_element_basic_type(n->in(2)) == T_BYTE &&
            Matcher::vector_length(n->in(2)) <= 32); // src2
  match(Set dst (MulReductionVI src1 src2));
  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_mul_reduction_byte $dst,$src1,$src2; using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ mulreduceB(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct mul_reduction64B(rRegI dst, rRegI src1, legVec src2, legVec vtmp1, legVec vtmp2) %{
  predicate(Matcher::vector_element_basic_type(n->in(2)) == T_BYTE &&
            Matcher::vector_length(n->in(2)) == 64); // src2
  match(Set dst (MulReductionVI src1 src2));
  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_mul_reduction_byte $dst,$src1,$src2; using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ mulreduceB(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

//--------------------Min/Max Float Reduction --------------------
// Float Min Reduction
instruct minmax_reduction2F(legRegF dst, immF src1, legVec src2, legVec tmp, legVec atmp,
                            legVec btmp, legVec xmm_1, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx10_2() && Matcher::vector_element_basic_type(n->in(2)) == T_FLOAT &&
            ((n->Opcode() == Op_MinReductionV && n->in(1)->bottom_type() == TypeF::POS_INF) ||
             (n->Opcode() == Op_MaxReductionV && n->in(1)->bottom_type() == TypeF::NEG_INF)) &&
            Matcher::vector_length(n->in(2)) == 2);
  match(Set dst (MinReductionV src1 src2));
  match(Set dst (MaxReductionV src1 src2));
  effect(TEMP dst, TEMP tmp, TEMP atmp, TEMP btmp, TEMP xmm_1, KILL cr);
  format %{ "vector_minmax2F_reduction $dst,$src1,$src2  ; using $tmp, $atmp, $btmp, $xmm_1 as TEMP" %}
  ins_encode %{
    assert(UseAVX > 0, "sanity");

    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ reduceFloatMinMax(opcode, vlen, false, $dst$$XMMRegister, $src2$$XMMRegister, $tmp$$XMMRegister,
                         $atmp$$XMMRegister, $btmp$$XMMRegister, $xmm_1$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct minmax_reductionF(legRegF dst, immF src1, legVec src2, legVec tmp, legVec atmp,
                           legVec btmp, legVec xmm_0, legVec xmm_1, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx10_2() && Matcher::vector_element_basic_type(n->in(2)) == T_FLOAT &&
            ((n->Opcode() == Op_MinReductionV && n->in(1)->bottom_type() == TypeF::POS_INF) ||
             (n->Opcode() == Op_MaxReductionV && n->in(1)->bottom_type() == TypeF::NEG_INF)) &&
            Matcher::vector_length(n->in(2)) >= 4);
  match(Set dst (MinReductionV src1 src2));
  match(Set dst (MaxReductionV src1 src2));
  effect(TEMP dst, TEMP tmp, TEMP atmp, TEMP btmp, TEMP xmm_0, TEMP xmm_1, KILL cr);
  format %{ "vector_minmaxF_reduction $dst,$src1,$src2  ; using $tmp, $atmp, $btmp, $xmm_0, $xmm_1 as TEMP" %}
  ins_encode %{
    assert(UseAVX > 0, "sanity");

    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ reduceFloatMinMax(opcode, vlen, false, $dst$$XMMRegister, $src2$$XMMRegister, $tmp$$XMMRegister,
                         $atmp$$XMMRegister, $btmp$$XMMRegister, $xmm_0$$XMMRegister, $xmm_1$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct minmax_reduction2F_av(legRegF dst, legVec src, legVec tmp, legVec atmp,
                               legVec btmp, legVec xmm_1, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx10_2() && Matcher::vector_element_basic_type(n->in(2)) == T_FLOAT &&
            Matcher::vector_length(n->in(2)) == 2);
  match(Set dst (MinReductionV dst src));
  match(Set dst (MaxReductionV dst src));
  effect(TEMP dst, TEMP tmp, TEMP atmp, TEMP btmp, TEMP xmm_1, KILL cr);
  format %{ "vector_minmax2F_reduction $dst,$src ; using $tmp, $atmp, $btmp, $xmm_1 as TEMP" %}
  ins_encode %{
    assert(UseAVX > 0, "sanity");

    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src);
    __ reduceFloatMinMax(opcode, vlen, true, $dst$$XMMRegister, $src$$XMMRegister, $tmp$$XMMRegister,
                         $atmp$$XMMRegister, $btmp$$XMMRegister, $xmm_1$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}


instruct minmax_reductionF_av(legRegF dst, legVec src, legVec tmp, legVec atmp, legVec btmp,
                              legVec xmm_0, legVec xmm_1, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx10_2() && Matcher::vector_element_basic_type(n->in(2)) == T_FLOAT &&
            Matcher::vector_length(n->in(2)) >= 4);
  match(Set dst (MinReductionV dst src));
  match(Set dst (MaxReductionV dst src));
  effect(TEMP dst, TEMP tmp, TEMP atmp, TEMP btmp, TEMP xmm_0, TEMP xmm_1, KILL cr);
  format %{ "vector_minmaxF_reduction $dst,$src ; using $tmp, $atmp, $btmp, $xmm_0, $xmm_1 as TEMP" %}
  ins_encode %{
    assert(UseAVX > 0, "sanity");

    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src);
    __ reduceFloatMinMax(opcode, vlen, true, $dst$$XMMRegister, $src$$XMMRegister, $tmp$$XMMRegister,
                         $atmp$$XMMRegister, $btmp$$XMMRegister, $xmm_0$$XMMRegister, $xmm_1$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct minmax_reduction2F_avx10(regF dst, immF src1, vec src2, vec xtmp1) %{
  predicate(VM_Version::supports_avx10_2() && Matcher::vector_element_basic_type(n->in(2)) == T_FLOAT &&
            ((n->Opcode() == Op_MinReductionV && n->in(1)->bottom_type() == TypeF::POS_INF) ||
             (n->Opcode() == Op_MaxReductionV && n->in(1)->bottom_type() == TypeF::NEG_INF)) &&
            Matcher::vector_length(n->in(2)) == 2);
  match(Set dst (MinReductionV src1 src2));
  match(Set dst (MaxReductionV src1 src2));
  effect(TEMP dst, TEMP xtmp1);
  format %{ "vector_minmax_reduction $dst, $src1, $src2 \t; using $xtmp1 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ reduceFloatMinMax(opcode, vlen, false, $dst$$XMMRegister, $src2$$XMMRegister,
                         xnoreg, xnoreg, xnoreg, $xtmp1$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct minmax_reductionF_avx10(regF dst, immF src1, vec src2, vec xtmp1, vec xtmp2) %{
  predicate(VM_Version::supports_avx10_2() && Matcher::vector_element_basic_type(n->in(2)) == T_FLOAT &&
            ((n->Opcode() == Op_MinReductionV && n->in(1)->bottom_type() == TypeF::POS_INF) ||
             (n->Opcode() == Op_MaxReductionV && n->in(1)->bottom_type() == TypeF::NEG_INF)) &&
            Matcher::vector_length(n->in(2)) >= 4);
  match(Set dst (MinReductionV src1 src2));
  match(Set dst (MaxReductionV src1 src2));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2);
  format %{ "vector_minmax_reduction $dst, $src1, $src2 \t; using $xtmp1 and $xtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ reduceFloatMinMax(opcode, vlen, false, $dst$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg,
                         xnoreg, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct minmax_reduction2F_avx10_av(regF dst, vec src, vec xtmp1) %{
  predicate(VM_Version::supports_avx10_2() && Matcher::vector_element_basic_type(n->in(2)) == T_FLOAT &&
            Matcher::vector_length(n->in(2)) == 2);
  match(Set dst (MinReductionV dst src));
  match(Set dst (MaxReductionV dst src));
  effect(TEMP dst, TEMP xtmp1);
  format %{ "vector_minmax2F_reduction $dst, $src \t; using $xtmp1 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src);
    __ reduceFloatMinMax(opcode, vlen, true, $dst$$XMMRegister, $src$$XMMRegister, xnoreg, xnoreg, xnoreg,
                         $xtmp1$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct minmax_reductionF_avx10_av(regF dst, vec src, vec xtmp1, vec xtmp2) %{
  predicate(VM_Version::supports_avx10_2() && Matcher::vector_element_basic_type(n->in(2)) == T_FLOAT &&
            Matcher::vector_length(n->in(2)) >= 4);
  match(Set dst (MinReductionV dst src));
  match(Set dst (MaxReductionV dst src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2);
  format %{ "vector_minmax2F_reduction $dst, $src \t; using $xtmp1 and $xtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src);
    __ reduceFloatMinMax(opcode, vlen, true, $dst$$XMMRegister, $src$$XMMRegister, xnoreg, xnoreg, xnoreg,
                         $xtmp1$$XMMRegister, $xtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

//--------------------Min Double Reduction --------------------
instruct minmax_reduction2D(legRegD dst, immD src1, legVec src2, legVec tmp1, legVec tmp2,
                            legVec tmp3, legVec tmp4, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx10_2() && Matcher::vector_element_basic_type(n->in(2)) == T_DOUBLE &&
            ((n->Opcode() == Op_MinReductionV && n->in(1)->bottom_type() == TypeD::POS_INF) ||
             (n->Opcode() == Op_MaxReductionV && n->in(1)->bottom_type() == TypeD::NEG_INF)) &&
            Matcher::vector_length(n->in(2)) == 2);
  match(Set dst (MinReductionV src1 src2));
  match(Set dst (MaxReductionV src1 src2));
  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, KILL cr);
  format %{ "vector_minmax2D_reduction $dst,$src1,$src2 ; using $tmp1, $tmp2, $tmp3, $tmp4 as TEMP" %}
  ins_encode %{
    assert(UseAVX > 0, "sanity");

    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ reduceDoubleMinMax(opcode, vlen, false, $dst$$XMMRegister, $src2$$XMMRegister,
                          $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister, $tmp4$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct minmax_reductionD(legRegD dst, immD src1, legVec src2, legVec tmp1, legVec tmp2,
                           legVec tmp3, legVec tmp4, legVec tmp5, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx10_2() && Matcher::vector_element_basic_type(n->in(2)) == T_DOUBLE &&
            ((n->Opcode() == Op_MinReductionV && n->in(1)->bottom_type() == TypeD::POS_INF) ||
             (n->Opcode() == Op_MaxReductionV && n->in(1)->bottom_type() == TypeD::NEG_INF)) &&
            Matcher::vector_length(n->in(2)) >= 4);
  match(Set dst (MinReductionV src1 src2));
  match(Set dst (MaxReductionV src1 src2));
  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP tmp5, KILL cr);
  format %{ "vector_minmaxD_reduction $dst,$src1,$src2 ; using $tmp1, $tmp2, $tmp3, $tmp4, $tmp5 as TEMP" %}
  ins_encode %{
    assert(UseAVX > 0, "sanity");

    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ reduceDoubleMinMax(opcode, vlen, false, $dst$$XMMRegister, $src2$$XMMRegister,
                          $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister, $tmp4$$XMMRegister, $tmp5$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}


instruct minmax_reduction2D_av(legRegD dst, legVec src, legVec tmp1, legVec tmp2,
                               legVec tmp3, legVec tmp4, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx10_2() && Matcher::vector_element_basic_type(n->in(2)) == T_DOUBLE &&
            Matcher::vector_length(n->in(2)) == 2);
  match(Set dst (MinReductionV dst src));
  match(Set dst (MaxReductionV dst src));
  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, KILL cr);
  format %{ "vector_minmax2D_reduction $dst,$src ; using $tmp1, $tmp2, $tmp3, $tmp4 as TEMP" %}
  ins_encode %{
    assert(UseAVX > 0, "sanity");

    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src);
    __ reduceDoubleMinMax(opcode, vlen, true, $dst$$XMMRegister, $src$$XMMRegister,
                          $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister, $tmp4$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct minmax_reductionD_av(legRegD dst, legVec src, legVec tmp1, legVec tmp2, legVec tmp3,
                              legVec tmp4, legVec tmp5, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx10_2() && Matcher::vector_element_basic_type(n->in(2)) == T_DOUBLE &&
            Matcher::vector_length(n->in(2)) >= 4);
  match(Set dst (MinReductionV dst src));
  match(Set dst (MaxReductionV dst src));
  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP tmp5, KILL cr);
  format %{ "vector_minmaxD_reduction $dst,$src ; using $tmp1, $tmp2, $tmp3, $tmp4, $tmp5 as TEMP" %}
  ins_encode %{
    assert(UseAVX > 0, "sanity");

    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src);
    __ reduceDoubleMinMax(opcode, vlen, true, $dst$$XMMRegister, $src$$XMMRegister,
                          $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister, $tmp4$$XMMRegister, $tmp5$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct minmax_reduction2D_avx10(regD dst, immD src1, vec src2, vec xtmp1) %{
  predicate(VM_Version::supports_avx10_2() && Matcher::vector_element_basic_type(n->in(2)) == T_DOUBLE &&
            ((n->Opcode() == Op_MinReductionV && n->in(1)->bottom_type() == TypeD::POS_INF) ||
             (n->Opcode() == Op_MaxReductionV && n->in(1)->bottom_type() == TypeD::NEG_INF)) &&
            Matcher::vector_length(n->in(2)) == 2);
  match(Set dst (MinReductionV src1 src2));
  match(Set dst (MaxReductionV src1 src2));
  effect(TEMP dst, TEMP xtmp1);
  format %{ "vector_minmax2D_reduction $dst, $src1, $src2 ; using $xtmp1 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ reduceDoubleMinMax(opcode, vlen, false, $dst$$XMMRegister, $src2$$XMMRegister, xnoreg,
                          xnoreg, xnoreg, $xtmp1$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct minmax_reductionD_avx10(regD dst, immD src1, vec src2, vec xtmp1, vec xtmp2) %{
  predicate(VM_Version::supports_avx10_2() && Matcher::vector_element_basic_type(n->in(2)) == T_DOUBLE &&
            ((n->Opcode() == Op_MinReductionV && n->in(1)->bottom_type() == TypeD::POS_INF) ||
             (n->Opcode() == Op_MaxReductionV && n->in(1)->bottom_type() == TypeD::NEG_INF)) &&
            Matcher::vector_length(n->in(2)) >= 4);
  match(Set dst (MinReductionV src1 src2));
  match(Set dst (MaxReductionV src1 src2));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2);
  format %{ "vector_minmaxD_reduction $dst, $src1, $src2 ; using $xtmp1 and $xtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src2);
    __ reduceDoubleMinMax(opcode, vlen, false, $dst$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg,
                          xnoreg, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}


instruct minmax_reduction2D_av_avx10(regD dst, vec src, vec xtmp1) %{
  predicate(VM_Version::supports_avx10_2() && Matcher::vector_element_basic_type(n->in(2)) == T_DOUBLE &&
            Matcher::vector_length(n->in(2)) == 2);
  match(Set dst (MinReductionV dst src));
  match(Set dst (MaxReductionV dst src));
  effect(TEMP dst, TEMP xtmp1);
  format %{ "vector_minmax2D_reduction $dst, $src ; using $xtmp1 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src);
    __ reduceDoubleMinMax(opcode, vlen, true, $dst$$XMMRegister, $src$$XMMRegister,
                          xnoreg, xnoreg, xnoreg, $xtmp1$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct minmax_reductionD_av_avx10(regD dst, vec src, vec xtmp1, vec xtmp2) %{
  predicate(VM_Version::supports_avx10_2() && Matcher::vector_element_basic_type(n->in(2)) == T_DOUBLE &&
            Matcher::vector_length(n->in(2)) >= 4);
  match(Set dst (MinReductionV dst src));
  match(Set dst (MaxReductionV dst src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2);
  format %{ "vector_minmaxD_reduction $dst, $src ; using $xtmp1 and $xtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this, $src);
    __ reduceDoubleMinMax(opcode, vlen, true, $dst$$XMMRegister, $src$$XMMRegister,
                          xnoreg, xnoreg, xnoreg, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

// ====================VECTOR ARITHMETIC=======================================

// --------------------------------- ADD --------------------------------------

// Bytes vector add
instruct vaddB(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (AddVB dst src));
  format %{ "paddb   $dst,$src\t! add packedB" %}
  ins_encode %{
    __ paddb($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vaddB_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (AddVB src1 src2));
  format %{ "vpaddb  $dst,$src1,$src2\t! add packedB" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpaddb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vaddB_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (AddVB src (LoadVector mem)));
  format %{ "vpaddb  $dst,$src,$mem\t! add packedB" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpaddb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Shorts/Chars vector add
instruct vaddS(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (AddVS dst src));
  format %{ "paddw   $dst,$src\t! add packedS" %}
  ins_encode %{
    __ paddw($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vaddS_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (AddVS src1 src2));
  format %{ "vpaddw  $dst,$src1,$src2\t! add packedS" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpaddw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vaddS_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (AddVS src (LoadVector mem)));
  format %{ "vpaddw  $dst,$src,$mem\t! add packedS" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpaddw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Integers vector add
instruct vaddI(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (AddVI dst src));
  format %{ "paddd   $dst,$src\t! add packedI" %}
  ins_encode %{
    __ paddd($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vaddI_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (AddVI src1 src2));
  format %{ "vpaddd  $dst,$src1,$src2\t! add packedI" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpaddd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}


instruct vaddI_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (AddVI src (LoadVector mem)));
  format %{ "vpaddd  $dst,$src,$mem\t! add packedI" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpaddd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Longs vector add
instruct vaddL(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (AddVL dst src));
  format %{ "paddq   $dst,$src\t! add packedL" %}
  ins_encode %{
    __ paddq($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vaddL_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (AddVL src1 src2));
  format %{ "vpaddq  $dst,$src1,$src2\t! add packedL" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpaddq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vaddL_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (AddVL src (LoadVector mem)));
  format %{ "vpaddq  $dst,$src,$mem\t! add packedL" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpaddq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Floats vector add
instruct vaddF(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (AddVF dst src));
  format %{ "addps   $dst,$src\t! add packedF" %}
  ins_encode %{
    __ addps($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vaddF_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (AddVF src1 src2));
  format %{ "vaddps  $dst,$src1,$src2\t! add packedF" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vaddps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vaddF_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (AddVF src (LoadVector mem)));
  format %{ "vaddps  $dst,$src,$mem\t! add packedF" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vaddps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Doubles vector add
instruct vaddD(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (AddVD dst src));
  format %{ "addpd   $dst,$src\t! add packedD" %}
  ins_encode %{
    __ addpd($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vaddD_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (AddVD src1 src2));
  format %{ "vaddpd  $dst,$src1,$src2\t! add packedD" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vaddpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vaddD_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (AddVD src (LoadVector mem)));
  format %{ "vaddpd  $dst,$src,$mem\t! add packedD" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vaddpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- SUB --------------------------------------

// Bytes vector sub
instruct vsubB(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (SubVB dst src));
  format %{ "psubb   $dst,$src\t! sub packedB" %}
  ins_encode %{
    __ psubb($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vsubB_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (SubVB src1 src2));
  format %{ "vpsubb  $dst,$src1,$src2\t! sub packedB" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpsubb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vsubB_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (SubVB src (LoadVector mem)));
  format %{ "vpsubb  $dst,$src,$mem\t! sub packedB" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpsubb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Shorts/Chars vector sub
instruct vsubS(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (SubVS dst src));
  format %{ "psubw   $dst,$src\t! sub packedS" %}
  ins_encode %{
    __ psubw($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}


instruct vsubS_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (SubVS src1 src2));
  format %{ "vpsubw  $dst,$src1,$src2\t! sub packedS" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpsubw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vsubS_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (SubVS src (LoadVector mem)));
  format %{ "vpsubw  $dst,$src,$mem\t! sub packedS" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpsubw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Integers vector sub
instruct vsubI(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (SubVI dst src));
  format %{ "psubd   $dst,$src\t! sub packedI" %}
  ins_encode %{
    __ psubd($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vsubI_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (SubVI src1 src2));
  format %{ "vpsubd  $dst,$src1,$src2\t! sub packedI" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpsubd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vsubI_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (SubVI src (LoadVector mem)));
  format %{ "vpsubd  $dst,$src,$mem\t! sub packedI" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpsubd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Longs vector sub
instruct vsubL(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (SubVL dst src));
  format %{ "psubq   $dst,$src\t! sub packedL" %}
  ins_encode %{
    __ psubq($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vsubL_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (SubVL src1 src2));
  format %{ "vpsubq  $dst,$src1,$src2\t! sub packedL" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpsubq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}


instruct vsubL_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (SubVL src (LoadVector mem)));
  format %{ "vpsubq  $dst,$src,$mem\t! sub packedL" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpsubq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Floats vector sub
instruct vsubF(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (SubVF dst src));
  format %{ "subps   $dst,$src\t! sub packedF" %}
  ins_encode %{
    __ subps($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vsubF_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (SubVF src1 src2));
  format %{ "vsubps  $dst,$src1,$src2\t! sub packedF" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vsubps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vsubF_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (SubVF src (LoadVector mem)));
  format %{ "vsubps  $dst,$src,$mem\t! sub packedF" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vsubps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Doubles vector sub
instruct vsubD(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (SubVD dst src));
  format %{ "subpd   $dst,$src\t! sub packedD" %}
  ins_encode %{
    __ subpd($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vsubD_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (SubVD src1 src2));
  format %{ "vsubpd  $dst,$src1,$src2\t! sub packedD" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vsubpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vsubD_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (SubVD src (LoadVector mem)));
  format %{ "vsubpd  $dst,$src,$mem\t! sub packedD" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vsubpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- MUL --------------------------------------

// Byte vector mul
instruct vmul8B(vec dst, vec src1, vec src2, vec xtmp) %{
  predicate(Matcher::vector_length_in_bytes(n) <= 8);
  match(Set dst (MulVB src1 src2));
  effect(TEMP dst, TEMP xtmp);
  format %{ "mulVB   $dst, $src1, $src2\t! using $xtmp as TEMP" %}
  ins_encode %{
    assert(UseSSE > 3, "required");
    __ pmovsxbw($dst$$XMMRegister, $src1$$XMMRegister);
    __ pmovsxbw($xtmp$$XMMRegister, $src2$$XMMRegister);
    __ pmullw($dst$$XMMRegister, $xtmp$$XMMRegister);
    __ psllw($dst$$XMMRegister, 8);
    __ psrlw($dst$$XMMRegister, 8);
    __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmulB(vec dst, vec src1, vec src2, vec xtmp) %{
  predicate(UseAVX == 0 && Matcher::vector_length_in_bytes(n) > 8);
  match(Set dst (MulVB src1 src2));
  effect(TEMP dst, TEMP xtmp);
  format %{ "mulVB   $dst, $src1, $src2\t! using $xtmp as TEMP" %}
  ins_encode %{
    assert(UseSSE > 3, "required");
    // Odd-index elements
    __ movdqu($dst$$XMMRegister, $src1$$XMMRegister);
    __ psrlw($dst$$XMMRegister, 8);
    __ movdqu($xtmp$$XMMRegister, $src2$$XMMRegister);
    __ psrlw($xtmp$$XMMRegister, 8);
    __ pmullw($dst$$XMMRegister, $xtmp$$XMMRegister);
    __ psllw($dst$$XMMRegister, 8);
    // Even-index elements
    __ movdqu($xtmp$$XMMRegister, $src1$$XMMRegister);
    __ pmullw($xtmp$$XMMRegister, $src2$$XMMRegister);
    __ psllw($xtmp$$XMMRegister, 8);
    __ psrlw($xtmp$$XMMRegister, 8);
    // Combine
    __ por($dst$$XMMRegister, $xtmp$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmulB_reg(vec dst, vec src1, vec src2, vec xtmp1, vec xtmp2) %{
  predicate(UseAVX > 0 && Matcher::vector_length_in_bytes(n) > 8);
  match(Set dst (MulVB src1 src2));
  effect(TEMP xtmp1, TEMP xtmp2);
  format %{ "vmulVB  $dst, $src1, $src2\t! using $xtmp1, $xtmp2 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    // Odd-index elements
    __ vpsrlw($xtmp2$$XMMRegister, $src1$$XMMRegister, 8, vlen_enc);
    __ vpsrlw($xtmp1$$XMMRegister, $src2$$XMMRegister, 8, vlen_enc);
    __ vpmullw($xtmp2$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, vlen_enc);
    __ vpsllw($xtmp2$$XMMRegister, $xtmp2$$XMMRegister, 8, vlen_enc);
    // Even-index elements
    __ vpmullw($xtmp1$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
    __ vpsllw($xtmp1$$XMMRegister, $xtmp1$$XMMRegister, 8, vlen_enc);
    __ vpsrlw($xtmp1$$XMMRegister, $xtmp1$$XMMRegister, 8, vlen_enc);
    // Combine
    __ vpor($dst$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Shorts/Chars vector mul
instruct vmulS(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (MulVS dst src));
  format %{ "pmullw  $dst,$src\t! mul packedS" %}
  ins_encode %{
    __ pmullw($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmulS_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (MulVS src1 src2));
  format %{ "vpmullw $dst,$src1,$src2\t! mul packedS" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpmullw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmulS_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (MulVS src (LoadVector mem)));
  format %{ "vpmullw $dst,$src,$mem\t! mul packedS" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpmullw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Integers vector mul
instruct vmulI(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (MulVI dst src));
  format %{ "pmulld  $dst,$src\t! mul packedI" %}
  ins_encode %{
    assert(UseSSE > 3, "required");
    __ pmulld($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmulI_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (MulVI src1 src2));
  format %{ "vpmulld $dst,$src1,$src2\t! mul packedI" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpmulld($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmulI_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (MulVI src (LoadVector mem)));
  format %{ "vpmulld $dst,$src,$mem\t! mul packedI" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpmulld($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Longs vector mul
instruct evmulL_reg(vec dst, vec src1, vec src2) %{
  predicate((Matcher::vector_length_in_bytes(n) == 64 &&
             VM_Version::supports_avx512dq()) ||
            VM_Version::supports_avx512vldq());
  match(Set dst (MulVL src1 src2));
  ins_cost(500);
  format %{ "evpmullq $dst,$src1,$src2\t! mul packedL" %}
  ins_encode %{
    assert(UseAVX > 2, "required");
    int vlen_enc = vector_length_encoding(this);
    __ evpmullq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct evmulL_mem(vec dst, vec src, memory mem) %{
  predicate((Matcher::vector_length_in_bytes(n) == 64 &&
             VM_Version::supports_avx512dq()) ||
            (Matcher::vector_length_in_bytes(n) > 8 &&
             VM_Version::supports_avx512vldq()));
  match(Set dst (MulVL src (LoadVector mem)));
  format %{ "evpmullq $dst,$src,$mem\t! mul packedL" %}
  ins_cost(500);
  ins_encode %{
    assert(UseAVX > 2, "required");
    int vlen_enc = vector_length_encoding(this);
    __ evpmullq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmulL(vec dst, vec src1, vec src2, vec xtmp) %{
  predicate(UseAVX == 0);
  match(Set dst (MulVL src1 src2));
  ins_cost(500);
  effect(TEMP dst, TEMP xtmp);
  format %{ "mulVL   $dst, $src1, $src2\t! using $xtmp as TEMP" %}
  ins_encode %{
    assert(VM_Version::supports_sse4_1(), "required");
    // Get the lo-hi products, only the lower 32 bits is in concerns
    __ pshufd($xtmp$$XMMRegister, $src2$$XMMRegister, 0xB1);
    __ pmulld($xtmp$$XMMRegister, $src1$$XMMRegister);
    __ pshufd($dst$$XMMRegister, $xtmp$$XMMRegister, 0xB1);
    __ paddd($dst$$XMMRegister, $xtmp$$XMMRegister);
    __ psllq($dst$$XMMRegister, 32);
    // Get the lo-lo products
    __ movdqu($xtmp$$XMMRegister, $src1$$XMMRegister);
    __ pmuludq($xtmp$$XMMRegister, $src2$$XMMRegister);
    __ paddq($dst$$XMMRegister, $xtmp$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmulL_reg(vec dst, vec src1, vec src2, vec xtmp1, vec xtmp2) %{
  predicate(UseAVX > 0 &&
            ((Matcher::vector_length_in_bytes(n) == 64 &&
              !VM_Version::supports_avx512dq()) ||
             (Matcher::vector_length_in_bytes(n) < 64 &&
              !VM_Version::supports_avx512vldq())));
  match(Set dst (MulVL src1 src2));
  effect(TEMP xtmp1, TEMP xtmp2);
  ins_cost(500);
  format %{ "vmulVL  $dst, $src1, $src2\t! using $xtmp1, $xtmp2 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    // Get the lo-hi products, only the lower 32 bits is in concerns
    __ vpshufd($xtmp1$$XMMRegister, $src2$$XMMRegister, 0xB1, vlen_enc);
    __ vpmulld($xtmp1$$XMMRegister, $src1$$XMMRegister, $xtmp1$$XMMRegister, vlen_enc);
    __ vpshufd($xtmp2$$XMMRegister, $xtmp1$$XMMRegister, 0xB1, vlen_enc);
    __ vpaddd($xtmp2$$XMMRegister, $xtmp2$$XMMRegister, $xtmp1$$XMMRegister, vlen_enc);
    __ vpsllq($xtmp2$$XMMRegister, $xtmp2$$XMMRegister, 32, vlen_enc);
    // Get the lo-lo products
    __ vpmuludq($xtmp1$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
    __ vpaddq($dst$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmuludq_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0 && n->as_MulVL()->has_uint_inputs());
  match(Set dst (MulVL src1 src2));
  ins_cost(100);
  format %{ "vpmuludq $dst,$src1,$src2\t! muludq packedL" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpmuludq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmuldq_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0 && n->as_MulVL()->has_int_inputs());
  match(Set dst (MulVL src1 src2));
  ins_cost(100);
  format %{ "vpmuldq $dst,$src1,$src2\t! muldq packedL" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpmuldq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Floats vector mul
instruct vmulF(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (MulVF dst src));
  format %{ "mulps   $dst,$src\t! mul packedF" %}
  ins_encode %{
    __ mulps($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmulF_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (MulVF src1 src2));
  format %{ "vmulps  $dst,$src1,$src2\t! mul packedF" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vmulps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmulF_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (MulVF src (LoadVector mem)));
  format %{ "vmulps  $dst,$src,$mem\t! mul packedF" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vmulps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Doubles vector mul
instruct vmulD(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (MulVD dst src));
  format %{ "mulpd   $dst,$src\t! mul packedD" %}
  ins_encode %{
    __ mulpd($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmulD_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (MulVD src1 src2));
  format %{ "vmulpd  $dst,$src1,$src2\t! mul packedD" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vmulpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmulD_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (MulVD src (LoadVector mem)));
  format %{ "vmulpd  $dst,$src,$mem\t! mul packedD" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vmulpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- DIV --------------------------------------

// Floats vector div
instruct vdivF(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (DivVF dst src));
  format %{ "divps   $dst,$src\t! div packedF" %}
  ins_encode %{
    __ divps($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vdivF_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (DivVF src1 src2));
  format %{ "vdivps  $dst,$src1,$src2\t! div packedF" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vdivps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vdivF_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (DivVF src (LoadVector mem)));
  format %{ "vdivps  $dst,$src,$mem\t! div packedF" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vdivps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Doubles vector div
instruct vdivD(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (DivVD dst src));
  format %{ "divpd   $dst,$src\t! div packedD" %}
  ins_encode %{
    __ divpd($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vdivD_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (DivVD src1 src2));
  format %{ "vdivpd  $dst,$src1,$src2\t! div packedD" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vdivpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vdivD_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (DivVD src (LoadVector mem)));
  format %{ "vdivpd  $dst,$src,$mem\t! div packedD" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vdivpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// ------------------------------ MinMax ---------------------------------------

// Byte, Short, Int vector Min/Max
instruct minmax_reg_sse(vec dst, vec src) %{
  predicate(is_integral_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_element_basic_type(n) != T_LONG && // T_BYTE, T_SHORT, T_INT
            UseAVX == 0);
  match(Set dst (MinV dst src));
  match(Set dst (MaxV dst src));
  format %{ "vector_minmax  $dst,$src\t!  " %}
  ins_encode %{
    assert(UseSSE >= 4, "required");

    int opcode = this->ideal_Opcode();
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ pminmax(opcode, elem_bt, $dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vminmax_reg(vec dst, vec src1, vec src2) %{
  predicate(is_integral_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_element_basic_type(n) != T_LONG && // T_BYTE, T_SHORT, T_INT
            UseAVX > 0);
  match(Set dst (MinV src1 src2));
  match(Set dst (MaxV src1 src2));
  format %{ "vector_minmax  $dst,$src1,$src2\t!  " %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);

    __ vpminmax(opcode, elem_bt, $dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Long vector Min/Max
instruct minmaxL_reg_sse(vec dst, vec src, rxmm0 tmp) %{
  predicate(Matcher::vector_length_in_bytes(n) == 16 && Matcher::vector_element_basic_type(n) == T_LONG &&
            UseAVX == 0);
  match(Set dst (MinV dst src));
  match(Set dst (MaxV src dst));
  effect(TEMP dst, TEMP tmp);
  format %{ "vector_minmaxL  $dst,$src\t!using $tmp as TEMP" %}
  ins_encode %{
    assert(UseSSE >= 4, "required");

    int opcode = this->ideal_Opcode();
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    assert(elem_bt == T_LONG, "sanity");

    __ pminmax(opcode, elem_bt, $dst$$XMMRegister, $src$$XMMRegister, $tmp$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vminmaxL_reg_avx(legVec dst, legVec src1, legVec src2) %{
  predicate(Matcher::vector_length_in_bytes(n) <= 32 && Matcher::vector_element_basic_type(n) == T_LONG &&
            UseAVX > 0 && !VM_Version::supports_avx512vl());
  match(Set dst (MinV src1 src2));
  match(Set dst (MaxV src1 src2));
  effect(TEMP dst);
  format %{ "vector_minmaxL  $dst,$src1,$src2\t! " %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    int opcode = this->ideal_Opcode();
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    assert(elem_bt == T_LONG, "sanity");

    __ vpminmax(opcode, elem_bt, $dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vminmaxL_reg_evex(vec dst, vec src1, vec src2) %{
  predicate((Matcher::vector_length_in_bytes(n) == 64 || VM_Version::supports_avx512vl()) &&
            Matcher::vector_element_basic_type(n) == T_LONG);
  match(Set dst (MinV src1 src2));
  match(Set dst (MaxV src1 src2));
  format %{ "vector_minmaxL  $dst,$src1,src2\t! " %}
  ins_encode %{
    assert(UseAVX > 2, "required");

    int vlen_enc = vector_length_encoding(this);
    int opcode = this->ideal_Opcode();
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    assert(elem_bt == T_LONG, "sanity");

    __ vpminmax(opcode, elem_bt, $dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Float/Double vector Min/Max
instruct minmaxFP_avx10_reg(vec dst, vec a, vec b) %{
  predicate(VM_Version::supports_avx10_2() &&
            is_floating_point_type(Matcher::vector_element_basic_type(n))); // T_FLOAT, T_DOUBLE
  match(Set dst (MinV a b));
  match(Set dst (MaxV a b));
  format %{ "vector_minmaxFP  $dst, $a, $b" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    int opcode = this->ideal_Opcode();
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ vminmax_fp(opcode, elem_bt, $dst$$XMMRegister, k0, $a$$XMMRegister, $b$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Float/Double vector Min/Max
instruct minmaxFP_reg(legVec dst, legVec a, legVec b, legVec tmp, legVec atmp, legVec btmp) %{
  predicate(!VM_Version::supports_avx10_2() && Matcher::vector_length_in_bytes(n) <= 32 &&
            is_floating_point_type(Matcher::vector_element_basic_type(n)) && // T_FLOAT, T_DOUBLE
            UseAVX > 0);
  match(Set dst (MinV a b));
  match(Set dst (MaxV a b));
  effect(USE a, USE b, TEMP tmp, TEMP atmp, TEMP btmp);
  format %{ "vector_minmaxFP  $dst,$a,$b\t!using $tmp, $atmp, $btmp as TEMP" %}
  ins_encode %{
    assert(UseAVX > 0, "required");

    int opcode = this->ideal_Opcode();
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);

    __ vminmax_fp(opcode, elem_bt,
                  $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister,
                  $tmp$$XMMRegister, $atmp$$XMMRegister , $btmp$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct evminmaxFP_reg_evex(vec dst, vec a, vec b, vec atmp, vec btmp, kReg ktmp) %{
  predicate(!VM_Version::supports_avx10_2() && Matcher::vector_length_in_bytes(n) == 64 &&
            is_floating_point_type(Matcher::vector_element_basic_type(n))); // T_FLOAT, T_DOUBLE
  match(Set dst (MinV a b));
  match(Set dst (MaxV a b));
  effect(TEMP dst, USE a, USE b, TEMP atmp, TEMP btmp, TEMP ktmp);
  format %{ "vector_minmaxFP  $dst,$a,$b\t!using $atmp, $btmp as TEMP" %}
  ins_encode %{
    assert(UseAVX > 2, "required");

    int opcode = this->ideal_Opcode();
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);

    __ evminmax_fp(opcode, elem_bt,
                   $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister,
                   $ktmp$$KRegister, $atmp$$XMMRegister , $btmp$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// ------------------------------ Unsigned vector Min/Max ----------------------

instruct vector_uminmax_reg(vec dst, vec a, vec b) %{
  predicate(VM_Version::supports_avx512vl() || Matcher::vector_element_basic_type(n) != T_LONG);
  match(Set dst (UMinV a b));
  match(Set dst (UMaxV a b));
  format %{ "vector_uminmax $dst,$a,$b\t!" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    assert(is_integral_type(elem_bt), "");
    __ vpuminmax(opcode, elem_bt, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vector_uminmax_mem(vec dst, vec a, memory b) %{
  predicate(VM_Version::supports_avx512vl() || Matcher::vector_element_basic_type(n) != T_LONG);
  match(Set dst (UMinV a (LoadVector b)));
  match(Set dst (UMaxV a (LoadVector b)));
  format %{ "vector_uminmax $dst,$a,$b\t!" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    assert(is_integral_type(elem_bt), "");
    __ vpuminmax(opcode, elem_bt, $dst$$XMMRegister, $a$$XMMRegister, $b$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vector_uminmaxq_reg(vec dst, vec a, vec b, vec xtmp1, vec xtmp2) %{
  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_element_basic_type(n) == T_LONG);
  match(Set dst (UMinV a b));
  match(Set dst (UMaxV a b));
  effect(TEMP xtmp1, TEMP xtmp2);
  format %{ "vector_uminmaxq $dst,$a,$b\t! using xtmp1 and xtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen_enc = vector_length_encoding(this);
    __ vpuminmaxq(opcode, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vector_uminmax_reg_masked(vec dst, vec src2, kReg mask) %{
  match(Set dst (UMinV (Binary dst src2) mask));
  match(Set dst (UMaxV (Binary dst src2) mask));
  format %{ "vector_uminmax_masked $dst, $dst, $src2, $mask\t! umin/max masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vector_uminmax_mem_masked(vec dst, memory src2, kReg mask) %{
  match(Set dst (UMinV (Binary dst (LoadVector src2)) mask));
  match(Set dst (UMaxV (Binary dst (LoadVector src2)) mask));
  format %{ "vector_uminmax_masked $dst, $dst, $src2, $mask\t! umin/max masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- Signum/CopySign ---------------------------

instruct signumF_reg(regF dst, regF zero, regF one, rFlagsReg cr) %{
  match(Set dst (SignumF dst (Binary zero one)));
  effect(KILL cr);
  format %{ "signumF $dst, $dst" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    __ signum_fp(opcode, $dst$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct signumD_reg(regD dst, regD zero, regD one, rFlagsReg cr) %{
  match(Set dst (SignumD dst (Binary zero one)));
  effect(KILL cr);
  format %{ "signumD $dst, $dst" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    __ signum_fp(opcode, $dst$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct signumV_reg_avx(vec dst, vec src, vec zero, vec one, vec xtmp1) %{
  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 32);
  match(Set dst (SignumVF src (Binary zero one)));
  match(Set dst (SignumVD src (Binary zero one)));
  effect(TEMP dst, TEMP xtmp1);
  format %{ "vector_signum_avx $dst, $src\t! using $xtmp1 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vec_enc = vector_length_encoding(this);
    __ vector_signum_avx(opcode, $dst$$XMMRegister, $src$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister,
                         $xtmp1$$XMMRegister, vec_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct signumV_reg_evex(vec dst, vec src, vec zero, vec one, kReg ktmp1) %{
  predicate(VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64);
  match(Set dst (SignumVF src (Binary zero one)));
  match(Set dst (SignumVD src (Binary zero one)));
  effect(TEMP dst, TEMP ktmp1);
  format %{ "vector_signum_evex $dst, $src\t! using $ktmp1 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vec_enc = vector_length_encoding(this);
    __ vector_signum_evex(opcode, $dst$$XMMRegister, $src$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister,
                          $ktmp1$$KRegister, vec_enc);
  %}
  ins_pipe( pipe_slow );
%}

// ---------------------------------------
// For copySign use 0xE4 as writemask for vpternlog
// Desired Truth Table: A -> xmm0 bit, B -> xmm1 bit, C -> xmm2 bit
// C (xmm2) is set to 0x7FFFFFFF
// Wherever xmm2 is 0, we want to pick from B (sign)
// Wherever xmm2 is 1, we want to pick from A (src)
//
// A B C Result
// 0 0 0 0
// 0 0 1 0
// 0 1 0 1
// 0 1 1 0
// 1 0 0 0
// 1 0 1 1
// 1 1 0 1
// 1 1 1 1
//
// Result going from high bit to low bit is 0x11100100 = 0xe4
// ---------------------------------------

instruct copySignF_reg(regF dst, regF src, regF tmp1, rRegI tmp2) %{
  match(Set dst (CopySignF dst src));
  effect(TEMP tmp1, TEMP tmp2);
  format %{ "CopySignF $dst, $src\t! using $tmp1 and $tmp2 as TEMP" %}
  ins_encode %{
    __ movl($tmp2$$Register, 0x7FFFFFFF);
    __ movdl($tmp1$$XMMRegister, $tmp2$$Register);
    __ vpternlogd($dst$$XMMRegister, 0xE4, $src$$XMMRegister, $tmp1$$XMMRegister, Assembler::AVX_128bit);
  %}
  ins_pipe( pipe_slow );
%}

instruct copySignD_imm(regD dst, regD src, regD tmp1, rRegL tmp2, immD zero) %{
  match(Set dst (CopySignD dst (Binary src zero)));
  ins_cost(100);
  effect(TEMP tmp1, TEMP tmp2);
  format %{ "CopySignD  $dst, $src\t! using $tmp1 and $tmp2 as TEMP" %}
  ins_encode %{
    __ mov64($tmp2$$Register, 0x7FFFFFFFFFFFFFFF);
    __ movq($tmp1$$XMMRegister, $tmp2$$Register);
    __ vpternlogq($dst$$XMMRegister, 0xE4, $src$$XMMRegister, $tmp1$$XMMRegister, Assembler::AVX_128bit);
  %}
  ins_pipe( pipe_slow );
%}

//----------------------------- CompressBits/ExpandBits ------------------------

instruct compressBitsI_reg(rRegI dst, rRegI src, rRegI mask) %{
  predicate(n->bottom_type()->isa_int());
  match(Set dst (CompressBits src mask));
  format %{ "pextl  $dst, $src, $mask\t! parallel bit extract" %}
  ins_encode %{
    __ pextl($dst$$Register, $src$$Register, $mask$$Register);
  %}
  ins_pipe( pipe_slow );
%}

instruct expandBitsI_reg(rRegI dst, rRegI src, rRegI mask) %{
  predicate(n->bottom_type()->isa_int());
  match(Set dst (ExpandBits src mask));
  format %{ "pdepl  $dst, $src, $mask\t! parallel bit deposit" %}
  ins_encode %{
    __ pdepl($dst$$Register, $src$$Register, $mask$$Register);
  %}
  ins_pipe( pipe_slow );
%}

instruct compressBitsI_mem(rRegI dst, rRegI src, memory mask) %{
  predicate(n->bottom_type()->isa_int());
  match(Set dst (CompressBits src (LoadI mask)));
  format %{ "pextl  $dst, $src, $mask\t! parallel bit extract" %}
  ins_encode %{
    __ pextl($dst$$Register, $src$$Register, $mask$$Address);
  %}
  ins_pipe( pipe_slow );
%}

instruct expandBitsI_mem(rRegI dst, rRegI src, memory mask) %{
  predicate(n->bottom_type()->isa_int());
  match(Set dst (ExpandBits src (LoadI mask)));
  format %{ "pdepl  $dst, $src, $mask\t! parallel bit deposit" %}
  ins_encode %{
    __ pdepl($dst$$Register, $src$$Register, $mask$$Address);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- Sqrt --------------------------------------

instruct vsqrtF_reg(vec dst, vec src) %{
  match(Set dst (SqrtVF src));
  format %{ "vsqrtps  $dst,$src\t! sqrt packedF" %}
  ins_encode %{
    assert(UseAVX > 0, "required");
    int vlen_enc = vector_length_encoding(this);
    __ vsqrtps($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vsqrtF_mem(vec dst, memory mem) %{
  predicate(Matcher::vector_length_in_bytes(n->in(1)) > 8);
  match(Set dst (SqrtVF (LoadVector mem)));
  format %{ "vsqrtps  $dst,$mem\t! sqrt packedF" %}
  ins_encode %{
    assert(UseAVX > 0, "required");
    int vlen_enc = vector_length_encoding(this);
    __ vsqrtps($dst$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Floating point vector sqrt
instruct vsqrtD_reg(vec dst, vec src) %{
  match(Set dst (SqrtVD src));
  format %{ "vsqrtpd  $dst,$src\t! sqrt packedD" %}
  ins_encode %{
    assert(UseAVX > 0, "required");
    int vlen_enc = vector_length_encoding(this);
    __ vsqrtpd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vsqrtD_mem(vec dst, memory mem) %{
  predicate(Matcher::vector_length_in_bytes(n->in(1)) > 8);
  match(Set dst (SqrtVD (LoadVector mem)));
  format %{ "vsqrtpd  $dst,$mem\t! sqrt packedD" %}
  ins_encode %{
    assert(UseAVX > 0, "required");
    int vlen_enc = vector_length_encoding(this);
    __ vsqrtpd($dst$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// ------------------------------ Shift ---------------------------------------

// Left and right shift count vectors are the same on x86
// (only lowest bits of xmm reg are used for count).
instruct vshiftcnt(vec dst, rRegI cnt) %{
  match(Set dst (LShiftCntV cnt));
  match(Set dst (RShiftCntV cnt));
  format %{ "movdl    $dst,$cnt\t! load shift count" %}
  ins_encode %{
    __ movdl($dst$$XMMRegister, $cnt$$Register);
  %}
  ins_pipe( pipe_slow );
%}

// Byte vector shift
instruct vshiftB(vec dst, vec src, vec shift, vec tmp) %{
  predicate(Matcher::vector_length(n) <= 8 && !n->as_ShiftV()->is_var_shift());
  match(Set dst ( LShiftVB src shift));
  match(Set dst ( RShiftVB src shift));
  match(Set dst (URShiftVB src shift));
  effect(TEMP dst, USE src, USE shift, TEMP tmp);
  format %{"vector_byte_shift $dst,$src,$shift" %}
  ins_encode %{
    assert(UseSSE > 3, "required");
    int opcode = this->ideal_Opcode();
    bool sign = (opcode != Op_URShiftVB);
    __ vextendbw(sign, $tmp$$XMMRegister, $src$$XMMRegister);
    __ vshiftw(opcode, $tmp$$XMMRegister, $shift$$XMMRegister);
    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);
    __ pand($dst$$XMMRegister, $tmp$$XMMRegister);
    __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vshift16B(vec dst, vec src, vec shift, vec tmp1, vec tmp2) %{
  predicate(Matcher::vector_length(n) == 16 && !n->as_ShiftV()->is_var_shift() &&
            UseAVX <= 1);
  match(Set dst ( LShiftVB src shift));
  match(Set dst ( RShiftVB src shift));
  match(Set dst (URShiftVB src shift));
  effect(TEMP dst, USE src, USE shift, TEMP tmp1, TEMP tmp2);
  format %{"vector_byte_shift $dst,$src,$shift" %}
  ins_encode %{
    assert(UseSSE > 3, "required");
    int opcode = this->ideal_Opcode();
    bool sign = (opcode != Op_URShiftVB);
    __ vextendbw(sign, $tmp1$$XMMRegister, $src$$XMMRegister);
    __ vshiftw(opcode, $tmp1$$XMMRegister, $shift$$XMMRegister);
    __ pshufd($tmp2$$XMMRegister, $src$$XMMRegister, 0xE);
    __ vextendbw(sign, $tmp2$$XMMRegister, $tmp2$$XMMRegister);
    __ vshiftw(opcode, $tmp2$$XMMRegister, $shift$$XMMRegister);
    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);
    __ pand($tmp2$$XMMRegister, $dst$$XMMRegister);
    __ pand($dst$$XMMRegister, $tmp1$$XMMRegister);
    __ packuswb($dst$$XMMRegister, $tmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vshift16B_avx(vec dst, vec src, vec shift, vec tmp) %{
  predicate(Matcher::vector_length(n) == 16 && !n->as_ShiftV()->is_var_shift() &&
            UseAVX > 1);
  match(Set dst ( LShiftVB src shift));
  match(Set dst ( RShiftVB src shift));
  match(Set dst (URShiftVB src shift));
  effect(TEMP dst, TEMP tmp);
  format %{"vector_byte_shift $dst,$src,$shift" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    bool sign = (opcode != Op_URShiftVB);
    int vlen_enc = Assembler::AVX_256bit;
    __ vextendbw(sign, $tmp$$XMMRegister, $src$$XMMRegister, vlen_enc);
    __ vshiftw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vlen_enc);
    __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, noreg);
    __ vextracti128_high($dst$$XMMRegister, $tmp$$XMMRegister);
    __ vpackuswb($dst$$XMMRegister, $tmp$$XMMRegister, $dst$$XMMRegister, 0);
  %}
  ins_pipe( pipe_slow );
%}

instruct vshift32B_avx(vec dst, vec src, vec shift, vec tmp) %{
  predicate(Matcher::vector_length(n) == 32 && !n->as_ShiftV()->is_var_shift());
  match(Set dst ( LShiftVB src shift));
  match(Set dst ( RShiftVB src shift));
  match(Set dst (URShiftVB src shift));
  effect(TEMP dst, TEMP tmp);
  format %{"vector_byte_shift $dst,$src,$shift" %}
  ins_encode %{
    assert(UseAVX > 1, "required");
    int opcode = this->ideal_Opcode();
    bool sign = (opcode != Op_URShiftVB);
    int vlen_enc = Assembler::AVX_256bit;
    __ vextracti128_high($tmp$$XMMRegister, $src$$XMMRegister);
    __ vextendbw(sign, $tmp$$XMMRegister, $tmp$$XMMRegister, vlen_enc);
    __ vextendbw(sign, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
    __ vshiftw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vlen_enc);
    __ vshiftw(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $shift$$XMMRegister, vlen_enc);
    __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, noreg);
    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, noreg);
    __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vlen_enc);
    __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 0xD8, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vshift64B_avx(vec dst, vec src, vec shift, vec tmp1, vec tmp2) %{
  predicate(Matcher::vector_length(n) == 64 && !n->as_ShiftV()->is_var_shift());
  match(Set dst ( LShiftVB src shift));
  match(Set dst  (RShiftVB src shift));
  match(Set dst (URShiftVB src shift));
  effect(TEMP dst, TEMP tmp1, TEMP tmp2);
  format %{"vector_byte_shift $dst,$src,$shift" %}
  ins_encode %{
    assert(UseAVX > 2, "required");
    int opcode = this->ideal_Opcode();
    bool sign = (opcode != Op_URShiftVB);
    int vlen_enc = Assembler::AVX_512bit;
    __ vextracti64x4($tmp1$$XMMRegister, $src$$XMMRegister, 1);
    __ vextendbw(sign, $tmp1$$XMMRegister, $tmp1$$XMMRegister, vlen_enc);
    __ vextendbw(sign, $tmp2$$XMMRegister, $src$$XMMRegister, vlen_enc);
    __ vshiftw(opcode, $tmp1$$XMMRegister, $tmp1$$XMMRegister, $shift$$XMMRegister, vlen_enc);
    __ vshiftw(opcode, $tmp2$$XMMRegister, $tmp2$$XMMRegister, $shift$$XMMRegister, vlen_enc);
    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);
    __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
    __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vlen_enc);
    __ vpand($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vlen_enc);
    __ vpackuswb($dst$$XMMRegister, $tmp1$$XMMRegister, $tmp2$$XMMRegister, vlen_enc);
    __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vlen_enc, noreg);
    __ vpermq($dst$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// Shorts vector logical right shift produces incorrect Java result
// for negative data because java code convert short value into int with
// sign extension before a shift. But char vectors are fine since chars are
// unsigned values.
// Shorts/Chars vector left shift
instruct vshiftS(vec dst, vec src, vec shift) %{
  predicate(!n->as_ShiftV()->is_var_shift());
  match(Set dst ( LShiftVS src shift));
  match(Set dst ( RShiftVS src shift));
  match(Set dst (URShiftVS src shift));
  effect(TEMP dst, USE src, USE shift);
  format %{ "vshiftw  $dst,$src,$shift\t! shift packedS" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    if (UseAVX > 0) {
      int vlen_enc = vector_length_encoding(this);
      __ vshiftw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc);
    } else {
      int vlen = Matcher::vector_length(this);
      if (vlen == 2) {
        __ movflt($dst$$XMMRegister, $src$$XMMRegister);
        __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
      } else if (vlen == 4) {
        __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
        __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
      } else {
        assert (vlen == 8, "sanity");
        __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
        __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
      }
    }
  %}
  ins_pipe( pipe_slow );
%}

// Integers vector left shift
instruct vshiftI(vec dst, vec src, vec shift) %{
  predicate(!n->as_ShiftV()->is_var_shift());
  match(Set dst ( LShiftVI src shift));
  match(Set dst ( RShiftVI src shift));
  match(Set dst (URShiftVI src shift));
  effect(TEMP dst, USE src, USE shift);
  format %{ "vshiftd  $dst,$src,$shift\t! shift packedI" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    if (UseAVX > 0) {
      int vlen_enc = vector_length_encoding(this);
      __ vshiftd(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc);
    } else {
      int vlen = Matcher::vector_length(this);
      if (vlen == 2) {
        __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
        __ vshiftd(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
      } else {
        assert(vlen == 4, "sanity");
        __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
        __ vshiftd(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
      }
    }
  %}
  ins_pipe( pipe_slow );
%}

// Integers vector left constant shift
instruct vshiftI_imm(vec dst, vec src, immI8 shift) %{
  match(Set dst (LShiftVI src (LShiftCntV shift)));
  match(Set dst (RShiftVI src (RShiftCntV shift)));
  match(Set dst (URShiftVI src (RShiftCntV shift)));
  format %{ "vshiftd_imm  $dst,$src,$shift\t! shift packedI" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    if (UseAVX > 0) {
      int vector_len = vector_length_encoding(this);
      __ vshiftd_imm(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$constant, vector_len);
    } else {
      int vlen = Matcher::vector_length(this);
      if (vlen == 2) {
        __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
        __ vshiftd_imm(opcode, $dst$$XMMRegister, $shift$$constant);
      } else {
        assert(vlen == 4, "sanity");
        __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
        __ vshiftd_imm(opcode, $dst$$XMMRegister, $shift$$constant);
      }
    }
  %}
  ins_pipe( pipe_slow );
%}

// Longs vector shift
instruct vshiftL(vec dst, vec src, vec shift) %{
  predicate(!n->as_ShiftV()->is_var_shift());
  match(Set dst ( LShiftVL src shift));
  match(Set dst (URShiftVL src shift));
  effect(TEMP dst, USE src, USE shift);
  format %{ "vshiftq  $dst,$src,$shift\t! shift packedL" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    if (UseAVX > 0) {
      int vlen_enc = vector_length_encoding(this);
      __ vshiftq(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc);
    } else {
      assert(Matcher::vector_length(this) == 2, "");
      __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
      __ vshiftq(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
    }
  %}
  ins_pipe( pipe_slow );
%}

// Longs vector constant shift
instruct vshiftL_imm(vec dst, vec src, immI8 shift) %{
  match(Set dst (LShiftVL src (LShiftCntV shift)));
  match(Set dst (URShiftVL src (RShiftCntV shift)));
  format %{ "vshiftq_imm  $dst,$src,$shift\t! shift packedL" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    if (UseAVX > 0) {
      int vector_len = vector_length_encoding(this);
      __ vshiftq_imm(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$constant, vector_len);
    } else {
      assert(Matcher::vector_length(this) == 2, "");
      __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
      __ vshiftq_imm(opcode, $dst$$XMMRegister, $shift$$constant);
    }
  %}
  ins_pipe( pipe_slow );
%}

// -------------------ArithmeticRightShift -----------------------------------
// Long vector arithmetic right shift
instruct vshiftL_arith_reg(vec dst, vec src, vec shift, vec tmp) %{
  predicate(!n->as_ShiftV()->is_var_shift() && UseAVX <= 2);
  match(Set dst (RShiftVL src shift));
  effect(TEMP dst, TEMP tmp);
  format %{ "vshiftq $dst,$src,$shift" %}
  ins_encode %{
    uint vlen = Matcher::vector_length(this);
    if (vlen == 2) {
      __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
      __ psrlq($dst$$XMMRegister, $shift$$XMMRegister);
      __ movdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), noreg);
      __ psrlq($tmp$$XMMRegister, $shift$$XMMRegister);
      __ pxor($dst$$XMMRegister, $tmp$$XMMRegister);
      __ psubq($dst$$XMMRegister, $tmp$$XMMRegister);
    } else {
      assert(vlen == 4, "sanity");
      assert(UseAVX > 1, "required");
      int vlen_enc = Assembler::AVX_256bit;
      __ vpsrlq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc);
      __ vmovdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), noreg);
      __ vpsrlq($tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vlen_enc);
      __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vlen_enc);
      __ vpsubq($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vlen_enc);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct vshiftL_arith_reg_evex(vec dst, vec src, vec shift) %{
  predicate(!n->as_ShiftV()->is_var_shift() && UseAVX > 2);
  match(Set dst (RShiftVL src shift));
  format %{ "vshiftq $dst,$src,$shift" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ evpsraq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// ------------------- Variable Shift -----------------------------
// Byte variable shift
instruct vshift8B_var_nobw(vec dst, vec src, vec shift, vec vtmp) %{
  predicate(Matcher::vector_length(n) <= 8 &&
            n->as_ShiftV()->is_var_shift() &&
            !VM_Version::supports_avx512bw());
  match(Set dst ( LShiftVB src shift));
  match(Set dst ( RShiftVB src shift));
  match(Set dst (URShiftVB src shift));
  effect(TEMP dst, TEMP vtmp);
  format %{ "vector_varshift_byte $dst, $src, $shift\n\t! using $vtmp as TEMP" %}
  ins_encode %{
    assert(UseAVX >= 2, "required");

    int opcode = this->ideal_Opcode();
    int vlen_enc = Assembler::AVX_128bit;
    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp$$XMMRegister);
    __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0);
  %}
  ins_pipe( pipe_slow );
%}

instruct vshift16B_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2) %{
  predicate(Matcher::vector_length(n) == 16 &&
            n->as_ShiftV()->is_var_shift() &&
            !VM_Version::supports_avx512bw());
  match(Set dst ( LShiftVB src shift));
  match(Set dst ( RShiftVB src shift));
  match(Set dst (URShiftVB src shift));
  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_varshift_byte $dst, $src, $shift\n\t! using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    assert(UseAVX >= 2, "required");

    int opcode = this->ideal_Opcode();
    int vlen_enc = Assembler::AVX_128bit;
    // Shift lower half and get word result in dst
    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister);

    // Shift upper half and get word result in vtmp1
    __ vpshufd($vtmp1$$XMMRegister, $src$$XMMRegister, 0xE, 0);
    __ vpshufd($vtmp2$$XMMRegister, $shift$$XMMRegister, 0xE, 0);
    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister);

    // Merge and down convert the two word results to byte in dst
    __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $vtmp1$$XMMRegister, 0);
  %}
  ins_pipe( pipe_slow );
%}

instruct vshift32B_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2, vec vtmp3, vec vtmp4) %{
  predicate(Matcher::vector_length(n) == 32 &&
            n->as_ShiftV()->is_var_shift() &&
            !VM_Version::supports_avx512bw());
  match(Set dst ( LShiftVB src shift));
  match(Set dst ( RShiftVB src shift));
  match(Set dst (URShiftVB src shift));
  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3, TEMP vtmp4);
  format %{ "vector_varshift_byte $dst, $src, $shift\n\t using $vtmp1, $vtmp2, $vtmp3, $vtmp4 as TEMP" %}
  ins_encode %{
    assert(UseAVX >= 2, "required");

    int opcode = this->ideal_Opcode();
    int vlen_enc = Assembler::AVX_128bit;
    // Process lower 128 bits and get result in dst
    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister);
    __ vpshufd($vtmp1$$XMMRegister, $src$$XMMRegister, 0xE, 0);
    __ vpshufd($vtmp2$$XMMRegister, $shift$$XMMRegister, 0xE, 0);
    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister);
    __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $vtmp1$$XMMRegister, 0);

    // Process higher 128 bits and get result in vtmp3
    __ vextracti128_high($vtmp1$$XMMRegister, $src$$XMMRegister);
    __ vextracti128_high($vtmp2$$XMMRegister, $shift$$XMMRegister);
    __ varshiftbw(opcode, $vtmp3$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp4$$XMMRegister);
    __ vpshufd($vtmp1$$XMMRegister, $vtmp1$$XMMRegister, 0xE, 0);
    __ vpshufd($vtmp2$$XMMRegister, $vtmp2$$XMMRegister, 0xE, 0);
    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister);
    __ vpackuswb($vtmp1$$XMMRegister, $vtmp3$$XMMRegister, $vtmp1$$XMMRegister, 0);

    // Merge the two results in dst
    __ vinserti128($dst$$XMMRegister, $dst$$XMMRegister, $vtmp1$$XMMRegister, 0x1);
  %}
  ins_pipe( pipe_slow );
%}

instruct vshiftB_var_evex_bw(vec dst, vec src, vec shift, vec vtmp) %{
  predicate(Matcher::vector_length(n) <= 32 &&
            n->as_ShiftV()->is_var_shift() &&
            VM_Version::supports_avx512bw());
  match(Set dst ( LShiftVB src shift));
  match(Set dst ( RShiftVB src shift));
  match(Set dst (URShiftVB src shift));
  effect(TEMP dst, TEMP vtmp);
  format %{ "vector_varshift_byte $dst, $src, $shift\n\t! using $vtmp as TEMP" %}
  ins_encode %{
    assert(UseAVX > 2, "required");

    int opcode = this->ideal_Opcode();
    int vlen_enc = vector_length_encoding(this);
    __ evarshiftb(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vshift64B_var_evex_bw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2) %{
  predicate(Matcher::vector_length(n) == 64 &&
            n->as_ShiftV()->is_var_shift() &&
            VM_Version::supports_avx512bw());
  match(Set dst ( LShiftVB src shift));
  match(Set dst ( RShiftVB src shift));
  match(Set dst (URShiftVB src shift));
  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_varshift_byte $dst, $src, $shift\n\t! using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    assert(UseAVX > 2, "required");

    int opcode = this->ideal_Opcode();
    int vlen_enc = Assembler::AVX_256bit;
    __ evarshiftb(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister);
    __ vextracti64x4_high($vtmp1$$XMMRegister, $src$$XMMRegister);
    __ vextracti64x4_high($vtmp2$$XMMRegister, $shift$$XMMRegister);
    __ evarshiftb(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister);
    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $vtmp1$$XMMRegister, 0x1);
  %}
  ins_pipe( pipe_slow );
%}

// Short variable shift
instruct vshift8S_var_nobw(vec dst, vec src, vec shift, vec vtmp) %{
  predicate(Matcher::vector_length(n) <= 8 &&
            n->as_ShiftV()->is_var_shift() &&
            !VM_Version::supports_avx512bw());
  match(Set dst ( LShiftVS src shift));
  match(Set dst ( RShiftVS src shift));
  match(Set dst (URShiftVS src shift));
  effect(TEMP dst, TEMP vtmp);
  format %{ "vector_var_shift_left_short $dst, $src, $shift\n\t" %}
  ins_encode %{
    assert(UseAVX >= 2, "required");

    int opcode = this->ideal_Opcode();
    bool sign = (opcode != Op_URShiftVS);
    int vlen_enc = Assembler::AVX_256bit;
    __ vextendwd(sign, $dst$$XMMRegister, $src$$XMMRegister, 1);
    __ vpmovzxwd($vtmp$$XMMRegister, $shift$$XMMRegister, 1);
    __ varshiftd(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);
    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);
    __ vextracti128_high($vtmp$$XMMRegister, $dst$$XMMRegister);
    __ vpackusdw($dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, 0);
  %}
  ins_pipe( pipe_slow );
%}

instruct vshift16S_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2) %{
  predicate(Matcher::vector_length(n) == 16 &&
            n->as_ShiftV()->is_var_shift() &&
            !VM_Version::supports_avx512bw());
  match(Set dst ( LShiftVS src shift));
  match(Set dst ( RShiftVS src shift));
  match(Set dst (URShiftVS src shift));
  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_var_shift_left_short $dst, $src, $shift\n\t" %}
  ins_encode %{
    assert(UseAVX >= 2, "required");

    int opcode = this->ideal_Opcode();
    bool sign = (opcode != Op_URShiftVS);
    int vlen_enc = Assembler::AVX_256bit;
    // Shift lower half, with result in vtmp2 using vtmp1 as TEMP
    __ vextendwd(sign, $vtmp2$$XMMRegister, $src$$XMMRegister, vlen_enc);
    __ vpmovzxwd($vtmp1$$XMMRegister, $shift$$XMMRegister, vlen_enc);
    __ varshiftd(opcode, $vtmp2$$XMMRegister, $vtmp2$$XMMRegister, $vtmp1$$XMMRegister, vlen_enc);
    __ vpand($vtmp2$$XMMRegister, $vtmp2$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);

    // Shift upper half, with result in dst using vtmp1 as TEMP
    __ vextracti128_high($dst$$XMMRegister, $src$$XMMRegister);
    __ vextracti128_high($vtmp1$$XMMRegister, $shift$$XMMRegister);
    __ vextendwd(sign, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
    __ vpmovzxwd($vtmp1$$XMMRegister, $vtmp1$$XMMRegister, vlen_enc);
    __ varshiftd(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $vtmp1$$XMMRegister, vlen_enc);
    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);

    // Merge lower and upper half result into dst
    __ vpackusdw($dst$$XMMRegister, $vtmp2$$XMMRegister, $dst$$XMMRegister, vlen_enc);
    __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 0xD8, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vshift16S_var_evex_bw(vec dst, vec src, vec shift) %{
  predicate(n->as_ShiftV()->is_var_shift() &&
            VM_Version::supports_avx512bw());
  match(Set dst ( LShiftVS src shift));
  match(Set dst ( RShiftVS src shift));
  match(Set dst (URShiftVS src shift));
  format %{ "vector_varshift_short $dst,$src,$shift\t!" %}
  ins_encode %{
    assert(UseAVX > 2, "required");

    int opcode = this->ideal_Opcode();
    int vlen_enc = vector_length_encoding(this);
    if (!VM_Version::supports_avx512vl()) {
      vlen_enc = Assembler::AVX_512bit;
    }
    __ varshiftw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

//Integer variable shift
instruct vshiftI_var(vec dst, vec src, vec shift) %{
  predicate(n->as_ShiftV()->is_var_shift());
  match(Set dst ( LShiftVI src shift));
  match(Set dst ( RShiftVI src shift));
  match(Set dst (URShiftVI src shift));
  format %{ "vector_varshift_int $dst,$src,$shift\t!" %}
  ins_encode %{
    assert(UseAVX >= 2, "required");

    int opcode = this->ideal_Opcode();
    int vlen_enc = vector_length_encoding(this);
    __ varshiftd(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

//Long variable shift
instruct vshiftL_var(vec dst, vec src, vec shift) %{
  predicate(n->as_ShiftV()->is_var_shift());
  match(Set dst ( LShiftVL src shift));
  match(Set dst (URShiftVL src shift));
  format %{ "vector_varshift_long $dst,$src,$shift\t!" %}
  ins_encode %{
    assert(UseAVX >= 2, "required");

    int opcode = this->ideal_Opcode();
    int vlen_enc = vector_length_encoding(this);
    __ varshiftq(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

//Long variable right shift arithmetic
instruct vshiftL_arith_var(vec dst, vec src, vec shift, vec vtmp) %{
  predicate(Matcher::vector_length(n) <= 4 &&
            n->as_ShiftV()->is_var_shift() &&
            UseAVX == 2);
  match(Set dst (RShiftVL src shift));
  effect(TEMP dst, TEMP vtmp);
  format %{ "vector_varshift_long  $dst,$src,$shift\n\t! using $vtmp as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen_enc = vector_length_encoding(this);
    __ varshiftq(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc,
                 $vtmp$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vshiftL_arith_var_evex(vec dst, vec src, vec shift) %{
  predicate(n->as_ShiftV()->is_var_shift() &&
            UseAVX > 2);
  match(Set dst (RShiftVL src shift));
  format %{ "vector_varfshift_long $dst,$src,$shift\t!" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen_enc = vector_length_encoding(this);
    __ varshiftq(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- AND --------------------------------------

instruct vand(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (AndV dst src));
  format %{ "pand    $dst,$src\t! and vectors" %}
  ins_encode %{
    __ pand($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vand_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (AndV src1 src2));
  format %{ "vpand   $dst,$src1,$src2\t! and vectors" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpand($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vand_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (AndV src (LoadVector mem)));
  format %{ "vpand   $dst,$src,$mem\t! and vectors" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpand($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- OR ---------------------------------------

instruct vor(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (OrV dst src));
  format %{ "por     $dst,$src\t! or vectors" %}
  ins_encode %{
    __ por($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vor_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (OrV src1 src2));
  format %{ "vpor    $dst,$src1,$src2\t! or vectors" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vor_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (OrV src (LoadVector mem)));
  format %{ "vpor    $dst,$src,$mem\t! or vectors" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- XOR --------------------------------------

instruct vxor(vec dst, vec src) %{
  predicate(UseAVX == 0);
  match(Set dst (XorV dst src));
  format %{ "pxor    $dst,$src\t! xor vectors" %}
  ins_encode %{
    __ pxor($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vxor_reg(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (XorV src1 src2));
  format %{ "vpxor   $dst,$src1,$src2\t! xor vectors" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpxor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vxor_mem(vec dst, vec src, memory mem) %{
  predicate((UseAVX > 0) &&
            (Matcher::vector_length_in_bytes(n->in(1)) > 8));
  match(Set dst (XorV src (LoadVector mem)));
  format %{ "vpxor   $dst,$src,$mem\t! xor vectors" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpxor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- VectorCast --------------------------------------

instruct vcastBtoX(vec dst, vec src) %{
  predicate(VM_Version::supports_avx512vl() || Matcher::vector_element_basic_type(n) != T_DOUBLE);
  match(Set dst (VectorCastB2X src));
  format %{ "vector_cast_b2x $dst,$src\t!" %}
  ins_encode %{
    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);
    int vlen_enc = vector_length_encoding(this);
    __ vconvert_b2x(to_elem_bt, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vcastBtoD(legVec dst, legVec src) %{
  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_element_basic_type(n) == T_DOUBLE);
  match(Set dst (VectorCastB2X src));
  format %{ "vector_cast_b2x $dst,$src\t!" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vconvert_b2x(T_DOUBLE, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct castStoX(vec dst, vec src) %{
  predicate((UseAVX <= 2 || !VM_Version::supports_avx512vlbw()) &&
            Matcher::vector_length(n->in(1)) <= 8 && // src
            Matcher::vector_element_basic_type(n) == T_BYTE);
  match(Set dst (VectorCastS2X src));
  format %{ "vector_cast_s2x $dst,$src" %}
  ins_encode %{
    assert(UseAVX > 0, "required");

    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), 0, noreg);
    __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0);
  %}
  ins_pipe( pipe_slow );
%}

instruct vcastStoX(vec dst, vec src, vec vtmp) %{
  predicate((UseAVX <= 2 || !VM_Version::supports_avx512vlbw()) &&
            Matcher::vector_length(n->in(1)) == 16 && // src
            Matcher::vector_element_basic_type(n) == T_BYTE);
  effect(TEMP dst, TEMP vtmp);
  match(Set dst (VectorCastS2X src));
  format %{ "vector_cast_s2x $dst,$src\t! using $vtmp as TEMP" %}
  ins_encode %{
    assert(UseAVX > 0, "required");

    int vlen_enc = vector_length_encoding(Matcher::vector_length_in_bytes(this, $src));
    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, noreg);
    __ vextracti128($vtmp$$XMMRegister, $dst$$XMMRegister, 0x1);
    __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, 0);
  %}
  ins_pipe( pipe_slow );
%}

instruct vcastStoX_evex(vec dst, vec src) %{
  predicate((UseAVX > 2 && VM_Version::supports_avx512vlbw()) ||
            (Matcher::vector_length_in_bytes(n) >= Matcher::vector_length_in_bytes(n->in(1)))); // dst >= src
  match(Set dst (VectorCastS2X src));
  format %{ "vector_cast_s2x $dst,$src\t!" %}
  ins_encode %{
    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);
    int src_vlen_enc = vector_length_encoding(this, $src);
    int vlen_enc = vector_length_encoding(this);
    switch (to_elem_bt) {
      case T_BYTE:
        if (!VM_Version::supports_avx512vl()) {
          vlen_enc = Assembler::AVX_512bit;
        }
        __ evpmovwb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);
        break;
      case T_INT:
        __ vpmovsxwd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
        break;
      case T_FLOAT:
        __ vpmovsxwd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
        __ vcvtdq2ps($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
        break;
      case T_LONG:
        __ vpmovsxwq($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
        break;
      case T_DOUBLE: {
        int mid_vlen_enc = (vlen_enc == Assembler::AVX_512bit) ? Assembler::AVX_256bit : Assembler::AVX_128bit;
        __ vpmovsxwd($dst$$XMMRegister, $src$$XMMRegister, mid_vlen_enc);
        __ vcvtdq2pd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
        break;
      }
      default:
        ShouldNotReachHere();
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct castItoX(vec dst, vec src) %{
  predicate(UseAVX <= 2 &&
            (Matcher::vector_length_in_bytes(n->in(1)) <= 16) &&
            (Matcher::vector_length_in_bytes(n) < Matcher::vector_length_in_bytes(n->in(1)))); // dst < src
  match(Set dst (VectorCastI2X src));
  format %{ "vector_cast_i2x $dst,$src" %}
  ins_encode %{
    assert(UseAVX > 0, "required");

    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);
    int vlen_enc = vector_length_encoding(this, $src);

    if (to_elem_bt == T_BYTE) {
      __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_byte_mask()), vlen_enc, noreg);
      __ vpackusdw($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
      __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
    } else {
      assert(to_elem_bt == T_SHORT, "%s", type2name(to_elem_bt));
      __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);
      __ vpackusdw($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct vcastItoX(vec dst, vec src, vec vtmp) %{
  predicate(UseAVX <= 2 &&
            (Matcher::vector_length_in_bytes(n->in(1)) == 32) &&
            (Matcher::vector_length_in_bytes(n) < Matcher::vector_length_in_bytes(n->in(1)))); // dst < src
  match(Set dst (VectorCastI2X src));
  format %{ "vector_cast_i2x $dst,$src\t! using $vtmp as TEMP" %}
  effect(TEMP dst, TEMP vtmp);
  ins_encode %{
    assert(UseAVX > 0, "required");

    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);
    int vlen_enc = vector_length_encoding(this, $src);

    if (to_elem_bt == T_BYTE) {
      __ vpand($vtmp$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_byte_mask()), vlen_enc, noreg);
      __ vextracti128($dst$$XMMRegister, $vtmp$$XMMRegister, 0x1);
      __ vpackusdw($dst$$XMMRegister, $vtmp$$XMMRegister, $dst$$XMMRegister, vlen_enc);
      __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, Assembler::AVX_128bit);
    } else {
      assert(to_elem_bt == T_SHORT, "%s", type2name(to_elem_bt));
      __ vpand($vtmp$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);
      __ vextracti128($dst$$XMMRegister, $vtmp$$XMMRegister, 0x1);
      __ vpackusdw($dst$$XMMRegister, $vtmp$$XMMRegister, $dst$$XMMRegister, vlen_enc);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct vcastItoX_evex(vec dst, vec src) %{
  predicate(UseAVX > 2 ||
            (Matcher::vector_length_in_bytes(n) >= Matcher::vector_length_in_bytes(n->in(1)))); // dst >= src
  match(Set dst (VectorCastI2X src));
  format %{ "vector_cast_i2x $dst,$src\t!" %}
  ins_encode %{
    assert(UseAVX > 0, "required");

    BasicType dst_elem_bt = Matcher::vector_element_basic_type(this);
    int src_vlen_enc = vector_length_encoding(this, $src);
    int dst_vlen_enc = vector_length_encoding(this);
    switch (dst_elem_bt) {
      case T_BYTE:
        if (!VM_Version::supports_avx512vl()) {
          src_vlen_enc = Assembler::AVX_512bit;
        }
        __ evpmovdb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);
        break;
      case T_SHORT:
        if (!VM_Version::supports_avx512vl()) {
          src_vlen_enc = Assembler::AVX_512bit;
        }
        __ evpmovdw($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);
        break;
      case T_FLOAT:
        __ vcvtdq2ps($dst$$XMMRegister, $src$$XMMRegister, dst_vlen_enc);
        break;
      case T_LONG:
        __ vpmovsxdq($dst$$XMMRegister, $src$$XMMRegister, dst_vlen_enc);
        break;
      case T_DOUBLE:
        __ vcvtdq2pd($dst$$XMMRegister, $src$$XMMRegister, dst_vlen_enc);
        break;
      default:
        ShouldNotReachHere();
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct vcastLtoBS(vec dst, vec src) %{
  predicate((Matcher::vector_element_basic_type(n) == T_BYTE || Matcher::vector_element_basic_type(n) == T_SHORT) &&
            UseAVX <= 2);
  match(Set dst (VectorCastL2X src));
  format %{ "vector_cast_l2x  $dst,$src" %}
  ins_encode %{
    assert(UseAVX > 0, "required");

    int vlen = Matcher::vector_length_in_bytes(this, $src);
    BasicType to_elem_bt  = Matcher::vector_element_basic_type(this);
    AddressLiteral mask_addr = (to_elem_bt == T_BYTE) ? ExternalAddress(vector_int_to_byte_mask())
                                                      : ExternalAddress(vector_int_to_short_mask());
    if (vlen <= 16) {
      __ vpshufd($dst$$XMMRegister, $src$$XMMRegister, 8, Assembler::AVX_128bit);
      __ vpand($dst$$XMMRegister, $dst$$XMMRegister, mask_addr, Assembler::AVX_128bit, noreg);
      __ vpackusdw($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, Assembler::AVX_128bit);
    } else {
      assert(vlen <= 32, "required");
      __ vpermilps($dst$$XMMRegister, $src$$XMMRegister, 8, Assembler::AVX_256bit);
      __ vpermpd($dst$$XMMRegister, $dst$$XMMRegister, 8, Assembler::AVX_256bit);
      __ vpand($dst$$XMMRegister, $dst$$XMMRegister, mask_addr, Assembler::AVX_128bit, noreg);
      __ vpackusdw($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, Assembler::AVX_128bit);
    }
    if (to_elem_bt == T_BYTE) {
      __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, Assembler::AVX_128bit);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct vcastLtoX_evex(vec dst, vec src) %{
  predicate(UseAVX > 2 ||
            (Matcher::vector_element_basic_type(n) == T_INT ||
             Matcher::vector_element_basic_type(n) == T_FLOAT ||
             Matcher::vector_element_basic_type(n) == T_DOUBLE));
  match(Set dst (VectorCastL2X src));
  format %{ "vector_cast_l2x  $dst,$src\t!" %}
  ins_encode %{
    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);
    int vlen = Matcher::vector_length_in_bytes(this, $src);
    int vlen_enc = vector_length_encoding(this, $src);
    switch (to_elem_bt) {
      case T_BYTE:
        if (UseAVX > 2 && !VM_Version::supports_avx512vl()) {
          vlen_enc = Assembler::AVX_512bit;
        }
        __ evpmovqb($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
        break;
      case T_SHORT:
        if (UseAVX > 2 && !VM_Version::supports_avx512vl()) {
          vlen_enc = Assembler::AVX_512bit;
        }
        __ evpmovqw($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
        break;
      case T_INT:
        if (vlen == 8) {
          if ($dst$$XMMRegister != $src$$XMMRegister) {
            __ movflt($dst$$XMMRegister, $src$$XMMRegister);
          }
        } else if (vlen == 16) {
          __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 8);
        } else if (vlen == 32) {
          if (UseAVX > 2) {
            if (!VM_Version::supports_avx512vl()) {
              vlen_enc = Assembler::AVX_512bit;
            }
            __ evpmovqd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
          } else {
            __ vpermilps($dst$$XMMRegister, $src$$XMMRegister, 8, vlen_enc);
            __ vpermpd($dst$$XMMRegister, $dst$$XMMRegister, 8, vlen_enc);
          }
        } else { // vlen == 64
          __ evpmovqd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
        }
        break;
      case T_FLOAT:
        assert(UseAVX > 2 && VM_Version::supports_avx512dq(), "required");
        __ evcvtqq2ps($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
        break;
      case T_DOUBLE:
        assert(UseAVX > 2 && VM_Version::supports_avx512dq(), "required");
        __ evcvtqq2pd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
        break;

      default: assert(false, "%s", type2name(to_elem_bt));
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct vcastFtoD_reg(vec dst, vec src) %{
  predicate(Matcher::vector_element_basic_type(n) == T_DOUBLE);
  match(Set dst (VectorCastF2X src));
  format %{ "vector_cast_f2d  $dst,$src\t!" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vcvtps2pd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}


instruct castFtoX_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx10_2() &&
            !VM_Version::supports_avx512vl() &&
            Matcher::vector_length_in_bytes(n->in(1)) < 64 &&
            type2aelembytes(Matcher::vector_element_basic_type(n)) <= 4 &&
            is_integral_type(Matcher::vector_element_basic_type(n)));
  match(Set dst (VectorCastF2X src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, KILL cr);
  format %{ "vector_cast_f2x $dst,$src\t! using $xtmp1, $xtmp2, $xtmp3 and $xtmp4 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);
    // JDK-8292878 removed the need for an explicit scratch register needed to load greater than
    // 32 bit addresses for register indirect addressing mode since stub constants
    // are part of code cache and there is a cap of 2G on ReservedCodeCacheSize currently.
    // However, targets are free to increase this limit, but having a large code cache size
    // greater than 2G looks unreasonable in practical scenario, on the hind side with given
    // cap we save a temporary register allocation which in limiting case can prevent
    // spilling in high register pressure blocks.
    __ vector_castF2X_avx(to_elem_bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
                          $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $xtmp4$$XMMRegister,
                          ExternalAddress(vector_float_signflip()), noreg, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct castFtoX_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx10_2() &&
            (VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n->in(1)) == 64) &&
            is_integral_type(Matcher::vector_element_basic_type(n)));
  match(Set dst (VectorCastF2X src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, KILL cr);
  format %{ "vector_cast_f2x $dst,$src\t! using $xtmp1, $xtmp2, $ktmp1 and $ktmp2 as TEMP" %}
  ins_encode %{
    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);
    if (to_elem_bt == T_LONG) {
      int vlen_enc = vector_length_encoding(this);
      __ vector_castF2L_evex($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
                             $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister,
                             ExternalAddress(vector_double_signflip()), noreg, vlen_enc);
    } else {
      int vlen_enc = vector_length_encoding(this, $src);
      __ vector_castF2X_evex(to_elem_bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
                             $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister,
                             ExternalAddress(vector_float_signflip()), noreg, vlen_enc);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct castFtoX_reg_avx10(vec dst, vec src) %{
  predicate(VM_Version::supports_avx10_2() &&
            is_integral_type(Matcher::vector_element_basic_type(n)));
  match(Set dst (VectorCastF2X src));
  format %{ "vector_cast_f2x_avx10 $dst, $src\t!" %}
  ins_encode %{
    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);
    int vlen_enc = (to_elem_bt == T_LONG) ? vector_length_encoding(this) : vector_length_encoding(this, $src);
    __ vector_castF2X_avx10(to_elem_bt, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct castFtoX_mem_avx10(vec dst, memory src) %{
  predicate(VM_Version::supports_avx10_2() &&
            is_integral_type(Matcher::vector_element_basic_type(n)));
  match(Set dst (VectorCastF2X (LoadVector src)));
  format %{ "vector_cast_f2x_avx10 $dst, $src\t!" %}
  ins_encode %{
    int vlen = Matcher::vector_length(this);
    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);
    int vlen_enc = (to_elem_bt == T_LONG) ? vector_length_encoding(this) : vector_length_encoding(vlen * sizeof(jfloat));
    __ vector_castF2X_avx10(to_elem_bt, $dst$$XMMRegister, $src$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vcastDtoF_reg(vec dst, vec src) %{
  predicate(Matcher::vector_element_basic_type(n) == T_FLOAT);
  match(Set dst (VectorCastD2X src));
  format %{ "vector_cast_d2x  $dst,$src\t!" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    __ vcvtpd2ps($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct castDtoX_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, vec xtmp5, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx10_2() &&
            !VM_Version::supports_avx512vl() &&
            Matcher::vector_length_in_bytes(n->in(1)) < 64 &&
            is_integral_type(Matcher::vector_element_basic_type(n)));
  match(Set dst (VectorCastD2X src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, TEMP xtmp5, KILL cr);
  format %{ "vector_cast_d2x $dst,$src\t! using $xtmp1, $xtmp2, $xtmp3, $xtmp4 and $xtmp5 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);
    __ vector_castD2X_avx(to_elem_bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
                          $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $xtmp4$$XMMRegister, $xtmp5$$XMMRegister,
                          ExternalAddress(vector_float_signflip()), noreg, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct castDtoX_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx10_2() &&
            (VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n->in(1)) == 64) &&
            is_integral_type(Matcher::vector_element_basic_type(n)));
  match(Set dst (VectorCastD2X src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, KILL cr);
  format %{ "vector_cast_d2x $dst,$src\t! using $xtmp1, $xtmp2, $ktmp1 and $ktmp2 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);
    AddressLiteral signflip = VM_Version::supports_avx512dq() ? ExternalAddress(vector_double_signflip()) :
                              ExternalAddress(vector_float_signflip());
    __ vector_castD2X_evex(to_elem_bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
                           $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister, signflip, noreg, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct castDtoX_reg_avx10(vec dst, vec src) %{
  predicate(VM_Version::supports_avx10_2() &&
            is_integral_type(Matcher::vector_element_basic_type(n)));
  match(Set dst (VectorCastD2X src));
  format %{ "vector_cast_d2x_avx10 $dst, $src\t!" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);
    __ vector_castD2X_avx10(to_elem_bt, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct castDtoX_mem_avx10(vec dst, memory src) %{
  predicate(VM_Version::supports_avx10_2() &&
            is_integral_type(Matcher::vector_element_basic_type(n)));
  match(Set dst (VectorCastD2X (LoadVector src)));
  format %{ "vector_cast_d2x_avx10 $dst, $src\t!" %}
  ins_encode %{
    int vlen = Matcher::vector_length(this);
    int vlen_enc = vector_length_encoding(vlen * sizeof(jdouble));
    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);
    __ vector_castD2X_avx10(to_elem_bt, $dst$$XMMRegister, $src$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vucast(vec dst, vec src) %{
  match(Set dst (VectorUCastB2X src));
  match(Set dst (VectorUCastS2X src));
  match(Set dst (VectorUCastI2X src));
  format %{ "vector_ucast $dst,$src\t!" %}
  ins_encode %{
    assert(UseAVX > 0, "required");

    BasicType from_elem_bt = Matcher::vector_element_basic_type(this, $src);
    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);
    int vlen_enc = vector_length_encoding(this);
    __ vector_unsigned_cast($dst$$XMMRegister, $src$$XMMRegister, vlen_enc, from_elem_bt, to_elem_bt);
  %}
  ins_pipe( pipe_slow );
%}

instruct vround_float_avx(vec dst, vec src, rRegP tmp, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx512vl() &&
            Matcher::vector_length_in_bytes(n) < 64 &&
            Matcher::vector_element_basic_type(n) == T_INT);
  match(Set dst (RoundVF src));
  effect(TEMP dst, TEMP tmp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, KILL cr);
  format %{ "vector_round_float $dst,$src\t! using $tmp, $xtmp1, $xtmp2, $xtmp3, $xtmp4 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    InternalAddress new_mxcsr = $constantaddress((jint)(EnableX86ECoreOpts ? 0x3FBF : 0x3F80));
    __ vector_round_float_avx($dst$$XMMRegister, $src$$XMMRegister,
                              ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), new_mxcsr, vlen_enc,
                              $tmp$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $xtmp4$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vround_float_evex(vec dst, vec src, rRegP tmp, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rFlagsReg cr) %{
  predicate((VM_Version::supports_avx512vl() ||
             Matcher::vector_length_in_bytes(n) == 64) &&
             Matcher::vector_element_basic_type(n) == T_INT);
  match(Set dst (RoundVF src));
  effect(TEMP dst, TEMP tmp, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, KILL cr);
  format %{ "vector_round_float $dst,$src\t! using $tmp, $xtmp1, $xtmp2, $ktmp1, $ktmp2 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    InternalAddress new_mxcsr = $constantaddress((jint)(EnableX86ECoreOpts ? 0x3FBF : 0x3F80));
    __ vector_round_float_evex($dst$$XMMRegister, $src$$XMMRegister,
                               ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), new_mxcsr, vlen_enc,
                               $tmp$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vround_reg_evex(vec dst, vec src, rRegP tmp, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rFlagsReg cr) %{
  predicate(Matcher::vector_element_basic_type(n) == T_LONG);
  match(Set dst (RoundVD src));
  effect(TEMP dst, TEMP tmp, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2,  KILL cr);
  format %{ "vector_round_long $dst,$src\t! using $tmp, $xtmp1, $xtmp2, $ktmp1, $ktmp2 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    InternalAddress new_mxcsr = $constantaddress((jint)(EnableX86ECoreOpts ? 0x3FBF : 0x3F80));
    __ vector_round_double_evex($dst$$XMMRegister, $src$$XMMRegister,
                                ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), new_mxcsr, vlen_enc,
                                $tmp$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- VectorMaskCmp --------------------------------------

instruct vcmpFD(legVec dst, legVec src1, legVec src2, immI8 cond) %{
  predicate(n->bottom_type()->isa_vectmask() == nullptr &&
            Matcher::vector_length_in_bytes(n->in(1)->in(1)) >=  8 && // src1
            Matcher::vector_length_in_bytes(n->in(1)->in(1)) <= 32 && // src1
            is_floating_point_type(Matcher::vector_element_basic_type(n->in(1)->in(1)))); // src1 T_FLOAT, T_DOUBLE
  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));
  format %{ "vector_compare $dst,$src1,$src2,$cond\t!" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src1);
    Assembler::ComparisonPredicateFP cmp = booltest_pred_to_comparison_pred_fp($cond$$constant);
    if (Matcher::vector_element_basic_type(this, $src1) == T_FLOAT) {
      __ vcmpps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);
    } else {
      __ vcmppd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct evcmpFD64(vec dst, vec src1, vec src2, immI8 cond, kReg ktmp) %{
  predicate(Matcher::vector_length_in_bytes(n->in(1)->in(1)) == 64 && // src1
            n->bottom_type()->isa_vectmask() == nullptr &&
            is_floating_point_type(Matcher::vector_element_basic_type(n->in(1)->in(1)))); // src1 T_FLOAT, T_DOUBLE
  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));
  effect(TEMP ktmp);
  format %{ "vector_compare $dst,$src1,$src2,$cond" %}
  ins_encode %{
    int vlen_enc = Assembler::AVX_512bit;
    Assembler::ComparisonPredicateFP cmp = booltest_pred_to_comparison_pred_fp($cond$$constant);
    KRegister mask = k0; // The comparison itself is not being masked.
    if (Matcher::vector_element_basic_type(this, $src1) == T_FLOAT) {
      __ evcmpps($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);
      __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), false, vlen_enc, noreg);
    } else {
      __ evcmppd($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);
      __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), false, vlen_enc, noreg);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct evcmpFD(kReg dst, vec src1, vec src2, immI8 cond) %{
  predicate(n->bottom_type()->isa_vectmask() &&
            is_floating_point_type(Matcher::vector_element_basic_type(n->in(1)->in(1)))); // src1 T_FLOAT, T_DOUBLE
  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));
  format %{ "vector_compare_evex $dst,$src1,$src2,$cond\t!" %}
  ins_encode %{
    assert(bottom_type()->isa_vectmask(), "TypeVectMask expected");
    int vlen_enc = vector_length_encoding(this, $src1);
    Assembler::ComparisonPredicateFP cmp = booltest_pred_to_comparison_pred_fp($cond$$constant);
    KRegister mask = k0; // The comparison itself is not being masked.
    if (Matcher::vector_element_basic_type(this, $src1) == T_FLOAT) {
      __ evcmpps($dst$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);
    } else {
      __ evcmppd($dst$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct vcmp_direct(legVec dst, legVec src1, legVec src2, immI8 cond) %{
  predicate(n->bottom_type()->isa_vectmask() == nullptr &&
            !Matcher::is_unsigned_booltest_pred(n->in(2)->get_int()) &&
            Matcher::vector_length_in_bytes(n->in(1)->in(1)) >=  4 && // src1
            Matcher::vector_length_in_bytes(n->in(1)->in(1)) <= 32 && // src1
            is_integral_type(Matcher::vector_element_basic_type(n->in(1)->in(1))) &&
            (n->in(2)->get_int() == BoolTest::eq ||
             n->in(2)->get_int() == BoolTest::lt ||
             n->in(2)->get_int() == BoolTest::gt)); // cond
  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));
  format %{ "vector_compare $dst,$src1,$src2,$cond\t!" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src1);
    Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);
    Assembler::Width ww = widthForType(Matcher::vector_element_basic_type(this, $src1));
    __ vpcmpCCW($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, cmp, ww, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vcmp_negate(legVec dst, legVec src1, legVec src2, immI8 cond, legVec xtmp) %{
  predicate(n->bottom_type()->isa_vectmask() == nullptr &&
            !Matcher::is_unsigned_booltest_pred(n->in(2)->get_int()) &&
            Matcher::vector_length_in_bytes(n->in(1)->in(1)) >=  4 && // src1
            Matcher::vector_length_in_bytes(n->in(1)->in(1)) <= 32 && // src1
            is_integral_type(Matcher::vector_element_basic_type(n->in(1)->in(1))) &&
            (n->in(2)->get_int() == BoolTest::ne ||
             n->in(2)->get_int() == BoolTest::le ||
             n->in(2)->get_int() == BoolTest::ge)); // cond
  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));
  effect(TEMP dst, TEMP xtmp);
  format %{ "vector_compare $dst,$src1,$src2,$cond\t! using $xtmp as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src1);
    Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);
    Assembler::Width ww = widthForType(Matcher::vector_element_basic_type(this, $src1));
    __ vpcmpCCW($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $xtmp$$XMMRegister, cmp, ww, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vcmpu(legVec dst, legVec src1, legVec src2, immI8 cond, legVec xtmp) %{
  predicate(n->bottom_type()->isa_vectmask() == nullptr &&
            Matcher::is_unsigned_booltest_pred(n->in(2)->get_int()) &&
            Matcher::vector_length_in_bytes(n->in(1)->in(1)) >=  4 && // src1
            Matcher::vector_length_in_bytes(n->in(1)->in(1)) <= 32 && // src1
            is_integral_type(Matcher::vector_element_basic_type(n->in(1)->in(1)))); // src1
  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));
  effect(TEMP dst, TEMP xtmp);
  format %{ "vector_compareu $dst,$src1,$src2,$cond\t! using $xtmp as TEMP" %}
  ins_encode %{
    InternalAddress flip_bit = $constantaddress(high_bit_set(Matcher::vector_element_basic_type(this, $src1)));
    int vlen_enc = vector_length_encoding(this, $src1);
    Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);
    Assembler::Width ww = widthForType(Matcher::vector_element_basic_type(this, $src1));

    if (vlen_enc == Assembler::AVX_128bit) {
      __ vmovddup($xtmp$$XMMRegister, flip_bit, vlen_enc, noreg);
    } else {
      __ vbroadcastsd($xtmp$$XMMRegister, flip_bit, vlen_enc, noreg);
    }
    __ vpxor($dst$$XMMRegister, $xtmp$$XMMRegister, $src1$$XMMRegister, vlen_enc);
    __ vpxor($xtmp$$XMMRegister, $xtmp$$XMMRegister, $src2$$XMMRegister, vlen_enc);
    __ vpcmpCCW($dst$$XMMRegister, $dst$$XMMRegister, $xtmp$$XMMRegister, $xtmp$$XMMRegister, cmp, ww, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vcmp64(vec dst, vec src1, vec src2, immI8 cond, kReg ktmp) %{
  predicate((n->bottom_type()->isa_vectmask() == nullptr &&
             Matcher::vector_length_in_bytes(n->in(1)->in(1)) == 64) && // src1
             is_integral_type(Matcher::vector_element_basic_type(n->in(1)->in(1)))); // src1
  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));
  effect(TEMP ktmp);
  format %{ "vector_compare $dst,$src1,$src2,$cond" %}
  ins_encode %{
    assert(UseAVX > 2, "required");

    int vlen_enc = vector_length_encoding(this, $src1);
    Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);
    bool is_unsigned = Matcher::is_unsigned_booltest_pred($cond$$constant);
    KRegister mask = k0; // The comparison itself is not being masked.
    bool merge = false;
    BasicType src1_elem_bt = Matcher::vector_element_basic_type(this, $src1);

    switch (src1_elem_bt) {
      case T_INT: {
        __ evpcmpd($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);
        __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, noreg);
        break;
      }
      case T_LONG: {
        __ evpcmpq($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);
        __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, noreg);
        break;
      }
      default: assert(false, "%s", type2name(src1_elem_bt));
    }
  %}
  ins_pipe( pipe_slow );
%}


instruct evcmp(kReg dst, vec src1, vec src2, immI8 cond) %{
  predicate(n->bottom_type()->isa_vectmask() &&
            is_integral_type(Matcher::vector_element_basic_type(n->in(1)->in(1)))); // src1
  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));
  format %{ "vector_compared_evex $dst,$src1,$src2,$cond\t!" %}
  ins_encode %{
    assert(UseAVX > 2, "required");
    assert(bottom_type()->isa_vectmask(), "TypeVectMask expected");

    int vlen_enc = vector_length_encoding(this, $src1);
    Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);
    bool is_unsigned = Matcher::is_unsigned_booltest_pred($cond$$constant);
    BasicType src1_elem_bt = Matcher::vector_element_basic_type(this, $src1);

    // Comparison i
    switch (src1_elem_bt) {
      case T_BYTE: {
        __ evpcmpb($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);
        break;
      }
      case T_SHORT: {
        __ evpcmpw($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);
        break;
      }
      case T_INT: {
        __ evpcmpd($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);
        break;
      }
      case T_LONG: {
        __ evpcmpq($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);
        break;
      }
      default: assert(false, "%s", type2name(src1_elem_bt));
    }
  %}
  ins_pipe( pipe_slow );
%}

// Extract

instruct extractI(rRegI dst, legVec src, immU8 idx) %{
  predicate(Matcher::vector_length_in_bytes(n->in(1)) <= 16); // src
  match(Set dst (ExtractI src idx));
  match(Set dst (ExtractS src idx));
  match(Set dst (ExtractB src idx));
  format %{ "extractI $dst,$src,$idx\t!" %}
  ins_encode %{
    assert($idx$$constant < (int)Matcher::vector_length(this, $src), "out of bounds");

    BasicType elem_bt = Matcher::vector_element_basic_type(this, $src);
    __ get_elem(elem_bt, $dst$$Register, $src$$XMMRegister, $idx$$constant);
  %}
  ins_pipe( pipe_slow );
%}

instruct vextractI(rRegI dst, legVec src, immI idx, legVec vtmp) %{
  predicate(Matcher::vector_length_in_bytes(n->in(1)) == 32 || // src
            Matcher::vector_length_in_bytes(n->in(1)) == 64);  // src
  match(Set dst (ExtractI src idx));
  match(Set dst (ExtractS src idx));
  match(Set dst (ExtractB src idx));
  effect(TEMP vtmp);
  format %{ "vextractI $dst,$src,$idx\t! using $vtmp as TEMP" %}
  ins_encode %{
    assert($idx$$constant < (int)Matcher::vector_length(this, $src), "out of bounds");

    BasicType elem_bt = Matcher::vector_element_basic_type(this, $src);
    XMMRegister lane_xmm = __ get_lane(elem_bt, $vtmp$$XMMRegister, $src$$XMMRegister, $idx$$constant);
    __ get_elem(elem_bt, $dst$$Register, lane_xmm, $idx$$constant);
  %}
  ins_pipe( pipe_slow );
%}

instruct extractL(rRegL dst, legVec src, immU8 idx) %{
  predicate(Matcher::vector_length(n->in(1)) <= 2); // src
  match(Set dst (ExtractL src idx));
  format %{ "extractL $dst,$src,$idx\t!" %}
  ins_encode %{
    assert(UseSSE >= 4, "required");
    assert($idx$$constant < (int)Matcher::vector_length(this, $src), "out of bounds");

    __ get_elem(T_LONG, $dst$$Register, $src$$XMMRegister, $idx$$constant);
  %}
  ins_pipe( pipe_slow );
%}

instruct vextractL(rRegL dst, legVec src, immU8 idx, legVec vtmp) %{
  predicate(Matcher::vector_length(n->in(1)) == 4 || // src
            Matcher::vector_length(n->in(1)) == 8);  // src
  match(Set dst (ExtractL src idx));
  effect(TEMP vtmp);
  format %{ "vextractL $dst,$src,$idx\t! using $vtmp as TEMP" %}
  ins_encode %{
    assert($idx$$constant < (int)Matcher::vector_length(this, $src), "out of bounds");

    XMMRegister lane_reg = __ get_lane(T_LONG, $vtmp$$XMMRegister, $src$$XMMRegister, $idx$$constant);
    __ get_elem(T_LONG, $dst$$Register, lane_reg, $idx$$constant);
  %}
  ins_pipe( pipe_slow );
%}

instruct extractF(legRegF dst, legVec src, immU8 idx, legVec vtmp) %{
  predicate(Matcher::vector_length(n->in(1)) <= 4);
  match(Set dst (ExtractF src idx));
  effect(TEMP dst, TEMP vtmp);
  format %{ "extractF $dst,$src,$idx\t! using $vtmp as TEMP" %}
  ins_encode %{
    assert($idx$$constant < (int)Matcher::vector_length(this, $src), "out of bounds");

    __ get_elem(T_FLOAT, $dst$$XMMRegister, $src$$XMMRegister, $idx$$constant, $vtmp$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vextractF(legRegF dst, legVec src, immU8 idx, legVec vtmp) %{
  predicate(Matcher::vector_length(n->in(1)/*src*/) == 8 ||
            Matcher::vector_length(n->in(1)/*src*/) == 16);
  match(Set dst (ExtractF src idx));
  effect(TEMP vtmp);
  format %{ "vextractF $dst,$src,$idx\t! using $vtmp as TEMP" %}
  ins_encode %{
    assert($idx$$constant < (int)Matcher::vector_length(this, $src), "out of bounds");

    XMMRegister lane_reg = __ get_lane(T_FLOAT, $vtmp$$XMMRegister, $src$$XMMRegister, $idx$$constant);
    __ get_elem(T_FLOAT, $dst$$XMMRegister, lane_reg, $idx$$constant);
  %}
  ins_pipe( pipe_slow );
%}

instruct extractD(legRegD dst, legVec src, immU8 idx) %{
  predicate(Matcher::vector_length(n->in(1)) == 2); // src
  match(Set dst (ExtractD src idx));
  format %{ "extractD $dst,$src,$idx\t!" %}
  ins_encode %{
    assert($idx$$constant < (int)Matcher::vector_length(this, $src), "out of bounds");

    __ get_elem(T_DOUBLE, $dst$$XMMRegister, $src$$XMMRegister, $idx$$constant);
  %}
  ins_pipe( pipe_slow );
%}

instruct vextractD(legRegD dst, legVec src, immU8 idx, legVec vtmp) %{
  predicate(Matcher::vector_length(n->in(1)) == 4 || // src
            Matcher::vector_length(n->in(1)) == 8);  // src
  match(Set dst (ExtractD src idx));
  effect(TEMP vtmp);
  format %{ "vextractD $dst,$src,$idx\t! using $vtmp as TEMP" %}
  ins_encode %{
    assert($idx$$constant < (int)Matcher::vector_length(this, $src), "out of bounds");

    XMMRegister lane_reg = __ get_lane(T_DOUBLE, $vtmp$$XMMRegister, $src$$XMMRegister, $idx$$constant);
    __ get_elem(T_DOUBLE, $dst$$XMMRegister, lane_reg, $idx$$constant);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- Vector Blend --------------------------------------

instruct blendvp(vec dst, vec src, vec mask, rxmm0 tmp) %{
  predicate(UseAVX == 0);
  match(Set dst (VectorBlend (Binary dst src) mask));
  format %{ "vector_blend  $dst,$src,$mask\t! using $tmp as TEMP" %}
  effect(TEMP tmp);
  ins_encode %{
    assert(UseSSE >= 4, "required");

    if ($mask$$XMMRegister != $tmp$$XMMRegister) {
      __ movdqu($tmp$$XMMRegister, $mask$$XMMRegister);
    }
    __ pblendvb($dst$$XMMRegister, $src$$XMMRegister); // uses xmm0 as mask
  %}
  ins_pipe( pipe_slow );
%}

instruct vblendvpI(legVec dst, legVec src1, legVec src2, legVec mask) %{
  predicate(UseAVX > 0 && !EnableX86ECoreOpts &&
            n->in(2)->bottom_type()->isa_vectmask() == nullptr &&
            Matcher::vector_length_in_bytes(n) <= 32 &&
            is_integral_type(Matcher::vector_element_basic_type(n)));
  match(Set dst (VectorBlend (Binary src1 src2) mask));
  format %{ "vector_blend  $dst,$src1,$src2,$mask\t!" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpblendvb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $mask$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vblendvpFD(legVec dst, legVec src1, legVec src2, legVec mask) %{
  predicate(UseAVX > 0 && !EnableX86ECoreOpts &&
            n->in(2)->bottom_type()->isa_vectmask() == nullptr &&
            Matcher::vector_length_in_bytes(n) <= 32 &&
            !is_integral_type(Matcher::vector_element_basic_type(n)));
  match(Set dst (VectorBlend (Binary src1 src2) mask));
  format %{ "vector_blend  $dst,$src1,$src2,$mask\t!" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vblendvps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $mask$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vblendvp(legVec dst, legVec src1, legVec src2, legVec mask, legVec vtmp) %{
  predicate(UseAVX > 0 && EnableX86ECoreOpts &&
            n->in(2)->bottom_type()->isa_vectmask() == nullptr &&
            Matcher::vector_length_in_bytes(n) <= 32);
  match(Set dst (VectorBlend (Binary src1 src2) mask));
  format %{ "vector_blend  $dst,$src1,$src2,$mask\t! using $vtmp as TEMP" %}
  effect(TEMP vtmp, TEMP dst);
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpandn($vtmp$$XMMRegister, $mask$$XMMRegister, $src1$$XMMRegister, vlen_enc);
    __ vpand ($dst$$XMMRegister,  $mask$$XMMRegister, $src2$$XMMRegister, vlen_enc);
    __ vpor  ($dst$$XMMRegister,  $dst$$XMMRegister,  $vtmp$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct evblendvp64(vec dst, vec src1, vec src2, vec mask, kReg ktmp) %{
  predicate(Matcher::vector_length_in_bytes(n) == 64 &&
            n->in(2)->bottom_type()->isa_vectmask() == nullptr);
  match(Set dst (VectorBlend (Binary src1 src2) mask));
  format %{ "vector_blend  $dst,$src1,$src2,$mask\t! using k2 as TEMP" %}
  effect(TEMP ktmp);
  ins_encode %{
     int vlen_enc = Assembler::AVX_512bit;
     BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ evpcmp(elem_bt, $ktmp$$KRegister, k0, $mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), Assembler::eq, vlen_enc, noreg);
    __ evpblend(elem_bt, $dst$$XMMRegister, $ktmp$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}


instruct evblendvp64_masked(vec dst, vec src1, vec src2, kReg mask) %{
  predicate(n->in(2)->bottom_type()->isa_vectmask() &&
            (!is_subword_type(Matcher::vector_element_basic_type(n)) ||
             VM_Version::supports_avx512bw()));
  match(Set dst (VectorBlend (Binary src1 src2) mask));
  format %{ "vector_blend  $dst,$src1,$src2,$mask\t! using k2 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ evpblend(elem_bt, $dst$$XMMRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- ABS --------------------------------------
// a = |a|
instruct vabsB_reg(vec dst, vec src) %{
  match(Set dst (AbsVB  src));
  format %{ "vabsb $dst,$src\t# $dst = |$src| abs packedB" %}
  ins_encode %{
    uint vlen = Matcher::vector_length(this);
    if (vlen <= 16) {
      __ pabsb($dst$$XMMRegister, $src$$XMMRegister);
    } else {
      int vlen_enc = vector_length_encoding(this);
      __ vpabsb($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct vabsS_reg(vec dst, vec src) %{
  match(Set dst (AbsVS  src));
  format %{ "vabsw $dst,$src\t# $dst = |$src| abs packedS" %}
  ins_encode %{
    uint vlen = Matcher::vector_length(this);
    if (vlen <= 8) {
      __ pabsw($dst$$XMMRegister, $src$$XMMRegister);
    } else {
      int vlen_enc = vector_length_encoding(this);
      __ vpabsw($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct vabsI_reg(vec dst, vec src) %{
  match(Set dst (AbsVI  src));
  format %{ "pabsd $dst,$src\t# $dst = |$src| abs packedI" %}
  ins_encode %{
    uint vlen = Matcher::vector_length(this);
    if (vlen <= 4) {
      __ pabsd($dst$$XMMRegister, $src$$XMMRegister);
    } else {
      int vlen_enc = vector_length_encoding(this);
      __ vpabsd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct vabsL_reg(vec dst, vec src) %{
  match(Set dst (AbsVL  src));
  format %{ "evpabsq $dst,$src\t# $dst = |$src| abs packedL" %}
  ins_encode %{
    assert(UseAVX > 2, "required");
    int vlen_enc = vector_length_encoding(this);
    if (!VM_Version::supports_avx512vl()) {
      vlen_enc = Assembler::AVX_512bit;
    }
    __ evpabsq($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- ABSNEG --------------------------------------

instruct vabsnegF(vec dst, vec src) %{
  predicate(Matcher::vector_length(n) != 4); // handled by 1-operand instruction vabsneg4F
  match(Set dst (AbsVF src));
  match(Set dst (NegVF src));
  format %{ "vabsnegf $dst,$src,[mask]\t# absneg packedF" %}
  ins_cost(150);
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen = Matcher::vector_length(this);
    if (vlen == 2) {
      __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister);
    } else {
      assert(vlen == 8 || vlen == 16, "required");
      int vlen_enc = vector_length_encoding(this);
      __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct vabsneg4F(vec dst) %{
  predicate(Matcher::vector_length(n) == 4);
  match(Set dst (AbsVF dst));
  match(Set dst (NegVF dst));
  format %{ "vabsnegf $dst,[mask]\t# absneg packed4F" %}
  ins_cost(150);
  ins_encode %{
    int opcode = this->ideal_Opcode();
    __ vabsnegf(opcode, $dst$$XMMRegister, $dst$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vabsnegD(vec dst, vec src) %{
  match(Set dst (AbsVD  src));
  match(Set dst (NegVD  src));
  format %{ "vabsnegd $dst,$src,[mask]\t# absneg packedD" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    uint vlen = Matcher::vector_length(this);
    if (vlen == 2) {
      __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister);
    } else {
      int vlen_enc = vector_length_encoding(this);
      __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
    }
  %}
  ins_pipe( pipe_slow );
%}

//------------------------------------- VectorTest --------------------------------------------

instruct vptest_lt16(rFlagsRegU cr, legVec src1, legVec src2, legVec vtmp) %{
  predicate(Matcher::vector_length_in_bytes(n->in(1)) < 16);
  match(Set cr (VectorTest src1 src2));
  effect(TEMP vtmp);
  format %{ "vptest_lt16  $src1, $src2\t! using $vtmp as TEMP" %}
  ins_encode %{
    BasicType bt = Matcher::vector_element_basic_type(this, $src1);
    int vlen = Matcher::vector_length_in_bytes(this, $src1);
    __ vectortest(bt, $src1$$XMMRegister, $src2$$XMMRegister, $vtmp$$XMMRegister, vlen);
  %}
  ins_pipe( pipe_slow );
%}

instruct vptest_ge16(rFlagsRegU cr, legVec src1, legVec src2) %{
  predicate(Matcher::vector_length_in_bytes(n->in(1)) >= 16);
  match(Set cr (VectorTest src1 src2));
  format %{ "vptest_ge16  $src1, $src2\n\t" %}
  ins_encode %{
    BasicType bt = Matcher::vector_element_basic_type(this, $src1);
    int vlen = Matcher::vector_length_in_bytes(this, $src1);
    __ vectortest(bt, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, vlen);
  %}
  ins_pipe( pipe_slow );
%}

instruct ktest_alltrue_le8(rFlagsRegU cr, kReg src1, kReg src2, rRegI tmp) %{
  predicate((Matcher::vector_length(n->in(1)) < 8 ||
             (Matcher::vector_length(n->in(1)) == 8 && !VM_Version::supports_avx512dq())) &&
            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);
  match(Set cr (VectorTest src1 src2));
  effect(TEMP tmp);
  format %{ "ktest_alltrue_le8  $src1, $src2\t! using $tmp as TEMP" %}
  ins_encode %{
    uint masklen = Matcher::vector_length(this, $src1);
    __ kmovwl($tmp$$Register, $src1$$KRegister);
    __ andl($tmp$$Register, (1 << masklen) - 1);
    __ cmpl($tmp$$Register, (1 << masklen) - 1);
  %}
  ins_pipe( pipe_slow );
%}

instruct ktest_anytrue_le8(rFlagsRegU cr, kReg src1, kReg src2, rRegI tmp) %{
  predicate((Matcher::vector_length(n->in(1)) < 8 ||
             (Matcher::vector_length(n->in(1)) == 8 && !VM_Version::supports_avx512dq())) &&
            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne);
  match(Set cr (VectorTest src1 src2));
  effect(TEMP tmp);
  format %{ "ktest_anytrue_le8  $src1, $src2\t! using $tmp as TEMP" %}
  ins_encode %{
    uint masklen = Matcher::vector_length(this, $src1);
    __ kmovwl($tmp$$Register, $src1$$KRegister);
    __ andl($tmp$$Register, (1 << masklen) - 1);
  %}
  ins_pipe( pipe_slow );
%}

instruct ktest_ge8(rFlagsRegU cr, kReg src1, kReg src2) %{
  predicate(Matcher::vector_length(n->in(1)) >= 16 ||
            (Matcher::vector_length(n->in(1)) == 8 && VM_Version::supports_avx512dq()));
  match(Set cr (VectorTest src1 src2));
  format %{ "ktest_ge8  $src1, $src2\n\t" %}
  ins_encode %{
    uint masklen = Matcher::vector_length(this, $src1);
    __ kortest(masklen, $src1$$KRegister, $src1$$KRegister);
  %}
  ins_pipe( pipe_slow );
%}

//------------------------------------- LoadMask --------------------------------------------

instruct loadMask(legVec dst, legVec src) %{
  predicate(n->bottom_type()->isa_vectmask() == nullptr && !VM_Version::supports_avx512vlbw());
  match(Set dst (VectorLoadMask src));
  effect(TEMP dst);
  format %{ "vector_loadmask_byte $dst, $src\n\t" %}
  ins_encode %{
    int vlen_in_bytes = Matcher::vector_length_in_bytes(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ load_vector_mask($dst$$XMMRegister, $src$$XMMRegister, vlen_in_bytes, elem_bt, true);
  %}
  ins_pipe( pipe_slow );
%}

instruct loadMask64(kReg dst, vec src, vec xtmp) %{
  predicate(n->bottom_type()->isa_vectmask() && !VM_Version::supports_avx512vlbw());
  match(Set dst (VectorLoadMask src));
  effect(TEMP xtmp);
  format %{ "vector_loadmask_64byte $dst, $src\t! using $xtmp as TEMP" %}
  ins_encode %{
    __ load_vector_mask($dst$$KRegister, $src$$XMMRegister, $xtmp$$XMMRegister,
                        true, Assembler::AVX_512bit);
  %}
  ins_pipe( pipe_slow );
%}

instruct loadMask_evex(kReg dst, vec src,  vec xtmp) %{
  predicate(n->bottom_type()->isa_vectmask() && VM_Version::supports_avx512vlbw());
  match(Set dst (VectorLoadMask src));
  effect(TEMP xtmp);
  format %{ "vector_loadmask_byte $dst, $src\t! using $xtmp as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(in(1));
    __ load_vector_mask($dst$$KRegister, $src$$XMMRegister, $xtmp$$XMMRegister,
                        false, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

//------------------------------------- StoreMask --------------------------------------------

instruct vstoreMask1B(vec dst, vec src, immI_1 size) %{
  predicate(Matcher::vector_length(n) < 64 && n->in(1)->bottom_type()->isa_vectmask() == nullptr);
  match(Set dst (VectorStoreMask src size));
  format %{ "vector_store_mask $dst, $src \t! elem size is $size byte[s]" %}
  ins_encode %{
    int vlen = Matcher::vector_length(this);
    if (vlen <= 16 && UseAVX <= 2) {
      assert(UseSSE >= 3, "required");
      __ pabsb($dst$$XMMRegister, $src$$XMMRegister);
    } else {
      assert(UseAVX > 0, "required");
      int src_vlen_enc = vector_length_encoding(this, $src);
      __ vpabsb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct vstoreMask2B(vec dst, vec src, vec xtmp, immI_2 size) %{
  predicate(Matcher::vector_length(n) <= 16 && n->in(1)->bottom_type()->isa_vectmask() == nullptr);
  match(Set dst (VectorStoreMask src size));
  effect(TEMP_DEF dst, TEMP xtmp);
  format %{ "vector_store_mask $dst, $src \t! elem size is $size byte[s]" %}
  ins_encode %{
    int vlen_enc = Assembler::AVX_128bit;
    int vlen = Matcher::vector_length(this);
    if (vlen <= 8) {
      assert(UseSSE >= 3, "required");
      __ pxor($xtmp$$XMMRegister, $xtmp$$XMMRegister);
      __ pabsw($dst$$XMMRegister, $src$$XMMRegister);
      __ packuswb($dst$$XMMRegister, $xtmp$$XMMRegister);
    } else {
      assert(UseAVX > 0, "required");
      __ vextracti128($dst$$XMMRegister, $src$$XMMRegister, 0x1);
      __ vpacksswb($dst$$XMMRegister, $src$$XMMRegister, $dst$$XMMRegister, vlen_enc);
      __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct vstoreMask4B(vec dst, vec src, vec xtmp, immI_4 size) %{
  predicate(UseAVX <= 2 && Matcher::vector_length(n) <= 8 && n->in(1)->bottom_type()->isa_vectmask() == nullptr);
  match(Set dst (VectorStoreMask src size));
  format %{ "vector_store_mask $dst, $src \t! elem size is $size byte[s]" %}
  effect(TEMP_DEF dst, TEMP xtmp);
  ins_encode %{
    int vlen_enc = Assembler::AVX_128bit;
    int vlen = Matcher::vector_length(this);
    if (vlen <= 4) {
      assert(UseSSE >= 3, "required");
      __ pxor($xtmp$$XMMRegister, $xtmp$$XMMRegister);
      __ pabsd($dst$$XMMRegister, $src$$XMMRegister);
      __ packusdw($dst$$XMMRegister, $xtmp$$XMMRegister);
      __ packuswb($dst$$XMMRegister, $xtmp$$XMMRegister);
    } else {
      assert(UseAVX > 0, "required");
      __ vpxor($xtmp$$XMMRegister, $xtmp$$XMMRegister, $xtmp$$XMMRegister, vlen_enc);
      __ vextracti128($dst$$XMMRegister, $src$$XMMRegister, 0x1);
      __ vpackssdw($dst$$XMMRegister, $src$$XMMRegister, $dst$$XMMRegister, vlen_enc);
      __ vpacksswb($dst$$XMMRegister, $dst$$XMMRegister, $xtmp$$XMMRegister, vlen_enc);
      __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct storeMask8B(vec dst, vec src, vec xtmp, immI_8 size) %{
  predicate(UseAVX <= 2 && Matcher::vector_length(n) == 2);
  match(Set dst (VectorStoreMask src size));
  effect(TEMP_DEF dst, TEMP xtmp);
  format %{ "vector_store_mask $dst, $src \t! elem size is $size byte[s]" %}
  ins_encode %{
    assert(UseSSE >= 3, "required");
    __ pxor($xtmp$$XMMRegister, $xtmp$$XMMRegister);
    __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x8);
    __ pabsd($dst$$XMMRegister, $dst$$XMMRegister);
    __ packusdw($dst$$XMMRegister, $xtmp$$XMMRegister);
    __ packuswb($dst$$XMMRegister, $xtmp$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct storeMask8B_avx(vec dst, vec src, immI_8 size, vec vtmp) %{
  predicate(UseAVX <= 2 && Matcher::vector_length(n) == 4);
  match(Set dst (VectorStoreMask src size));
  format %{ "vector_store_mask $dst, $src \t! elem size is $size byte[s], using $vtmp as TEMP" %}
  effect(TEMP_DEF dst, TEMP vtmp);
  ins_encode %{
    int vlen_enc = Assembler::AVX_128bit;
    __ vshufps($dst$$XMMRegister, $src$$XMMRegister, $src$$XMMRegister, 0x88, Assembler::AVX_256bit);
    __ vextracti128($vtmp$$XMMRegister, $dst$$XMMRegister, 0x1);
    __ vblendps($dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, 0xC, vlen_enc);
    __ vpxor($vtmp$$XMMRegister, $vtmp$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);
    __ vpackssdw($dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);
    __ vpacksswb($dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);
    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vstoreMask4B_evex_novectmask(vec dst, vec src, immI_4 size) %{
  predicate(UseAVX > 2 && n->in(1)->bottom_type()->isa_vectmask() == nullptr);
  match(Set dst (VectorStoreMask src size));
  format %{ "vector_store_mask $dst, $src \t! elem size is $size byte[s]" %}
  ins_encode %{
    int src_vlen_enc = vector_length_encoding(this, $src);
    int dst_vlen_enc = vector_length_encoding(this);
    if (!VM_Version::supports_avx512vl()) {
      src_vlen_enc = Assembler::AVX_512bit;
    }
    __ evpmovdb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);
    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vstoreMask8B_evex_novectmask(vec dst, vec src, immI_8 size) %{
  predicate(UseAVX > 2 && n->in(1)->bottom_type()->isa_vectmask() == nullptr);
  match(Set dst (VectorStoreMask src size));
  format %{ "vector_store_mask $dst, $src \t! elem size is $size byte[s]" %}
  ins_encode %{
    int src_vlen_enc = vector_length_encoding(this, $src);
    int dst_vlen_enc = vector_length_encoding(this);
    if (!VM_Version::supports_avx512vl()) {
      src_vlen_enc = Assembler::AVX_512bit;
    }
    __ evpmovqb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);
    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vstoreMask_evex_vectmask(vec dst, kReg mask, immI size) %{
  predicate(n->in(1)->bottom_type()->isa_vectmask() && !VM_Version::supports_avx512vlbw());
  match(Set dst (VectorStoreMask mask size));
  effect(TEMP_DEF dst);
  format %{ "vector_store_mask $dst, $mask \t! elem size is $size byte[s]" %}
  ins_encode %{
    assert(Matcher::vector_length_in_bytes(this, $mask) == 64, "");
    __ evmovdqul($dst$$XMMRegister, $mask$$KRegister, ExternalAddress(vector_int_mask_cmp_bits()),
                 false, Assembler::AVX_512bit, noreg);
    __ evpmovdb($dst$$XMMRegister, $dst$$XMMRegister, Assembler::AVX_512bit);
  %}
  ins_pipe( pipe_slow );
%}

instruct vstoreMask_evex(vec dst, kReg mask, immI size) %{
  predicate(n->in(1)->bottom_type()->isa_vectmask() && VM_Version::supports_avx512vlbw());
  match(Set dst (VectorStoreMask mask size));
  effect(TEMP_DEF dst);
  format %{ "vector_store_mask $dst, $mask \t! elem size is $size byte[s]" %}
  ins_encode %{
    int dst_vlen_enc = vector_length_encoding(this);
    __ evpmovm2b($dst$$XMMRegister, $mask$$KRegister, dst_vlen_enc);
    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmaskcast_evex(kReg dst) %{
  match(Set dst (VectorMaskCast dst));
  ins_cost(0);
  format %{ "vector_mask_cast $dst" %}
  ins_encode %{
    // empty
  %}
  ins_pipe(empty);
%}

instruct vmaskcast(vec dst) %{
  predicate(Matcher::vector_length_in_bytes(n) == Matcher::vector_length_in_bytes(n->in(1)));
  match(Set dst (VectorMaskCast dst));
  ins_cost(0);
  format %{ "vector_mask_cast $dst" %}
  ins_encode %{
    // empty
  %}
  ins_pipe(empty);
%}

instruct vmaskcast_avx(vec dst, vec src) %{
  predicate(Matcher::vector_length_in_bytes(n) != Matcher::vector_length_in_bytes(n->in(1)));
  match(Set dst (VectorMaskCast src));
  format %{ "vector_mask_cast $dst, $src" %}
  ins_encode %{
    int vlen = Matcher::vector_length(this);
    BasicType src_bt = Matcher::vector_element_basic_type(this, $src);
    BasicType dst_bt = Matcher::vector_element_basic_type(this);
    __ vector_mask_cast($dst$$XMMRegister, $src$$XMMRegister, dst_bt, src_bt, vlen);
  %}
  ins_pipe(pipe_slow);
%}

//-------------------------------- Load Iota Indices ----------------------------------

instruct loadIotaIndices(vec dst, immI_0 src) %{
  match(Set dst (VectorLoadConst src));
  format %{ "vector_load_iota $dst CONSTANT_MEMORY\t! load iota indices" %}
  ins_encode %{
     int vlen_in_bytes = Matcher::vector_length_in_bytes(this);
     BasicType bt = Matcher::vector_element_basic_type(this);
     __ load_iota_indices($dst$$XMMRegister, vlen_in_bytes, bt);
  %}
  ins_pipe( pipe_slow );
%}

instruct VectorPopulateIndex(vec dst, rRegI src1, immI_1 src2, vec vtmp) %{
  match(Set dst (PopulateIndex src1 src2));
  effect(TEMP dst, TEMP vtmp);
  format %{ "vector_populate_index $dst $src1 $src2\t! using $vtmp as TEMP" %}
  ins_encode %{
     assert($src2$$constant == 1, "required");
     int vlen_in_bytes = Matcher::vector_length_in_bytes(this);
     int vlen_enc = vector_length_encoding(this);
     BasicType elem_bt = Matcher::vector_element_basic_type(this);
     __ vpbroadcast(elem_bt, $vtmp$$XMMRegister, $src1$$Register, vlen_enc);
     __ load_iota_indices($dst$$XMMRegister, vlen_in_bytes, elem_bt);
     __ vpadd(elem_bt, $dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct VectorPopulateLIndex(vec dst, rRegL src1, immI_1 src2, vec vtmp) %{
  match(Set dst (PopulateIndex src1 src2));
  effect(TEMP dst, TEMP vtmp);
  format %{ "vector_populate_index $dst $src1 $src2\t! using $vtmp as TEMP" %}
  ins_encode %{
     assert($src2$$constant == 1, "required");
     int vlen_in_bytes = Matcher::vector_length_in_bytes(this);
     int vlen_enc = vector_length_encoding(this);
     BasicType elem_bt = Matcher::vector_element_basic_type(this);
     __ vpbroadcast(elem_bt, $vtmp$$XMMRegister, $src1$$Register, vlen_enc);
     __ load_iota_indices($dst$$XMMRegister, vlen_in_bytes, elem_bt);
     __ vpadd(elem_bt, $dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

//-------------------------------- Rearrange ----------------------------------

// LoadShuffle/Rearrange for Byte
instruct rearrangeB(vec dst, vec shuffle) %{
  predicate(Matcher::vector_element_basic_type(n) == T_BYTE &&
            Matcher::vector_length(n) < 32);
  match(Set dst (VectorRearrange dst shuffle));
  format %{ "vector_rearrange $dst, $shuffle, $dst" %}
  ins_encode %{
    assert(UseSSE >= 4, "required");
    __ pshufb($dst$$XMMRegister, $shuffle$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct rearrangeB_avx(legVec dst, legVec src, vec shuffle, legVec vtmp1, legVec vtmp2) %{
  predicate(Matcher::vector_element_basic_type(n) == T_BYTE &&
            Matcher::vector_length(n) == 32 && !VM_Version::supports_avx512_vbmi());
  match(Set dst (VectorRearrange src shuffle));
  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_rearrange $dst, $shuffle, $src\t! using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    assert(UseAVX >= 2, "required");
    // Swap src into vtmp1
    __ vperm2i128($vtmp1$$XMMRegister, $src$$XMMRegister, $src$$XMMRegister, 1);
    // Shuffle swapped src to get entries from other 128 bit lane
    __ vpshufb($vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $shuffle$$XMMRegister, Assembler::AVX_256bit);
    // Shuffle original src to get entries from self 128 bit lane
    __ vpshufb($dst$$XMMRegister, $src$$XMMRegister, $shuffle$$XMMRegister, Assembler::AVX_256bit);
    // Create a blend mask by setting high bits for entries coming from other lane in shuffle
    __ vpaddb($vtmp2$$XMMRegister, $shuffle$$XMMRegister, ExternalAddress(vector_byte_shufflemask()), Assembler::AVX_256bit, noreg);
    // Perform the blend
    __ vpblendvb($dst$$XMMRegister, $dst$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, Assembler::AVX_256bit);
  %}
  ins_pipe( pipe_slow );
%}


instruct rearrangeB_evex(vec dst, vec src, vec shuffle, vec xtmp1, vec xtmp2, vec xtmp3, kReg ktmp, rRegI rtmp) %{
  predicate(Matcher::vector_element_basic_type(n) == T_BYTE &&
            Matcher::vector_length(n) > 32 && !VM_Version::supports_avx512_vbmi());
  match(Set dst (VectorRearrange src shuffle));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP ktmp, TEMP rtmp);
  format %{ "vector_rearrange $dst, $shuffle, $src!\t using $xtmp1, $xtmp2, $xtmp3, $rtmp and $ktmp as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ rearrange_bytes($dst$$XMMRegister, $shuffle$$XMMRegister, $src$$XMMRegister,
                       $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $xtmp3$$XMMRegister,
                       $rtmp$$Register, $ktmp$$KRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct rearrangeB_evex_vbmi(vec dst, vec src, vec shuffle) %{
  predicate(Matcher::vector_element_basic_type(n) == T_BYTE &&
            Matcher::vector_length(n) >= 32 && VM_Version::supports_avx512_vbmi());
  match(Set dst (VectorRearrange src shuffle));
  format %{ "vector_rearrange $dst, $shuffle, $src" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpermb($dst$$XMMRegister, $shuffle$$XMMRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// LoadShuffle/Rearrange for Short

instruct loadShuffleS(vec dst, vec src, vec vtmp) %{
  predicate(Matcher::vector_element_basic_type(n) == T_SHORT &&
            !VM_Version::supports_avx512bw());
  match(Set dst (VectorLoadShuffle src));
  effect(TEMP dst, TEMP vtmp);
  format %{ "vector_load_shuffle $dst, $src\t! using $vtmp as TEMP" %}
  ins_encode %{
    // Create a byte shuffle mask from short shuffle mask
    // only byte shuffle instruction available on these platforms
    int vlen_in_bytes = Matcher::vector_length_in_bytes(this);
    if (UseAVX == 0) {
      assert(vlen_in_bytes <= 16, "required");
      // Multiply each shuffle by two to get byte index
      __ movdqu($vtmp$$XMMRegister, $src$$XMMRegister);
      __ psllw($vtmp$$XMMRegister, 1);

      // Duplicate to create 2 copies of byte index
      __ movdqu($dst$$XMMRegister, $vtmp$$XMMRegister);
      __ psllw($dst$$XMMRegister, 8);
      __ por($dst$$XMMRegister, $vtmp$$XMMRegister);

      // Add one to get alternate byte index
      __ movdqu($vtmp$$XMMRegister, ExternalAddress(vector_short_shufflemask()), noreg);
      __ paddb($dst$$XMMRegister, $vtmp$$XMMRegister);
    } else {
      assert(UseAVX > 1 || vlen_in_bytes <= 16, "required");
      int vlen_enc = vector_length_encoding(this);
      // Multiply each shuffle by two to get byte index
      __ vpsllw($vtmp$$XMMRegister, $src$$XMMRegister, 1, vlen_enc);

      // Duplicate to create 2 copies of byte index
      __ vpsllw($dst$$XMMRegister, $vtmp$$XMMRegister,  8, vlen_enc);
      __ vpor($dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);

      // Add one to get alternate byte index
      __ vpaddb($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_shufflemask()), vlen_enc, noreg);
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct rearrangeS(vec dst, vec shuffle) %{
  predicate(Matcher::vector_element_basic_type(n) == T_SHORT &&
            Matcher::vector_length(n) <= 8 && !VM_Version::supports_avx512bw());
  match(Set dst (VectorRearrange dst shuffle));
  format %{ "vector_rearrange $dst, $shuffle, $dst" %}
  ins_encode %{
    assert(UseSSE >= 4, "required");
    __ pshufb($dst$$XMMRegister, $shuffle$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct rearrangeS_avx(legVec dst, legVec src, vec shuffle, legVec vtmp1, legVec vtmp2) %{
  predicate(Matcher::vector_element_basic_type(n) == T_SHORT &&
            Matcher::vector_length(n) == 16 && !VM_Version::supports_avx512bw());
  match(Set dst (VectorRearrange src shuffle));
  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
  format %{ "vector_rearrange $dst, $shuffle, $src\t! using $vtmp1, $vtmp2 as TEMP" %}
  ins_encode %{
    assert(UseAVX >= 2, "required");
    // Swap src into vtmp1
    __ vperm2i128($vtmp1$$XMMRegister, $src$$XMMRegister, $src$$XMMRegister, 1);
    // Shuffle swapped src to get entries from other 128 bit lane
    __ vpshufb($vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $shuffle$$XMMRegister, Assembler::AVX_256bit);
    // Shuffle original src to get entries from self 128 bit lane
    __ vpshufb($dst$$XMMRegister, $src$$XMMRegister, $shuffle$$XMMRegister, Assembler::AVX_256bit);
    // Create a blend mask by setting high bits for entries coming from other lane in shuffle
    __ vpaddb($vtmp2$$XMMRegister, $shuffle$$XMMRegister, ExternalAddress(vector_byte_shufflemask()), Assembler::AVX_256bit, noreg);
    // Perform the blend
    __ vpblendvb($dst$$XMMRegister, $dst$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, Assembler::AVX_256bit);
  %}
  ins_pipe( pipe_slow );
%}

instruct rearrangeS_evex(vec dst, vec src, vec shuffle) %{
  predicate(Matcher::vector_element_basic_type(n) == T_SHORT &&
            VM_Version::supports_avx512bw());
  match(Set dst (VectorRearrange src shuffle));
  format %{ "vector_rearrange $dst, $shuffle, $src" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    if (!VM_Version::supports_avx512vl()) {
      vlen_enc = Assembler::AVX_512bit;
    }
    __ vpermw($dst$$XMMRegister, $shuffle$$XMMRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// LoadShuffle/Rearrange for Integer and Float

instruct loadShuffleI(vec dst, vec src, vec vtmp) %{
  predicate((Matcher::vector_element_basic_type(n) == T_INT || Matcher::vector_element_basic_type(n) == T_FLOAT) &&
            Matcher::vector_length(n) == 4 && UseAVX == 0);
  match(Set dst (VectorLoadShuffle src));
  effect(TEMP dst, TEMP vtmp);
  format %{ "vector_load_shuffle $dst, $src\t! using $vtmp as TEMP" %}
  ins_encode %{
    assert(UseSSE >= 4, "required");

    // Create a byte shuffle mask from int shuffle mask
    // only byte shuffle instruction available on these platforms

    // Duplicate and multiply each shuffle by 4
    __ movdqu($vtmp$$XMMRegister, $src$$XMMRegister);
    __ pshuflw($vtmp$$XMMRegister, $vtmp$$XMMRegister, 0xA0);
    __ pshufhw($vtmp$$XMMRegister, $vtmp$$XMMRegister, 0xA0);
    __ psllw($vtmp$$XMMRegister, 2);

    // Duplicate again to create 4 copies of byte index
    __ movdqu($dst$$XMMRegister, $vtmp$$XMMRegister);
    __ psllw($dst$$XMMRegister, 8);
    __ por($vtmp$$XMMRegister, $dst$$XMMRegister);

    // Add 3,2,1,0 to get alternate byte index
    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_int_shufflemask()), noreg);
    __ paddb($dst$$XMMRegister, $vtmp$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct rearrangeI(vec dst, vec shuffle) %{
  predicate((Matcher::vector_element_basic_type(n) == T_INT || Matcher::vector_element_basic_type(n) == T_FLOAT) &&
            UseAVX == 0);
  match(Set dst (VectorRearrange dst shuffle));
  format %{ "vector_rearrange $dst, $shuffle, $dst" %}
  ins_encode %{
    assert(UseSSE >= 4, "required");
    __ pshufb($dst$$XMMRegister, $shuffle$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct rearrangeI_avx(vec dst, vec src, vec shuffle) %{
  predicate((Matcher::vector_element_basic_type(n) == T_INT || Matcher::vector_element_basic_type(n) == T_FLOAT) &&
            UseAVX > 0);
  match(Set dst (VectorRearrange src shuffle));
  format %{ "vector_rearrange $dst, $shuffle, $src" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    __ vector_rearrange_int_float(bt, $dst$$XMMRegister, $shuffle$$XMMRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// LoadShuffle/Rearrange for Long and Double

instruct loadShuffleL(vec dst, vec src, vec vtmp) %{
  predicate(is_double_word_type(Matcher::vector_element_basic_type(n)) && // T_LONG, T_DOUBLE
            Matcher::vector_length(n) < 8 && !VM_Version::supports_avx512vl());
  match(Set dst (VectorLoadShuffle src));
  effect(TEMP dst, TEMP vtmp);
  format %{ "vector_load_shuffle $dst, $src\t! using $vtmp as TEMP" %}
  ins_encode %{
    assert(UseAVX >= 2, "required");

    int vlen_enc = vector_length_encoding(this);
    // Create a double word shuffle mask from long shuffle mask
    // only double word shuffle instruction available on these platforms

    // Multiply each shuffle by two to get double word index
    __ vpsllq($vtmp$$XMMRegister, $src$$XMMRegister, 1, vlen_enc);

    // Duplicate each double word shuffle
    __ vpsllq($dst$$XMMRegister, $vtmp$$XMMRegister, 32, vlen_enc);
    __ vpor($dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);

    // Add one to get alternate double word index
    __ vpaddd($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_long_shufflemask()), vlen_enc, noreg);
  %}
  ins_pipe( pipe_slow );
%}

instruct rearrangeL(vec dst, vec src, vec shuffle) %{
  predicate(is_double_word_type(Matcher::vector_element_basic_type(n)) && // T_LONG, T_DOUBLE
            Matcher::vector_length(n) < 8 && !VM_Version::supports_avx512vl());
  match(Set dst (VectorRearrange src shuffle));
  format %{ "vector_rearrange $dst, $shuffle, $src" %}
  ins_encode %{
    assert(UseAVX >= 2, "required");

    int vlen_enc = vector_length_encoding(this);
    __ vpermd($dst$$XMMRegister, $shuffle$$XMMRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct rearrangeL_evex(vec dst, vec src, vec shuffle) %{
  predicate(is_double_word_type(Matcher::vector_element_basic_type(n)) && // T_LONG, T_DOUBLE
            (Matcher::vector_length(n) == 8 || VM_Version::supports_avx512vl()));
  match(Set dst (VectorRearrange src shuffle));
  format %{ "vector_rearrange $dst, $shuffle, $src" %}
  ins_encode %{
    assert(UseAVX > 2, "required");

    int vlen_enc = vector_length_encoding(this);
    if (vlen_enc == Assembler::AVX_128bit) {
      vlen_enc = Assembler::AVX_256bit;
    }
    __ vpermq($dst$$XMMRegister, $shuffle$$XMMRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- FMA --------------------------------------
// a * b + c

instruct vfmaF_reg(vec a, vec b, vec c) %{
  match(Set c (FmaVF  c (Binary a b)));
  format %{ "fmaps $a,$b,$c\t# $c = $a * $b + $c fma packedF" %}
  ins_cost(150);
  ins_encode %{
    assert(UseFMA, "not enabled");
    int vlen_enc = vector_length_encoding(this);
    __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vfmaF_mem(vec a, memory b, vec c) %{
  predicate(Matcher::vector_length_in_bytes(n->in(1)) > 8);
  match(Set c (FmaVF  c (Binary a (LoadVector b))));
  format %{ "fmaps $a,$b,$c\t# $c = $a * $b + $c fma packedF" %}
  ins_cost(150);
  ins_encode %{
    assert(UseFMA, "not enabled");
    int vlen_enc = vector_length_encoding(this);
    __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vfmaD_reg(vec a, vec b, vec c) %{
  match(Set c (FmaVD  c (Binary a b)));
  format %{ "fmapd $a,$b,$c\t# $c = $a * $b + $c fma packedD" %}
  ins_cost(150);
  ins_encode %{
    assert(UseFMA, "not enabled");
    int vlen_enc = vector_length_encoding(this);
    __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vfmaD_mem(vec a, memory b, vec c) %{
  predicate(Matcher::vector_length_in_bytes(n->in(1)) > 8);
  match(Set c (FmaVD  c (Binary a (LoadVector b))));
  format %{ "fmapd $a,$b,$c\t# $c = $a * $b + $c fma packedD" %}
  ins_cost(150);
  ins_encode %{
    assert(UseFMA, "not enabled");
    int vlen_enc = vector_length_encoding(this);
    __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- Vector Multiply Add --------------------------------------

instruct vmuladdS2I_reg_sse(vec dst, vec src1) %{
  predicate(UseAVX == 0);
  match(Set dst (MulAddVS2VI dst src1));
  format %{ "pmaddwd $dst,$src1\t! muladd packedStoI" %}
  ins_encode %{
    __ pmaddwd($dst$$XMMRegister, $src1$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmuladdS2I_reg_avx(vec dst, vec src1, vec src2) %{
  predicate(UseAVX > 0);
  match(Set dst (MulAddVS2VI src1 src2));
  format %{ "vpmaddwd $dst,$src1,$src2\t! muladd packedStoI" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- Vector Multiply Add Add ----------------------------------

instruct vmuladdaddS2I_reg(vec dst, vec src1, vec src2) %{
  predicate(VM_Version::supports_avx512_vnni());
  match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
  format %{ "evpdpwssd $dst,$src1,$src2\t! muladdadd packedStoI" %}
  ins_encode %{
    assert(UseAVX > 2, "required");
    int vlen_enc = vector_length_encoding(this);
    __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
  ins_cost(10);
%}

// --------------------------------- PopCount --------------------------------------

instruct vpopcount_integral_reg_evex(vec dst, vec src) %{
  predicate(is_vector_popcount_predicate(Matcher::vector_element_basic_type(n->in(1))));
  match(Set dst (PopCountVI src));
  match(Set dst (PopCountVL src));
  format %{ "vector_popcount_integral $dst, $src" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType bt = Matcher::vector_element_basic_type(this, $src);
    __ vector_popcount_integral_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, k0, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vpopcount_integral_reg_evex_masked(vec dst, vec src, kReg mask) %{
  predicate(is_vector_popcount_predicate(Matcher::vector_element_basic_type(n->in(1))));
  match(Set dst (PopCountVI src mask));
  match(Set dst (PopCountVL src mask));
  format %{ "vector_popcount_integral_masked $dst, $src, $mask" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType bt = Matcher::vector_element_basic_type(this, $src);
    __ evmovdquq($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
    __ vector_popcount_integral_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, $mask$$KRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vpopcount_avx_reg(vec dst, vec src, vec xtmp1, vec xtmp2, rRegP rtmp) %{
  predicate(!is_vector_popcount_predicate(Matcher::vector_element_basic_type(n->in(1))));
  match(Set dst (PopCountVI src));
  match(Set dst (PopCountVL src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp);
  format %{ "vector_popcount_integral $dst, $src\t! using $xtmp1, $xtmp2, and $rtmp as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType bt = Matcher::vector_element_basic_type(this, $src);
    __ vector_popcount_integral(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
                                $xtmp2$$XMMRegister, $rtmp$$Register, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- Vector Trailing Zeros Count --------------------------------------

instruct vcount_trailing_zeros_reg_evex(vec dst, vec src, vec xtmp, rRegP rtmp) %{
  predicate(is_clz_non_subword_predicate_evex(Matcher::vector_element_basic_type(n->in(1)),
                                              Matcher::vector_length_in_bytes(n->in(1))));
  match(Set dst (CountTrailingZerosV src));
  effect(TEMP dst, TEMP xtmp, TEMP rtmp);
  ins_cost(400);
  format %{ "vector_count_trailing_zeros $dst, $src!\t using $xtmp and $rtmp as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType bt = Matcher::vector_element_basic_type(this, $src);
    __ vector_count_trailing_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, xnoreg,
                                        xnoreg, xnoreg, $xtmp$$XMMRegister, k0, $rtmp$$Register, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vcount_trailing_zeros_short_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, rRegP rtmp) %{
  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_SHORT &&
            VM_Version::supports_avx512cd() &&
            (VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64));
  match(Set dst (CountTrailingZerosV src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp);
  ins_cost(400);
  format %{ "vector_count_trailing_zeros $dst, $src!\t using $xtmp1, $xtmp2, $xtmp3 and $rtmp as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType bt = Matcher::vector_element_basic_type(this, $src);
    __ vector_count_trailing_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
                                        $xtmp2$$XMMRegister, xnoreg, $xtmp3$$XMMRegister, k0, $rtmp$$Register, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vcount_trailing_zeros_byte_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, kReg ktmp, rRegP rtmp) %{
  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_BYTE && VM_Version::supports_avx512vlbw());
  match(Set dst (CountTrailingZerosV src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, TEMP ktmp, TEMP rtmp);
  ins_cost(400);
  format %{ "vector_count_trailing_zeros $dst, $src!\t using $xtmp1, $xtmp2, $xtmp3, $xtmp4, $ktmp and $rtmp as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType bt = Matcher::vector_element_basic_type(this, $src);
    __ vector_count_trailing_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
                                        $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $xtmp4$$XMMRegister,
                                        $ktmp$$KRegister, $rtmp$$Register, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vcount_trailing_zeros_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, rRegP rtmp) %{
  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n->in(1)) < 64);
  match(Set dst (CountTrailingZerosV src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp);
  format %{ "vector_count_trailing_zeros $dst, $src\t! using $xtmp1, $xtmp2, $xtmp3, and $rtmp as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType bt = Matcher::vector_element_basic_type(this, $src);
    __ vector_count_trailing_zeros_avx(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
                                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}


// --------------------------------- Bitwise Ternary Logic ----------------------------------

instruct vpternlog(vec dst, vec src2, vec src3, immU8 func) %{
  match(Set dst (MacroLogicV (Binary dst src2) (Binary src3 func)));
  effect(TEMP dst);
  format %{ "vpternlogd $dst,$src2,$src3,$func\t! vector ternary logic" %}
  ins_encode %{
    int vector_len = vector_length_encoding(this);
    __ vpternlogd($dst$$XMMRegister, $func$$constant, $src2$$XMMRegister, $src3$$XMMRegister, vector_len);
  %}
  ins_pipe( pipe_slow );
%}

instruct vpternlog_mem(vec dst, vec src2, memory src3, immU8 func) %{
  predicate(Matcher::vector_length_in_bytes(n->in(1)->in(1)) > 8);
  match(Set dst (MacroLogicV (Binary dst src2) (Binary (LoadVector src3) func)));
  effect(TEMP dst);
  format %{ "vpternlogd $dst,$src2,$src3,$func\t! vector ternary logic" %}
  ins_encode %{
    int vector_len = vector_length_encoding(this);
    __ vpternlogd($dst$$XMMRegister, $func$$constant, $src2$$XMMRegister, $src3$$Address, vector_len);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- Rotation Operations ----------------------------------
instruct vprotate_immI8(vec dst, vec src, immI8 shift) %{
  match(Set dst (RotateLeftV src shift));
  match(Set dst (RotateRightV src shift));
  format %{ "vprotate_imm8 $dst,$src,$shift\t! vector rotate" %}
  ins_encode %{
    int opcode      = this->ideal_Opcode();
    int vector_len  = vector_length_encoding(this);
    BasicType etype = this->bottom_type()->is_vect()->element_basic_type();
    __ vprotate_imm(opcode, etype, $dst$$XMMRegister, $src$$XMMRegister, $shift$$constant, vector_len);
  %}
  ins_pipe( pipe_slow );
%}

instruct vprorate(vec dst, vec src, vec shift) %{
  match(Set dst (RotateLeftV src shift));
  match(Set dst (RotateRightV src shift));
  format %{ "vprotate $dst,$src,$shift\t! vector rotate" %}
  ins_encode %{
    int opcode      = this->ideal_Opcode();
    int vector_len  = vector_length_encoding(this);
    BasicType etype = this->bottom_type()->is_vect()->element_basic_type();
    __ vprotate_var(opcode, etype, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
  %}
  ins_pipe( pipe_slow );
%}

// ---------------------------------- Masked Operations ------------------------------------
instruct vmasked_load_avx_non_subword(vec dst, memory mem, vec mask) %{
  predicate(!n->in(3)->bottom_type()->isa_vectmask());
  match(Set dst (LoadVectorMasked mem mask));
  format %{ "vector_masked_load $dst, $mem, $mask \t! vector masked copy" %}
  ins_encode %{
    BasicType elmType = this->bottom_type()->is_vect()->element_basic_type();
    int vlen_enc = vector_length_encoding(this);
    __ vmovmask(elmType, $dst$$XMMRegister, $mem$$Address, $mask$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}


instruct vmasked_load_evex(vec dst, memory mem, kReg mask) %{
  predicate(n->in(3)->bottom_type()->isa_vectmask());
  match(Set dst (LoadVectorMasked mem mask));
  format %{ "vector_masked_load $dst, $mem, $mask \t! vector masked copy" %}
  ins_encode %{
    BasicType elmType =  this->bottom_type()->is_vect()->element_basic_type();
    int vector_len = vector_length_encoding(this);
    __ evmovdqu(elmType, $mask$$KRegister, $dst$$XMMRegister, $mem$$Address, false, vector_len);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmasked_store_avx_non_subword(memory mem, vec src, vec mask) %{
  predicate(!n->in(3)->in(2)->bottom_type()->isa_vectmask());
  match(Set mem (StoreVectorMasked mem (Binary src mask)));
  format %{ "vector_masked_store $mem, $src, $mask \t! vector masked store" %}
  ins_encode %{
    const MachNode* src_node = static_cast<const MachNode*>(this->in(this->operand_index($src)));
    int vlen_enc = vector_length_encoding(src_node);
    BasicType elmType =  src_node->bottom_type()->is_vect()->element_basic_type();
    __ vmovmask(elmType, $mem$$Address, $src$$XMMRegister, $mask$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmasked_store_evex(memory mem, vec src, kReg mask) %{
  predicate(n->in(3)->in(2)->bottom_type()->isa_vectmask());
  match(Set mem (StoreVectorMasked mem (Binary src mask)));
  format %{ "vector_masked_store $mem, $src, $mask \t! vector masked store" %}
  ins_encode %{
    const MachNode* src_node = static_cast<const MachNode*>(this->in(this->operand_index($src)));
    BasicType elmType =  src_node->bottom_type()->is_vect()->element_basic_type();
    int vlen_enc = vector_length_encoding(src_node);
    __ evmovdqu(elmType, $mask$$KRegister, $mem$$Address, $src$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct verify_vector_alignment(rRegP addr, immL32 mask, rFlagsReg cr) %{
  match(Set addr (VerifyVectorAlignment addr mask));
  effect(KILL cr);
  format %{ "verify_vector_alignment $addr $mask \t! verify alignment" %}
  ins_encode %{
    Label Lskip;
    // check if masked bits of addr are zero
    __ testq($addr$$Register, $mask$$constant);
    __ jccb(Assembler::equal, Lskip);
    __ stop("verify_vector_alignment found a misaligned vector memory access");
    __ bind(Lskip);
  %}
  ins_pipe(pipe_slow);
%}

instruct vmask_cmp_node(rRegI dst, vec src1, vec src2, kReg mask, kReg ktmp1, kReg ktmp2, rFlagsReg cr) %{
  match(Set dst (VectorCmpMasked src1 (Binary src2 mask)));
  effect(TEMP_DEF dst, TEMP ktmp1, TEMP ktmp2, KILL cr);
  format %{ "vector_mask_cmp $src1, $src2, $mask \t! vector mask comparison" %}
  ins_encode %{
    assert(vector_length_encoding(this, $src1) == vector_length_encoding(this, $src2), "mismatch");
    assert(Matcher::vector_element_basic_type(this, $src1) == Matcher::vector_element_basic_type(this, $src2), "mismatch");

    Label DONE;
    int vlen_enc = vector_length_encoding(this, $src1);
    BasicType elem_bt = Matcher::vector_element_basic_type(this, $src1);

    __ knotql($ktmp2$$KRegister, $mask$$KRegister);
    __ mov64($dst$$Register, -1L);
    __ evpcmp(elem_bt, $ktmp1$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, Assembler::eq, vlen_enc);
    __ kortestql($ktmp2$$KRegister, $ktmp1$$KRegister);
    __ jccb(Assembler::carrySet, DONE);
    __ kmovql($dst$$Register, $ktmp1$$KRegister);
    __ notq($dst$$Register);
    __ tzcntq($dst$$Register, $dst$$Register);
    __ bind(DONE);
  %}
  ins_pipe( pipe_slow );
%}


instruct vmask_gen(kReg dst, rRegL len, rRegL temp, rFlagsReg cr) %{
  match(Set dst (VectorMaskGen len));
  effect(TEMP temp, KILL cr);
  format %{ "vector_mask_gen32 $dst, $len \t! vector mask generator" %}
  ins_encode %{
    __ genmask($dst$$KRegister, $len$$Register, $temp$$Register);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmask_gen_imm(kReg dst, immL len, rRegL temp) %{
  match(Set dst (VectorMaskGen len));
  format %{ "vector_mask_gen $len \t! vector mask generator" %}
  effect(TEMP temp);
  ins_encode %{
    __ mov64($temp$$Register, (0xFFFFFFFFFFFFFFFFUL >> (64 -$len$$constant)));
    __ kmovql($dst$$KRegister, $temp$$Register);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmask_tolong_evex(rRegL dst, kReg mask, rFlagsReg cr) %{
  predicate(n->in(1)->bottom_type()->isa_vectmask());
  match(Set dst (VectorMaskToLong mask));
  effect(TEMP dst, KILL cr);
  format %{ "vector_tolong_evex $dst, $mask \t! vector mask tolong" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);
    int mask_len = Matcher::vector_length(this, $mask);
    int mask_size = mask_len * type2aelembytes(mbt);
    int vlen_enc = vector_length_encoding(this, $mask);
    __ vector_mask_operation(opcode, $dst$$Register, $mask$$KRegister,
                             $dst$$Register, mask_len, mask_size, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmask_tolong_bool(rRegL dst, vec mask, vec xtmp, rFlagsReg cr) %{
  predicate(n->in(1)->bottom_type()->isa_vectmask() == nullptr);
  match(Set dst (VectorMaskToLong mask));
  format %{ "vector_tolong_bool $dst, $mask \t! using $xtmp as TEMP" %}
  effect(TEMP_DEF dst, TEMP xtmp, KILL cr);
  ins_encode %{
    int opcode = this->ideal_Opcode();
    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);
    int mask_len = Matcher::vector_length(this, $mask);
    int vlen_enc = vector_length_encoding(this, $mask);
    __ vector_mask_operation(opcode, $dst$$Register, $mask$$XMMRegister, $xtmp$$XMMRegister,
                             $dst$$Register, mask_len, mbt, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmask_tolong_avx(rRegL dst, vec mask, immI size, vec xtmp, rFlagsReg cr) %{
  predicate(n->in(1)->in(1)->bottom_type()->isa_vectmask() == nullptr);
  match(Set dst (VectorMaskToLong (VectorStoreMask mask size)));
  format %{ "vector_tolong_avx $dst, $mask \t! using $xtmp as TEMP" %}
  effect(TEMP_DEF dst, TEMP xtmp, KILL cr);
  ins_encode %{
    int opcode = this->ideal_Opcode();
    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);
    int mask_len = Matcher::vector_length(this, $mask);
    int vlen_enc = vector_length_encoding(this, $mask);
    __ vector_mask_operation(opcode, $dst$$Register, $mask$$XMMRegister, $xtmp$$XMMRegister,
                             $dst$$Register, mask_len, mbt, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmask_truecount_evex(rRegI dst, kReg mask, rRegL tmp, rFlagsReg cr) %{
  predicate(n->in(1)->bottom_type()->isa_vectmask());
  match(Set dst (VectorMaskTrueCount mask));
  effect(TEMP_DEF dst, TEMP tmp, KILL cr);
  format %{ "vector_truecount_evex $dst, $mask \t! using $tmp as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);
    int mask_len = Matcher::vector_length(this, $mask);
    int mask_size = mask_len * type2aelembytes(mbt);
    int vlen_enc = vector_length_encoding(this, $mask);
    __ vector_mask_operation(opcode, $dst$$Register, $mask$$KRegister,
                             $tmp$$Register, mask_len, mask_size, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmask_truecount_bool(rRegI dst, vec mask, rRegL tmp, vec xtmp, rFlagsReg cr) %{
  predicate(n->in(1)->bottom_type()->isa_vectmask() == nullptr);
  match(Set dst (VectorMaskTrueCount mask));
  effect(TEMP_DEF dst, TEMP tmp, TEMP xtmp, KILL cr);
  format %{ "vector_truecount_bool $dst, $mask \t! using $tmp, $xtmp as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);
    int mask_len = Matcher::vector_length(this, $mask);
    int vlen_enc = vector_length_encoding(this, $mask);
    __ vector_mask_operation(opcode, $dst$$Register, $mask$$XMMRegister, $xtmp$$XMMRegister,
                             $tmp$$Register, mask_len, mbt, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmask_truecount_avx(rRegI dst, vec mask, immI size, rRegL tmp, vec xtmp, rFlagsReg cr) %{
  predicate(n->in(1)->in(1)->bottom_type()->isa_vectmask() == nullptr);
  match(Set dst (VectorMaskTrueCount (VectorStoreMask mask size)));
  effect(TEMP_DEF dst, TEMP tmp, TEMP xtmp, KILL cr);
  format %{ "vector_truecount_avx $dst, $mask \t! using $tmp, $xtmp as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);
    int mask_len = Matcher::vector_length(this, $mask);
    int vlen_enc = vector_length_encoding(this, $mask);
    __ vector_mask_operation(opcode, $dst$$Register, $mask$$XMMRegister, $xtmp$$XMMRegister,
                             $tmp$$Register, mask_len, mbt, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmask_first_or_last_true_evex(rRegI dst, kReg mask, rRegL tmp, rFlagsReg cr) %{
  predicate(n->in(1)->bottom_type()->isa_vectmask());
  match(Set dst (VectorMaskFirstTrue mask));
  match(Set dst (VectorMaskLastTrue mask));
  effect(TEMP_DEF dst, TEMP tmp, KILL cr);
  format %{ "vector_mask_first_or_last_true_evex $dst, $mask \t! using $tmp as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);
    int mask_len = Matcher::vector_length(this, $mask);
    int mask_size = mask_len * type2aelembytes(mbt);
    int vlen_enc = vector_length_encoding(this, $mask);
    __ vector_mask_operation(opcode, $dst$$Register, $mask$$KRegister,
                             $tmp$$Register, mask_len, mask_size, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmask_first_or_last_true_bool(rRegI dst, vec mask, rRegL tmp, vec xtmp, rFlagsReg cr) %{
  predicate(n->in(1)->bottom_type()->isa_vectmask() == nullptr);
  match(Set dst (VectorMaskFirstTrue mask));
  match(Set dst (VectorMaskLastTrue mask));
  effect(TEMP_DEF dst, TEMP tmp, TEMP xtmp, KILL cr);
  format %{ "vector_mask_first_or_last_true_bool $dst, $mask \t! using $tmp, $xtmp as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);
    int mask_len = Matcher::vector_length(this, $mask);
    int vlen_enc = vector_length_encoding(this, $mask);
    __ vector_mask_operation(opcode, $dst$$Register, $mask$$XMMRegister, $xtmp$$XMMRegister,
                             $tmp$$Register, mask_len, mbt, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmask_first_or_last_true_avx(rRegI dst, vec mask, immI size, rRegL tmp, vec xtmp, rFlagsReg cr) %{
  predicate(n->in(1)->in(1)->bottom_type()->isa_vectmask() == nullptr);
  match(Set dst (VectorMaskFirstTrue (VectorStoreMask mask size)));
  match(Set dst (VectorMaskLastTrue (VectorStoreMask mask size)));
  effect(TEMP_DEF dst, TEMP tmp, TEMP xtmp, KILL cr);
  format %{ "vector_mask_first_or_last_true_avx $dst, $mask \t! using $tmp, $xtmp as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);
    int mask_len = Matcher::vector_length(this, $mask);
    int vlen_enc = vector_length_encoding(this, $mask);
    __ vector_mask_operation(opcode, $dst$$Register, $mask$$XMMRegister, $xtmp$$XMMRegister,
                             $tmp$$Register, mask_len, mbt, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// --------------------------------- Compress/Expand Operations ---------------------------
instruct vcompress_reg_avx(vec dst, vec src, vec mask, rRegI rtmp, rRegL rscratch, vec perm, vec xtmp, rFlagsReg cr) %{
  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 32);
  match(Set dst (CompressV src mask));
  match(Set dst (ExpandV src mask));
  effect(TEMP_DEF dst, TEMP perm, TEMP xtmp, TEMP rtmp, TEMP rscratch, KILL cr);
  format %{ "vector_compress $dst, $src, $mask \t!using $xtmp, $rtmp, $rscratch and $perm as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vlen_enc = vector_length_encoding(this);
    BasicType bt  = Matcher::vector_element_basic_type(this);
    __ vector_compress_expand_avx2(opcode, $dst$$XMMRegister, $src$$XMMRegister, $mask$$XMMRegister, $rtmp$$Register,
                                   $rscratch$$Register, $perm$$XMMRegister, $xtmp$$XMMRegister, bt, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vcompress_expand_reg_evex(vec dst, vec src, kReg mask) %{
  predicate(VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64);
  match(Set dst (CompressV src mask));
  match(Set dst (ExpandV src mask));
  format %{ "vector_compress_expand $dst, $src, $mask" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    int vector_len = vector_length_encoding(this);
    BasicType bt  = Matcher::vector_element_basic_type(this);
    __ vector_compress_expand(opcode, $dst$$XMMRegister, $src$$XMMRegister, $mask$$KRegister, false, bt, vector_len);
  %}
  ins_pipe( pipe_slow );
%}

instruct vcompress_mask_reg_evex(kReg dst, kReg mask, rRegL rtmp1, rRegL rtmp2, rFlagsReg cr) %{
  match(Set dst (CompressM mask));
  effect(TEMP rtmp1, TEMP rtmp2, KILL cr);
  format %{ "mask_compress_evex $dst, $mask\t! using $rtmp1 and $rtmp2 as TEMP" %}
  ins_encode %{
    assert(this->in(1)->bottom_type()->isa_vectmask(), "");
    int mask_len = Matcher::vector_length(this);
    __ vector_mask_compress($dst$$KRegister, $mask$$KRegister, $rtmp1$$Register, $rtmp2$$Register, mask_len);
  %}
  ins_pipe( pipe_slow );
%}

// -------------------------------- Bit and Byte Reversal Vector Operations ------------------------

instruct vreverse_reg(vec dst, vec src, vec xtmp1, vec xtmp2, rRegI rtmp) %{
  predicate(!VM_Version::supports_gfni());
  match(Set dst (ReverseV src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp);
  format %{ "vector_reverse_bit_evex $dst, $src!\t using $xtmp1, $xtmp2 and $rtmp as TEMP" %}
  ins_encode %{
    int vec_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    __ vector_reverse_bit(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
                          $xtmp2$$XMMRegister, $rtmp$$Register, vec_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vreverse_reg_gfni(vec dst, vec src, vec xtmp) %{
  predicate(VM_Version::supports_gfni());
  match(Set dst (ReverseV src));
  effect(TEMP dst, TEMP xtmp);
  format %{ "vector_reverse_bit_gfni $dst, $src!\t using $xtmp as TEMP" %}
  ins_encode %{
    int vec_enc = vector_length_encoding(this);
    BasicType bt  = Matcher::vector_element_basic_type(this);
    InternalAddress addr = $constantaddress(jlong(0x8040201008040201));
    __ vector_reverse_bit_gfni(bt, $dst$$XMMRegister, $src$$XMMRegister, addr, vec_enc,
                               $xtmp$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vreverse_byte_reg(vec dst, vec src) %{
  predicate(VM_Version::supports_avx512bw() || Matcher::vector_length_in_bytes(n) < 64);
  match(Set dst (ReverseBytesV src));
  effect(TEMP dst);
  format %{ "vector_reverse_byte $dst, $src" %}
  ins_encode %{
    int vec_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    __ vector_reverse_byte(bt, $dst$$XMMRegister, $src$$XMMRegister, vec_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vreverse_byte64_reg(vec dst, vec src, vec xtmp1, vec xtmp2, rRegI rtmp) %{
  predicate(!VM_Version::supports_avx512bw() && Matcher::vector_length_in_bytes(n) == 64);
  match(Set dst (ReverseBytesV src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp);
  format %{ "vector_reverse_byte $dst, $src!\t using $xtmp1, $xtmp2 and $rtmp as TEMP" %}
  ins_encode %{
    int vec_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    __ vector_reverse_byte64(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
                             $xtmp2$$XMMRegister, $rtmp$$Register, vec_enc);
  %}
  ins_pipe( pipe_slow );
%}

// ---------------------------------- Vector Count Leading Zeros -----------------------------------

instruct vcount_leading_zeros_IL_reg_evex(vec dst, vec src) %{
  predicate(is_clz_non_subword_predicate_evex(Matcher::vector_element_basic_type(n->in(1)),
                                              Matcher::vector_length_in_bytes(n->in(1))));
  match(Set dst (CountLeadingZerosV src));
  format %{ "vector_count_leading_zeros $dst, $src" %}
  ins_encode %{
     int vlen_enc = vector_length_encoding(this, $src);
     BasicType bt = Matcher::vector_element_basic_type(this, $src);
     __ vector_count_leading_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, xnoreg,
                                        xnoreg, xnoreg, k0, noreg, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vcount_leading_zeros_IL_reg_evex_masked(vec dst, vec src, kReg mask) %{
  predicate(is_clz_non_subword_predicate_evex(Matcher::vector_element_basic_type(n->in(1)),
                                              Matcher::vector_length_in_bytes(n->in(1))));
  match(Set dst (CountLeadingZerosV src mask));
  format %{ "vector_count_leading_zeros $dst, $src, $mask" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType bt = Matcher::vector_element_basic_type(this, $src);
    __ evmovdquq($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
    __ vector_count_leading_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, xnoreg, xnoreg,
                                       xnoreg, $mask$$KRegister, noreg, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vcount_leading_zeros_short_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2) %{
  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_SHORT &&
            VM_Version::supports_avx512cd() &&
            (VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64));
  match(Set dst (CountLeadingZerosV src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2);
  format %{ "vector_count_leading_zeros $dst, $src!\t using $xtmp1 and $xtmp2 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType bt = Matcher::vector_element_basic_type(this, $src);
    __ vector_count_leading_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
                                       $xtmp2$$XMMRegister, xnoreg, k0, noreg, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vcount_leading_zeros_byte_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, kReg ktmp, rRegP rtmp) %{
  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_BYTE && VM_Version::supports_avx512vlbw());
  match(Set dst (CountLeadingZerosV src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP ktmp, TEMP rtmp);
  format %{ "vector_count_leading_zeros $dst, $src!\t using $xtmp1, $xtmp2, $xtmp3, $ktmp and $rtmp as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType bt = Matcher::vector_element_basic_type(this, $src);
    __ vector_count_leading_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
                                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $ktmp$$KRegister,
                                       $rtmp$$Register, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vcount_leading_zeros_int_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3) %{
  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_INT &&
            !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n->in(1)) < 64);
  match(Set dst (CountLeadingZerosV src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3);
  format %{ "vector_count_leading_zeros $dst, $src\t! using $xtmp1, $xtmp2 and $xtmp3 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType bt = Matcher::vector_element_basic_type(this, $src);
    __ vector_count_leading_zeros_avx(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
                                      $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, noreg, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vcount_leading_zeros_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, rRegP rtmp) %{
  predicate(Matcher::vector_element_basic_type(n->in(1)) != T_INT &&
            !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n->in(1)) < 64);
  match(Set dst (CountLeadingZerosV src));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp);
  format %{ "vector_count_leading_zeros $dst, $src\t! using $xtmp1, $xtmp2, $xtmp3, and $rtmp as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this, $src);
    BasicType bt = Matcher::vector_element_basic_type(this, $src);
    __ vector_count_leading_zeros_avx(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
                                      $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

// ---------------------------------- Vector Masked Operations ------------------------------------

instruct vadd_reg_masked(vec dst, vec src2, kReg mask) %{
  match(Set dst (AddVB (Binary dst src2) mask));
  match(Set dst (AddVS (Binary dst src2) mask));
  match(Set dst (AddVI (Binary dst src2) mask));
  match(Set dst (AddVL (Binary dst src2) mask));
  match(Set dst (AddVF (Binary dst src2) mask));
  match(Set dst (AddVD (Binary dst src2) mask));
  format %{ "vpadd_masked $dst, $dst, $src2, $mask\t! add masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vadd_mem_masked(vec dst, memory src2, kReg mask) %{
  match(Set dst (AddVB (Binary dst (LoadVector src2)) mask));
  match(Set dst (AddVS (Binary dst (LoadVector src2)) mask));
  match(Set dst (AddVI (Binary dst (LoadVector src2)) mask));
  match(Set dst (AddVL (Binary dst (LoadVector src2)) mask));
  match(Set dst (AddVF (Binary dst (LoadVector src2)) mask));
  match(Set dst (AddVD (Binary dst (LoadVector src2)) mask));
  format %{ "vpadd_masked $dst, $dst, $src2, $mask\t! add masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vxor_reg_masked(vec dst, vec src2, kReg mask) %{
  match(Set dst (XorV (Binary dst src2) mask));
  format %{ "vxor_masked $dst, $dst, $src2, $mask\t! xor masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vxor_mem_masked(vec dst, memory src2, kReg mask) %{
  match(Set dst (XorV (Binary dst (LoadVector src2)) mask));
  format %{ "vxor_masked $dst, $dst, $src2, $mask\t! xor masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vor_reg_masked(vec dst, vec src2, kReg mask) %{
  match(Set dst (OrV (Binary dst src2) mask));
  format %{ "vor_masked $dst, $dst, $src2, $mask\t! or masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vor_mem_masked(vec dst, memory src2, kReg mask) %{
  match(Set dst (OrV (Binary dst (LoadVector src2)) mask));
  format %{ "vor_masked $dst, $dst, $src2, $mask\t! or masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vand_reg_masked(vec dst, vec src2, kReg mask) %{
  match(Set dst (AndV (Binary dst src2) mask));
  format %{ "vand_masked $dst, $dst, $src2, $mask\t! and masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vand_mem_masked(vec dst, memory src2, kReg mask) %{
  match(Set dst (AndV (Binary dst (LoadVector src2)) mask));
  format %{ "vand_masked $dst, $dst, $src2, $mask\t! and masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vsub_reg_masked(vec dst, vec src2, kReg mask) %{
  match(Set dst (SubVB (Binary dst src2) mask));
  match(Set dst (SubVS (Binary dst src2) mask));
  match(Set dst (SubVI (Binary dst src2) mask));
  match(Set dst (SubVL (Binary dst src2) mask));
  match(Set dst (SubVF (Binary dst src2) mask));
  match(Set dst (SubVD (Binary dst src2) mask));
  format %{ "vpsub_masked $dst, $dst, $src2, $mask\t! sub masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vsub_mem_masked(vec dst, memory src2, kReg mask) %{
  match(Set dst (SubVB (Binary dst (LoadVector src2)) mask));
  match(Set dst (SubVS (Binary dst (LoadVector src2)) mask));
  match(Set dst (SubVI (Binary dst (LoadVector src2)) mask));
  match(Set dst (SubVL (Binary dst (LoadVector src2)) mask));
  match(Set dst (SubVF (Binary dst (LoadVector src2)) mask));
  match(Set dst (SubVD (Binary dst (LoadVector src2)) mask));
  format %{ "vpsub_masked $dst, $dst, $src2, $mask\t! sub masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmul_reg_masked(vec dst, vec src2, kReg mask) %{
  match(Set dst (MulVS (Binary dst src2) mask));
  match(Set dst (MulVI (Binary dst src2) mask));
  match(Set dst (MulVL (Binary dst src2) mask));
  match(Set dst (MulVF (Binary dst src2) mask));
  match(Set dst (MulVD (Binary dst src2) mask));
  format %{ "vpmul_masked $dst, $dst, $src2, $mask\t! mul masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmul_mem_masked(vec dst, memory src2, kReg mask) %{
  match(Set dst (MulVS (Binary dst (LoadVector src2)) mask));
  match(Set dst (MulVI (Binary dst (LoadVector src2)) mask));
  match(Set dst (MulVL (Binary dst (LoadVector src2)) mask));
  match(Set dst (MulVF (Binary dst (LoadVector src2)) mask));
  match(Set dst (MulVD (Binary dst (LoadVector src2)) mask));
  format %{ "vpmul_masked $dst, $dst, $src2, $mask\t! mul masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vsqrt_reg_masked(vec dst, kReg mask) %{
  match(Set dst (SqrtVF dst mask));
  match(Set dst (SqrtVD dst mask));
  format %{ "vpsqrt_masked $dst, $mask\t! sqrt masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $dst$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vdiv_reg_masked(vec dst, vec src2, kReg mask) %{
  match(Set dst (DivVF (Binary dst src2) mask));
  match(Set dst (DivVD (Binary dst src2) mask));
  format %{ "vpdiv_masked $dst, $dst, $src2, $mask\t! div masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vdiv_mem_masked(vec dst, memory src2, kReg mask) %{
  match(Set dst (DivVF (Binary dst (LoadVector src2)) mask));
  match(Set dst (DivVD (Binary dst (LoadVector src2)) mask));
  format %{ "vpdiv_masked $dst, $dst, $src2, $mask\t! div masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}


instruct vrol_imm_masked(vec dst, immI8 shift, kReg mask) %{
  match(Set dst (RotateLeftV (Binary dst shift) mask));
  match(Set dst (RotateRightV (Binary dst shift) mask));
  format %{ "vprotate_imm_masked $dst, $dst, $shift, $mask\t! rotate masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $shift$$constant, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vrol_reg_masked(vec dst, vec src2, kReg mask) %{
  match(Set dst (RotateLeftV (Binary dst src2) mask));
  match(Set dst (RotateRightV (Binary dst src2) mask));
  format %{ "vrotate_masked $dst, $dst, $src2, $mask\t! rotate masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vlshift_imm_masked(vec dst, immI8 shift, kReg mask) %{
  match(Set dst (LShiftVS (Binary dst (LShiftCntV shift)) mask));
  match(Set dst (LShiftVI (Binary dst (LShiftCntV shift)) mask));
  match(Set dst (LShiftVL (Binary dst (LShiftCntV shift)) mask));
  format %{ "vplshift_imm_masked $dst, $dst, $shift, $mask\t! lshift masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $shift$$constant, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vlshift_reg_masked(vec dst, vec src2, kReg mask) %{
  predicate(!n->as_ShiftV()->is_var_shift());
  match(Set dst (LShiftVS (Binary dst src2) mask));
  match(Set dst (LShiftVI (Binary dst src2) mask));
  match(Set dst (LShiftVL (Binary dst src2) mask));
  format %{ "vplshift_masked $dst, $dst, $src2, $mask\t! lshift masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, false);
  %}
  ins_pipe( pipe_slow );
%}

instruct vlshiftv_reg_masked(vec dst, vec src2, kReg mask) %{
  predicate(n->as_ShiftV()->is_var_shift());
  match(Set dst (LShiftVS (Binary dst src2) mask));
  match(Set dst (LShiftVI (Binary dst src2) mask));
  match(Set dst (LShiftVL (Binary dst src2) mask));
  format %{ "vplshiftv_masked $dst, $dst, $src2, $mask\t! lshift masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, true);
  %}
  ins_pipe( pipe_slow );
%}

instruct vrshift_imm_masked(vec dst, immI8 shift, kReg mask) %{
  match(Set dst (RShiftVS (Binary dst (RShiftCntV shift)) mask));
  match(Set dst (RShiftVI (Binary dst (RShiftCntV shift)) mask));
  match(Set dst (RShiftVL (Binary dst (RShiftCntV shift)) mask));
  format %{ "vprshift_imm_masked $dst, $dst, $shift, $mask\t! rshift masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $shift$$constant, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vrshift_reg_masked(vec dst, vec src2, kReg mask) %{
  predicate(!n->as_ShiftV()->is_var_shift());
  match(Set dst (RShiftVS (Binary dst src2) mask));
  match(Set dst (RShiftVI (Binary dst src2) mask));
  match(Set dst (RShiftVL (Binary dst src2) mask));
  format %{ "vprshift_masked $dst, $dst, $src2, $mask\t! rshift masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, false);
  %}
  ins_pipe( pipe_slow );
%}

instruct vrshiftv_reg_masked(vec dst, vec src2, kReg mask) %{
  predicate(n->as_ShiftV()->is_var_shift());
  match(Set dst (RShiftVS (Binary dst src2) mask));
  match(Set dst (RShiftVI (Binary dst src2) mask));
  match(Set dst (RShiftVL (Binary dst src2) mask));
  format %{ "vprshiftv_masked $dst, $dst, $src2, $mask\t! rshift masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, true);
  %}
  ins_pipe( pipe_slow );
%}

instruct vurshift_imm_masked(vec dst, immI8 shift, kReg mask) %{
  match(Set dst (URShiftVS (Binary dst (RShiftCntV shift)) mask));
  match(Set dst (URShiftVI (Binary dst (RShiftCntV shift)) mask));
  match(Set dst (URShiftVL (Binary dst (RShiftCntV shift)) mask));
  format %{ "vpurshift_imm_masked $dst, $dst, $shift, $mask\t! urshift masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $shift$$constant, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vurshift_reg_masked(vec dst, vec src2, kReg mask) %{
  predicate(!n->as_ShiftV()->is_var_shift());
  match(Set dst (URShiftVS (Binary dst src2) mask));
  match(Set dst (URShiftVI (Binary dst src2) mask));
  match(Set dst (URShiftVL (Binary dst src2) mask));
  format %{ "vpurshift_masked $dst, $dst, $src2, $mask\t! urshift masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, false);
  %}
  ins_pipe( pipe_slow );
%}

instruct vurshiftv_reg_masked(vec dst, vec src2, kReg mask) %{
  predicate(n->as_ShiftV()->is_var_shift());
  match(Set dst (URShiftVS (Binary dst src2) mask));
  match(Set dst (URShiftVI (Binary dst src2) mask));
  match(Set dst (URShiftVL (Binary dst src2) mask));
  format %{ "vpurshiftv_masked $dst, $dst, $src2, $mask\t! urshift masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, true);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmaxv_reg_masked(vec dst, vec src2, kReg mask) %{
  match(Set dst (MaxV (Binary dst src2) mask));
  format %{ "vpmax_masked $dst, $dst, $src2, $mask\t! max masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vmaxv_mem_masked(vec dst, memory src2, kReg mask) %{
  match(Set dst (MaxV (Binary dst (LoadVector src2)) mask));
  format %{ "vpmax_masked $dst, $dst, $src2, $mask\t! max masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vminv_reg_masked(vec dst, vec src2, kReg mask) %{
  match(Set dst (MinV (Binary dst src2) mask));
  format %{ "vpmin_masked $dst, $dst, $src2, $mask\t! min masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vminv_mem_masked(vec dst, memory src2, kReg mask) %{
  match(Set dst (MinV (Binary dst (LoadVector src2)) mask));
  format %{ "vpmin_masked $dst, $dst, $src2, $mask\t! min masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vrearrangev_reg_masked(vec dst, vec src2, kReg mask) %{
  match(Set dst (VectorRearrange (Binary dst src2) mask));
  format %{ "vprearrange_masked $dst, $dst, $src2, $mask\t! rearrange masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $src2$$XMMRegister, false, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vabs_masked(vec dst, kReg mask) %{
  match(Set dst (AbsVB dst mask));
  match(Set dst (AbsVS dst mask));
  match(Set dst (AbsVI dst mask));
  match(Set dst (AbsVL dst mask));
  format %{ "vabs_masked $dst, $mask \t! vabs masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $dst$$XMMRegister, $dst$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vfma_reg_masked(vec dst, vec src2, vec src3, kReg mask) %{
  match(Set dst (FmaVF (Binary dst src2) (Binary src3 mask)));
  match(Set dst (FmaVD (Binary dst src2) (Binary src3 mask)));
  format %{ "vfma_masked $dst, $src2, $src3, $mask \t! vfma masked operation" %}
  ins_encode %{
    assert(UseFMA, "Needs FMA instructions support.");
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $src2$$XMMRegister, $src3$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vfma_mem_masked(vec dst, vec src2, memory src3, kReg mask) %{
  match(Set dst (FmaVF (Binary dst src2) (Binary (LoadVector src3) mask)));
  match(Set dst (FmaVD (Binary dst src2) (Binary (LoadVector src3) mask)));
  format %{ "vfma_masked $dst, $src2, $src3, $mask \t! vfma masked operation" %}
  ins_encode %{
    assert(UseFMA, "Needs FMA instructions support.");
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    int opc = this->ideal_Opcode();
    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,
                   $src2$$XMMRegister, $src3$$Address, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct evcmp_masked(kReg dst, vec src1, vec src2, immI8 cond, kReg mask) %{
  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond mask)));
  format %{ "vcmp_masked $dst, $src1, $src2, $cond, $mask" %}
  ins_encode %{
    assert(bottom_type()->isa_vectmask(), "TypeVectMask expected");
    int vlen_enc = vector_length_encoding(this, $src1);
    BasicType src1_elem_bt = Matcher::vector_element_basic_type(this, $src1);

    // Comparison i
    switch (src1_elem_bt) {
      case T_BYTE: {
        bool is_unsigned = Matcher::is_unsigned_booltest_pred($cond$$constant);
        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);
        __ evpcmpb($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);
        break;
      }
      case T_SHORT: {
        bool is_unsigned = Matcher::is_unsigned_booltest_pred($cond$$constant);
        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);
        __ evpcmpw($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);
        break;
      }
      case T_INT: {
        bool is_unsigned = Matcher::is_unsigned_booltest_pred($cond$$constant);
        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);
        __ evpcmpd($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);
        break;
      }
      case T_LONG: {
        bool is_unsigned = Matcher::is_unsigned_booltest_pred($cond$$constant);
        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);
        __ evpcmpq($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);
        break;
      }
      case T_FLOAT: {
        Assembler::ComparisonPredicateFP cmp = booltest_pred_to_comparison_pred_fp($cond$$constant);
        __ evcmpps($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);
        break;
      }
      case T_DOUBLE: {
        Assembler::ComparisonPredicateFP cmp = booltest_pred_to_comparison_pred_fp($cond$$constant);
        __ evcmppd($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);
        break;
      }
      default: assert(false, "%s", type2name(src1_elem_bt)); break;
    }
  %}
  ins_pipe( pipe_slow );
%}

instruct mask_all_evexI_LE32(kReg dst, rRegI src) %{
  predicate(Matcher::vector_length(n) <= 32);
  match(Set dst (MaskAll src));
  format %{ "mask_all_evexI_LE32 $dst, $src \t" %}
  ins_encode %{
    int mask_len = Matcher::vector_length(this);
    __ vector_maskall_operation($dst$$KRegister, $src$$Register, mask_len);
  %}
  ins_pipe( pipe_slow );
%}

instruct mask_not_immLT8(kReg dst, kReg src, rRegI rtmp, kReg ktmp, immI_M1 cnt) %{
  predicate(Matcher::vector_length(n) < 8 && VM_Version::supports_avx512dq());
  match(Set dst (XorVMask src (MaskAll cnt)));
  effect(TEMP_DEF dst, TEMP rtmp, TEMP ktmp);
  format %{ "mask_not_LT8 $dst, $src, $cnt \t!using $ktmp and $rtmp as TEMP" %}
  ins_encode %{
    uint masklen = Matcher::vector_length(this);
    __ knot(masklen, $dst$$KRegister, $src$$KRegister, $ktmp$$KRegister, $rtmp$$Register);
  %}
  ins_pipe( pipe_slow );
%}

instruct mask_not_imm(kReg dst, kReg src, immI_M1 cnt) %{
  predicate((Matcher::vector_length(n) == 8 && VM_Version::supports_avx512dq()) ||
            (Matcher::vector_length(n) == 16) ||
            (Matcher::vector_length(n) > 16 && VM_Version::supports_avx512bw()));
  match(Set dst (XorVMask src (MaskAll cnt)));
  format %{ "mask_not $dst, $src, $cnt \t! mask not operation" %}
  ins_encode %{
    uint masklen = Matcher::vector_length(this);
    __ knot(masklen, $dst$$KRegister, $src$$KRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct long_to_maskLE8_avx(vec dst, rRegL src, rRegL rtmp1, rRegL rtmp2, vec xtmp) %{
  predicate(n->bottom_type()->isa_vectmask() == nullptr && Matcher::vector_length(n) <= 8);
  match(Set dst (VectorLongToMask src));
  effect(TEMP dst, TEMP rtmp1, TEMP rtmp2, TEMP xtmp);
  format %{ "long_to_mask_avx $dst, $src\t! using $rtmp1, $rtmp2, $xtmp as TEMP" %}
  ins_encode %{
    int mask_len = Matcher::vector_length(this);
    int vec_enc  = vector_length_encoding(mask_len);
    __ vector_long_to_maskvec($dst$$XMMRegister, $src$$Register, $rtmp1$$Register,
                              $rtmp2$$Register, xnoreg, mask_len, vec_enc);
  %}
  ins_pipe( pipe_slow );
%}


instruct long_to_maskGT8_avx(vec dst, rRegL src, rRegL rtmp1, rRegL rtmp2, vec xtmp1, rFlagsReg cr) %{
  predicate(n->bottom_type()->isa_vectmask() == nullptr && Matcher::vector_length(n) > 8);
  match(Set dst (VectorLongToMask src));
  effect(TEMP dst, TEMP rtmp1, TEMP rtmp2, TEMP xtmp1, KILL cr);
  format %{ "long_to_mask_avx $dst, $src\t! using $rtmp1, $rtmp2, $xtmp1, as TEMP" %}
  ins_encode %{
    int mask_len = Matcher::vector_length(this);
    assert(mask_len <= 32, "invalid mask length");
    int vec_enc  = vector_length_encoding(mask_len);
    __ vector_long_to_maskvec($dst$$XMMRegister, $src$$Register, $rtmp1$$Register,
                              $rtmp2$$Register, $xtmp1$$XMMRegister, mask_len, vec_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct long_to_mask_evex(kReg dst, rRegL src) %{
  predicate(n->bottom_type()->isa_vectmask());
  match(Set dst (VectorLongToMask src));
  format %{ "long_to_mask_evex $dst, $src\t!" %}
  ins_encode %{
    __ kmov($dst$$KRegister, $src$$Register);
  %}
  ins_pipe( pipe_slow );
%}

instruct mask_opers_evex(kReg dst, kReg src1, kReg src2, kReg kscratch) %{
  match(Set dst (AndVMask src1 src2));
  match(Set dst (OrVMask src1 src2));
  match(Set dst (XorVMask src1 src2));
  effect(TEMP kscratch);
  format %{ "mask_opers_evex $dst, $src1, $src2\t! using $kscratch as TEMP" %}
  ins_encode %{
    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));
    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));
    assert(Type::equals(mask1->bottom_type(), mask2->bottom_type()), "Mask types must be equal");
    uint masklen = Matcher::vector_length(this);
    masklen = (masklen < 16 && !VM_Version::supports_avx512dq()) ? 16 : masklen;
    __ masked_op(this->ideal_Opcode(), masklen, $dst$$KRegister, $src1$$KRegister, $src2$$KRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct vternlog_reg_masked(vec dst, vec src2, vec src3, immU8 func, kReg mask) %{
  match(Set dst (MacroLogicV dst (Binary src2 (Binary src3 (Binary func mask)))));
  format %{ "vternlog_masked $dst,$src2,$src3,$func,$mask\t! vternlog masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    __ evpternlog($dst$$XMMRegister, $func$$constant, $mask$$KRegister,
                  $src2$$XMMRegister, $src3$$XMMRegister, true, bt, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vternlogd_mem_masked(vec dst, vec src2, memory src3, immU8 func, kReg mask) %{
  match(Set dst (MacroLogicV dst (Binary src2 (Binary src3 (Binary func mask)))));
  format %{ "vternlog_masked $dst,$src2,$src3,$func,$mask\t! vternlog masked operation" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    __ evpternlog($dst$$XMMRegister, $func$$constant, $mask$$KRegister,
                  $src2$$XMMRegister, $src3$$Address, true, bt, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct castMM(kReg dst)
%{
  match(Set dst (CastVV dst));

  size(0);
  format %{ "# castVV of $dst" %}
  ins_encode(/* empty encoding */);
  ins_cost(0);
  ins_pipe(empty);
%}

instruct castVV(vec dst)
%{
  match(Set dst (CastVV dst));

  size(0);
  format %{ "# castVV of $dst" %}
  ins_encode(/* empty encoding */);
  ins_cost(0);
  ins_pipe(empty);
%}

instruct castVVLeg(legVec dst)
%{
  match(Set dst (CastVV dst));

  size(0);
  format %{ "# castVV of $dst" %}
  ins_encode(/* empty encoding */);
  ins_cost(0);
  ins_pipe(empty);
%}

instruct FloatClassCheck_reg_reg_vfpclass(rRegI dst, regF src, kReg ktmp, rFlagsReg cr)
%{
  match(Set dst (IsInfiniteF src));
  effect(TEMP ktmp, KILL cr);
  format %{ "float_class_check $dst, $src" %}
  ins_encode %{
    __ vfpclassss($ktmp$$KRegister, $src$$XMMRegister, 0x18);
    __ kmovbl($dst$$Register, $ktmp$$KRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct DoubleClassCheck_reg_reg_vfpclass(rRegI dst, regD src, kReg ktmp, rFlagsReg cr)
%{
  match(Set dst (IsInfiniteD src));
  effect(TEMP ktmp, KILL cr);
  format %{ "double_class_check $dst, $src" %}
  ins_encode %{
    __ vfpclasssd($ktmp$$KRegister, $src$$XMMRegister, 0x18);
    __ kmovbl($dst$$Register, $ktmp$$KRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct vector_addsub_saturating_subword_reg(vec dst, vec src1, vec src2)
%{
  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) &&
            n->is_SaturatingVector() && !n->as_SaturatingVector()->is_unsigned());
  match(Set dst (SaturatingAddV src1 src2));
  match(Set dst (SaturatingSubV src1 src2));
  format %{ "vector_addsub_saturating_subword $dst, $src1, $src2" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ vector_saturating_op(this->ideal_Opcode(), elem_bt, $dst$$XMMRegister,
                            $src1$$XMMRegister, $src2$$XMMRegister, false, vlen_enc);
  %}
  ins_pipe(pipe_slow);
%}

instruct vector_addsub_saturating_unsigned_subword_reg(vec dst, vec src1, vec src2)
%{
  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) &&
            n->is_SaturatingVector() && n->as_SaturatingVector()->is_unsigned());
  match(Set dst (SaturatingAddV src1 src2));
  match(Set dst (SaturatingSubV src1 src2));
  format %{ "vector_addsub_saturating_unsigned_subword $dst, $src1, $src2" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ vector_saturating_op(this->ideal_Opcode(), elem_bt, $dst$$XMMRegister,
                            $src1$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);
  %}
  ins_pipe(pipe_slow);
%}

instruct vector_addsub_saturating_reg_evex(vec dst, vec src1, vec src2, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2)
%{
  predicate(!is_subword_type(Matcher::vector_element_basic_type(n)) &&
            n->is_SaturatingVector() && !n->as_SaturatingVector()->is_unsigned() &&
            (Matcher::vector_length_in_bytes(n) == 64 || VM_Version::supports_avx512vl()));
  match(Set dst (SaturatingAddV src1 src2));
  match(Set dst (SaturatingSubV src1 src2));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2);
  format %{ "vector_addsub_saturating_evex $dst, $src1, $src2 \t! using $xtmp1, $xtmp2, $ktmp1 and $ktmp2 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ vector_addsub_dq_saturating_evex(this->ideal_Opcode(), elem_bt, $dst$$XMMRegister,
                                        $src1$$XMMRegister, $src2$$XMMRegister,
                                        $xtmp1$$XMMRegister, $xtmp2$$XMMRegister,
                                        $ktmp1$$KRegister, $ktmp2$$KRegister, vlen_enc);
  %}
  ins_pipe(pipe_slow);
%}

instruct vector_addsub_saturating_reg_avx(vec dst, vec src1, vec src2, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4)
%{
  predicate(!is_subword_type(Matcher::vector_element_basic_type(n)) &&
            n->is_SaturatingVector() && !n->as_SaturatingVector()->is_unsigned() &&
            Matcher::vector_length_in_bytes(n) <= 32 && !VM_Version::supports_avx512vl());
  match(Set dst (SaturatingAddV src1 src2));
  match(Set dst (SaturatingSubV src1 src2));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4);
  format %{ "vector_addsub_saturating_avx $dst, $src1, $src2 \t! using $xtmp1, $xtmp2, $xtmp3 and $xtmp4 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ vector_addsub_dq_saturating_avx(this->ideal_Opcode(), elem_bt, $dst$$XMMRegister, $src1$$XMMRegister,
                                       $src2$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister,
                                       $xtmp3$$XMMRegister, $xtmp4$$XMMRegister, vlen_enc);
  %}
  ins_pipe(pipe_slow);
%}

instruct vector_add_saturating_unsigned_reg_evex(vec dst, vec src1, vec src2, vec xtmp1, vec xtmp2, kReg ktmp)
%{
  predicate(!is_subword_type(Matcher::vector_element_basic_type(n)) &&
            n->is_SaturatingVector() && n->as_SaturatingVector()->is_unsigned() &&
            (Matcher::vector_length_in_bytes(n) == 64 || VM_Version::supports_avx512vl()));
  match(Set dst (SaturatingAddV src1 src2));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp);
  format %{ "vector_add_saturating_unsigned_evex $dst, $src1, $src2 \t! using $xtmp1, $xtmp2 and $ktmp as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ vector_add_dq_saturating_unsigned_evex(elem_bt, $dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister,
                                              $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $ktmp$$KRegister, vlen_enc);
  %}
  ins_pipe(pipe_slow);
%}

instruct vector_add_saturating_unsigned_reg_avx(vec dst, vec src1, vec src2, vec xtmp1, vec xtmp2, vec xtmp3)
%{
  predicate(!is_subword_type(Matcher::vector_element_basic_type(n)) &&
            n->is_SaturatingVector() && n->as_SaturatingVector()->is_unsigned() &&
            Matcher::vector_length_in_bytes(n) <= 32 && !VM_Version::supports_avx512vl());
  match(Set dst (SaturatingAddV src1 src2));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3);
  format %{ "vector_add_saturating_unsigned_avx $dst, $src1, $src2 \t! using $xtmp1, $xtmp2 and $xtmp3 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ vector_add_dq_saturating_unsigned_avx(elem_bt, $dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister,
                                             $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, vlen_enc);
  %}
  ins_pipe(pipe_slow);
%}

instruct vector_sub_saturating_unsigned_reg_evex(vec dst, vec src1, vec src2, kReg ktmp)
%{
  predicate(!is_subword_type(Matcher::vector_element_basic_type(n)) &&
            n->is_SaturatingVector() && n->as_SaturatingVector()->is_unsigned() &&
            (Matcher::vector_length_in_bytes(n) == 64 || VM_Version::supports_avx512vl()));
  match(Set dst (SaturatingSubV src1 src2));
  effect(TEMP ktmp);
  format %{ "vector_sub_saturating_unsigned_evex $dst, $src1, $src2 \t! using $ktmp as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ vector_sub_dq_saturating_unsigned_evex(elem_bt, $dst$$XMMRegister, $src1$$XMMRegister,
                                              $src2$$XMMRegister, $ktmp$$KRegister, vlen_enc);
  %}
  ins_pipe(pipe_slow);
%}

instruct vector_sub_saturating_unsigned_reg_avx(vec dst, vec src1, vec src2, vec xtmp1, vec xtmp2)
%{
  predicate(!is_subword_type(Matcher::vector_element_basic_type(n)) &&
            n->is_SaturatingVector() && n->as_SaturatingVector()->is_unsigned() &&
            Matcher::vector_length_in_bytes(n) <= 32 && !VM_Version::supports_avx512vl());
  match(Set dst (SaturatingSubV src1 src2));
  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2);
  format %{ "vector_sub_saturating_unsigned_avx $dst, $src1, $src2 \t! using $xtmp1 and $xtmp2 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ vector_sub_dq_saturating_unsigned_avx(elem_bt, $dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister,
                                             $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, vlen_enc);
  %}
  ins_pipe(pipe_slow);
%}

instruct vector_addsub_saturating_subword_mem(vec dst, vec src1, memory src2)
%{
  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) &&
            n->is_SaturatingVector() && !n->as_SaturatingVector()->is_unsigned());
  match(Set dst (SaturatingAddV src1 (LoadVector src2)));
  match(Set dst (SaturatingSubV src1 (LoadVector src2)));
  format %{ "vector_addsub_saturating_subword $dst, $src1, $src2" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ vector_saturating_op(this->ideal_Opcode(), elem_bt, $dst$$XMMRegister,
                            $src1$$XMMRegister, $src2$$Address, false, vlen_enc);
  %}
  ins_pipe(pipe_slow);
%}

instruct vector_addsub_saturating_unsigned_subword_mem(vec dst, vec src1, memory src2)
%{
  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) &&
            n->is_SaturatingVector() && n->as_SaturatingVector()->is_unsigned());
  match(Set dst (SaturatingAddV src1 (LoadVector src2)));
  match(Set dst (SaturatingSubV src1 (LoadVector src2)));
  format %{ "vector_addsub_saturating_unsigned_subword $dst, $src1, $src2" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ vector_saturating_op(this->ideal_Opcode(), elem_bt, $dst$$XMMRegister,
                            $src1$$XMMRegister, $src2$$Address, true, vlen_enc);
  %}
  ins_pipe(pipe_slow);
%}

instruct vector_addsub_saturating_subword_masked_reg(vec dst, vec src, kReg mask) %{
  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) &&
            n->is_SaturatingVector() && !n->as_SaturatingVector()->is_unsigned());
  match(Set dst (SaturatingAddV (Binary dst src) mask));
  match(Set dst (SaturatingSubV (Binary dst src) mask));
  format %{ "vector_addsub_saturating_subword_masked $dst, $mask, $src" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ evmasked_saturating_op(this->ideal_Opcode(), elem_bt, $mask$$KRegister, $dst$$XMMRegister,
                              $dst$$XMMRegister, $src$$XMMRegister, false, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vector_addsub_saturating_unsigned_subword_masked_reg(vec dst, vec src, kReg mask) %{
  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) &&
            n->is_SaturatingVector() && n->as_SaturatingVector()->is_unsigned());
  match(Set dst (SaturatingAddV (Binary dst src) mask));
  match(Set dst (SaturatingSubV (Binary dst src) mask));
  format %{ "vector_addsub_saturating_unsigned_subword_masked $dst, $mask, $src" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ evmasked_saturating_op(this->ideal_Opcode(), elem_bt, $mask$$KRegister, $dst$$XMMRegister,
                              $dst$$XMMRegister, $src$$XMMRegister, true, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vector_addsub_saturating_subword_masked_mem(vec dst, memory src, kReg mask) %{
  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) &&
            n->is_SaturatingVector() && !n->as_SaturatingVector()->is_unsigned());
  match(Set dst (SaturatingAddV (Binary dst (LoadVector src)) mask));
  match(Set dst (SaturatingSubV (Binary dst (LoadVector src)) mask));
  format %{ "vector_addsub_saturating_subword_masked $dst, $mask, $src" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ evmasked_saturating_op(this->ideal_Opcode(), elem_bt, $mask$$KRegister, $dst$$XMMRegister,
                              $dst$$XMMRegister, $src$$Address, false, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vector_addsub_saturating_unsigned_subword_masked_mem(vec dst, memory src, kReg mask) %{
  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) &&
            n->is_SaturatingVector() && n->as_SaturatingVector()->is_unsigned());
  match(Set dst (SaturatingAddV (Binary dst (LoadVector src)) mask));
  match(Set dst (SaturatingSubV (Binary dst (LoadVector src)) mask));
  format %{ "vector_addsub_saturating_unsigned_subword_masked $dst, $mask, $src" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType elem_bt = Matcher::vector_element_basic_type(this);
    __ evmasked_saturating_op(this->ideal_Opcode(), elem_bt, $mask$$KRegister, $dst$$XMMRegister,
                              $dst$$XMMRegister, $src$$Address, true, true, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vector_selectfrom_twovectors_reg_evex(vec index, vec src1, vec src2)
%{
  match(Set index (SelectFromTwoVector (Binary index src1) src2));
  format %{ "select_from_two_vector $index, $src1, $src2 \t!" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    BasicType bt = Matcher::vector_element_basic_type(this);
    __ select_from_two_vectors_evex(bt, $index$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe(pipe_slow);
%}

instruct reinterpretS2HF(regF dst, rRegI src)
%{
  match(Set dst (ReinterpretS2HF src));
  format %{ "vmovw $dst, $src" %}
  ins_encode %{
    __ vmovw($dst$$XMMRegister, $src$$Register);
  %}
  ins_pipe(pipe_slow);
%}

instruct reinterpretHF2S(rRegI dst, regF src)
%{
  match(Set dst (ReinterpretHF2S src));
  format %{ "vmovw $dst, $src" %}
  ins_encode %{
    __ vmovw($dst$$Register, $src$$XMMRegister);
    __ movswl($dst$$Register, $dst$$Register);
  %}
  ins_pipe(pipe_slow);
%}

instruct convF2HFAndS2HF(regF dst, regF src)
%{
  match(Set dst (ReinterpretS2HF (ConvF2HF src)));
  format %{ "convF2HFAndS2HF $dst, $src" %}
  ins_encode %{
    __ vcvtps2ph($dst$$XMMRegister, $src$$XMMRegister, 0x04, Assembler::AVX_128bit);
  %}
  ins_pipe(pipe_slow);
%}

instruct convHF2SAndHF2F(regF dst, regF src)
%{
  match(Set dst (ConvHF2F (ReinterpretHF2S src)));
  format %{ "convHF2SAndHF2F $dst, $src" %}
  ins_encode %{
    __ vcvtph2ps($dst$$XMMRegister, $src$$XMMRegister, Assembler::AVX_128bit);
  %}
  ins_pipe(pipe_slow);
%}

instruct scalar_sqrt_HF_reg(regF dst, regF src)
%{
  match(Set dst (SqrtHF src));
  format %{ "scalar_sqrt_fp16 $dst, $src" %}
  ins_encode %{
    __ vsqrtsh($dst$$XMMRegister, $src$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct scalar_binOps_HF_reg(regF dst, regF src1, regF src2)
%{
  match(Set dst (AddHF src1 src2));
  match(Set dst (DivHF src1 src2));
  match(Set dst (MulHF src1 src2));
  match(Set dst (SubHF src1 src2));
  format %{ "scalar_binop_fp16 $dst, $src1, $src2" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    __ efp16sh(opcode, $dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
  %}
  ins_pipe(pipe_slow);
%}

instruct scalar_minmax_HF_avx10_reg(regF dst, regF src1, regF src2)
%{
  predicate(VM_Version::supports_avx10_2());
  match(Set dst (MaxHF src1 src2));
  match(Set dst (MinHF src1 src2));
  format %{ "scalar_min_max_fp16 $dst, $src1, $src2" %}
  ins_encode %{
    int function = this->ideal_Opcode() == Op_MinHF ? AVX10_MINMAX_MIN_COMPARE_SIGN : AVX10_MINMAX_MAX_COMPARE_SIGN;
    __ eminmaxsh($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, function);
  %}
  ins_pipe( pipe_slow );
%}

instruct scalar_minmax_HF_reg(regF dst, regF src1, regF src2, kReg ktmp, regF xtmp1, regF xtmp2)
%{
  predicate(!VM_Version::supports_avx10_2());
  match(Set dst (MaxHF src1 src2));
  match(Set dst (MinHF src1 src2));
  effect(TEMP_DEF dst, TEMP ktmp, TEMP xtmp1, TEMP xtmp2);
  format %{ "scalar_min_max_fp16 $dst, $src1, $src2\t using $ktmp, $xtmp1 and $xtmp2 as TEMP" %}
  ins_encode %{
    int opcode = this->ideal_Opcode();
    __ scalar_max_min_fp16(opcode, $dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $ktmp$$KRegister,
                           $xtmp1$$XMMRegister, $xtmp2$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}

instruct scalar_fma_HF_reg(regF dst, regF src1, regF src2)
%{
  match(Set dst (FmaHF  src2 (Binary dst src1)));
  effect(DEF dst);
  format %{ "scalar_fma_fp16 $dst, $src1, $src2\t# $dst = $dst * $src1 + $src2 fma packedH" %}
  ins_encode %{
    __ vfmadd132sh($dst$$XMMRegister, $src2$$XMMRegister, $src1$$XMMRegister);
  %}
  ins_pipe( pipe_slow );
%}


instruct vector_sqrt_HF_reg(vec dst, vec src)
%{
  match(Set dst (SqrtVHF src));
  format %{ "vector_sqrt_fp16 $dst, $src" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ evsqrtph($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
  %}
  ins_pipe(pipe_slow);
%}

instruct vector_sqrt_HF_mem(vec dst, memory src)
%{
  match(Set dst (SqrtVHF (VectorReinterpret (LoadVector src))));
  format %{ "vector_sqrt_fp16_mem $dst, $src" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ evsqrtph($dst$$XMMRegister, $src$$Address, vlen_enc);
  %}
  ins_pipe(pipe_slow);
%}

instruct vector_binOps_HF_reg(vec dst, vec src1, vec src2)
%{
  match(Set dst (AddVHF src1 src2));
  match(Set dst (DivVHF src1 src2));
  match(Set dst (MulVHF src1 src2));
  match(Set dst (SubVHF src1 src2));
  format %{ "vector_binop_fp16 $dst, $src1, $src2" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    int opcode = this->ideal_Opcode();
    __ evfp16ph(opcode, $dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);
  %}
  ins_pipe(pipe_slow);
%}


instruct vector_binOps_HF_mem(vec dst, vec src1, memory src2)
%{
  match(Set dst (AddVHF src1 (VectorReinterpret (LoadVector src2))));
  match(Set dst (DivVHF src1 (VectorReinterpret (LoadVector src2))));
  match(Set dst (MulVHF src1 (VectorReinterpret (LoadVector src2))));
  match(Set dst (SubVHF src1 (VectorReinterpret (LoadVector src2))));
  format %{ "vector_binop_fp16_mem $dst, $src1, $src2" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    int opcode = this->ideal_Opcode();
    __ evfp16ph(opcode, $dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address, vlen_enc);
  %}
  ins_pipe(pipe_slow);
%}

instruct vector_fma_HF_reg(vec dst, vec src1, vec src2)
%{
  match(Set dst (FmaVHF src2 (Binary dst src1)));
  format %{ "vector_fma_fp16 $dst, $src1, $src2\t# $dst = $dst * $src1 + $src2 fma packedH" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ evfmadd132ph($dst$$XMMRegister, $src2$$XMMRegister, $src1$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vector_fma_HF_mem(vec dst, memory src1, vec src2)
%{
  match(Set dst (FmaVHF src2 (Binary dst (VectorReinterpret (LoadVector src1)))));
  format %{ "vector_fma_fp16_mem $dst, $src1, $src2\t# $dst = $dst * $src1 + $src2 fma packedH" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    __ evfmadd132ph($dst$$XMMRegister, $src2$$XMMRegister, $src1$$Address, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vector_minmax_HF_avx10_mem(vec dst, vec src1, memory src2)
%{
  predicate(VM_Version::supports_avx10_2());
  match(Set dst (MinVHF src1 (VectorReinterpret (LoadVector src2))));
  match(Set dst (MaxVHF src1 (VectorReinterpret (LoadVector src2))));
  format %{ "vector_min_max_fp16_mem $dst, $src1, $src2" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    int function =  this->ideal_Opcode() == Op_MinVHF ? AVX10_MINMAX_MIN_COMPARE_SIGN : AVX10_MINMAX_MAX_COMPARE_SIGN;
    __ evminmaxph($dst$$XMMRegister, k0, $src1$$XMMRegister, $src2$$Address, true, function, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vector_minmax_HF_avx10_reg(vec dst, vec src1, vec src2)
%{
  predicate(VM_Version::supports_avx10_2());
  match(Set dst (MinVHF src1 src2));
  match(Set dst (MaxVHF src1 src2));
  format %{ "vector_min_max_fp16 $dst, $src1, $src2" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    int function =  this->ideal_Opcode() == Op_MinVHF ? AVX10_MINMAX_MIN_COMPARE_SIGN : AVX10_MINMAX_MAX_COMPARE_SIGN;
    __ evminmaxph($dst$$XMMRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, true, function, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

instruct vector_minmax_HF_reg(vec dst, vec src1, vec src2, kReg ktmp, vec xtmp1, vec xtmp2)
%{
  predicate(!VM_Version::supports_avx10_2());
  match(Set dst (MinVHF src1 src2));
  match(Set dst (MaxVHF src1 src2));
  effect(TEMP_DEF dst, TEMP ktmp, TEMP xtmp1, TEMP xtmp2);
  format %{ "vector_min_max_fp16 $dst, $src1, $src2\t using $ktmp, $xtmp1 and $xtmp2 as TEMP" %}
  ins_encode %{
    int vlen_enc = vector_length_encoding(this);
    int opcode = this->ideal_Opcode();
    __ vector_max_min_fp16(opcode, $dst$$XMMRegister, $src2$$XMMRegister, $src1$$XMMRegister, $ktmp$$KRegister,
                           $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, vlen_enc);
  %}
  ins_pipe( pipe_slow );
%}

//----------PEEPHOLE RULES-----------------------------------------------------
// These must follow all instruction definitions as they use the names
// defined in the instructions definitions.
//
// peeppredicate ( rule_predicate );
// // the predicate unless which the peephole rule will be ignored
//
// peepmatch ( root_instr_name [preceding_instruction]* );
//
// peepprocedure ( procedure_name );
// // provide a procedure name to perform the optimization, the procedure should
// // reside in the architecture dependent peephole file, the method has the
// // signature of MachNode* (Block*, int, PhaseRegAlloc*, (MachNode*)(*)(), int...)
// // with the arguments being the basic block, the current node index inside the
// // block, the register allocator, the functions upon invoked return a new node
// // defined in peepreplace, and the rules of the nodes appearing in the
// // corresponding peepmatch, the function return true if successful, else
// // return false
//
// peepconstraint %{
// (instruction_number.operand_name relational_op instruction_number.operand_name
//  [, ...] );
// // instruction numbers are zero-based using left to right order in peepmatch
//
// peepreplace ( instr_name  ( [instruction_number.operand_name]* ) );
// // provide an instruction_number.operand_name for each operand that appears
// // in the replacement instruction's match rule
//
// ---------VM FLAGS---------------------------------------------------------
//
// All peephole optimizations can be turned off using -XX:-OptoPeephole
//
// Each peephole rule is given an identifying number starting with zero and
// increasing by one in the order seen by the parser.  An individual peephole
// can be enabled, and all others disabled, by using -XX:OptoPeepholeAt=#
// on the command-line.
//
// ---------CURRENT LIMITATIONS----------------------------------------------
//
// Only transformations inside a basic block (do we need more for peephole)
//
// ---------EXAMPLE----------------------------------------------------------
//
// // pertinent parts of existing instructions in architecture description
// instruct movI(rRegI dst, rRegI src)
// %{
//   match(Set dst (CopyI src));
// %}
//
// instruct incI_rReg(rRegI dst, immI_1 src, rFlagsReg cr)
// %{
//   match(Set dst (AddI dst src));
//   effect(KILL cr);
// %}
//
// instruct leaI_rReg_immI(rRegI dst, immI_1 src)
// %{
//   match(Set dst (AddI dst src));
// %}
//
// 1. Simple replacement
// - Only match adjacent instructions in same basic block
// - Only equality constraints
// - Only constraints between operands, not (0.dest_reg == RAX_enc)
// - Only one replacement instruction
//
// // Change (inc mov) to lea
// peephole %{
//   // lea should only be emitted when beneficial
//   peeppredicate( VM_Version::supports_fast_2op_lea() );
//   // increment preceded by register-register move
//   peepmatch ( incI_rReg movI );
//   // require that the destination register of the increment
//   // match the destination register of the move
//   peepconstraint ( 0.dst == 1.dst );
//   // construct a replacement instruction that sets
//   // the destination to ( move's source register + one )
//   peepreplace ( leaI_rReg_immI( 0.dst 1.src 0.src ) );
// %}
//
// 2. Procedural replacement
// - More flexible finding relevent nodes
// - More flexible constraints
// - More flexible transformations
// - May utilise architecture-dependent API more effectively
// - Currently only one replacement instruction due to adlc parsing capabilities
//
// // Change (inc mov) to lea
// peephole %{
//   // lea should only be emitted when beneficial
//   peeppredicate( VM_Version::supports_fast_2op_lea() );
//   // the rule numbers of these nodes inside are passed into the function below
//   peepmatch ( incI_rReg movI );
//   // the method that takes the responsibility of transformation
//   peepprocedure ( inc_mov_to_lea );
//   // the replacement is a leaI_rReg_immI, a lambda upon invoked creating this
//   // node is passed into the function above
//   peepreplace ( leaI_rReg_immI() );
// %}

// These instructions is not matched by the matcher but used by the peephole
instruct leaI_rReg_rReg_peep(rRegI dst, rRegI src1, rRegI src2)
%{
  predicate(false);
  match(Set dst (AddI src1 src2));
  format %{ "leal    $dst, [$src1 + $src2]" %}
  ins_encode %{
    Register dst = $dst$$Register;
    Register src1 = $src1$$Register;
    Register src2 = $src2$$Register;
    if (src1 != rbp && src1 != r13) {
      __ leal(dst, Address(src1, src2, Address::times_1));
    } else {
      assert(src2 != rbp && src2 != r13, "");
      __ leal(dst, Address(src2, src1, Address::times_1));
    }
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct leaI_rReg_immI_peep(rRegI dst, rRegI src1, immI src2)
%{
  predicate(false);
  match(Set dst (AddI src1 src2));
  format %{ "leal    $dst, [$src1 + $src2]" %}
  ins_encode %{
    __ leal($dst$$Register, Address($src1$$Register, $src2$$constant));
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct leaI_rReg_immI2_peep(rRegI dst, rRegI src, immI2 shift)
%{
  predicate(false);
  match(Set dst (LShiftI src shift));
  format %{ "leal    $dst, [$src << $shift]" %}
  ins_encode %{
    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($shift$$constant);
    Register src = $src$$Register;
    if (scale == Address::times_2 && src != rbp && src != r13) {
      __ leal($dst$$Register, Address(src, src, Address::times_1));
    } else {
      __ leal($dst$$Register, Address(noreg, src, scale));
    }
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct leaL_rReg_rReg_peep(rRegL dst, rRegL src1, rRegL src2)
%{
  predicate(false);
  match(Set dst (AddL src1 src2));
  format %{ "leaq    $dst, [$src1 + $src2]" %}
  ins_encode %{
    Register dst = $dst$$Register;
    Register src1 = $src1$$Register;
    Register src2 = $src2$$Register;
    if (src1 != rbp && src1 != r13) {
      __ leaq(dst, Address(src1, src2, Address::times_1));
    } else {
      assert(src2 != rbp && src2 != r13, "");
      __ leaq(dst, Address(src2, src1, Address::times_1));
    }
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct leaL_rReg_immL32_peep(rRegL dst, rRegL src1, immL32 src2)
%{
  predicate(false);
  match(Set dst (AddL src1 src2));
  format %{ "leaq    $dst, [$src1 + $src2]" %}
  ins_encode %{
    __ leaq($dst$$Register, Address($src1$$Register, $src2$$constant));
  %}
  ins_pipe(ialu_reg_reg);
%}

instruct leaL_rReg_immI2_peep(rRegL dst, rRegL src, immI2 shift)
%{
  predicate(false);
  match(Set dst (LShiftL src shift));
  format %{ "leaq    $dst, [$src << $shift]" %}
  ins_encode %{
    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($shift$$constant);
    Register src = $src$$Register;
    if (scale == Address::times_2 && src != rbp && src != r13) {
      __ leaq($dst$$Register, Address(src, src, Address::times_1));
    } else {
      __ leaq($dst$$Register, Address(noreg, src, scale));
    }
  %}
  ins_pipe(ialu_reg_reg);
%}

// These peephole rules replace mov + I pairs (where I is one of {add, inc, dec,
// sal}) with lea instructions. The {add, sal} rules are beneficial in
// processors with at least partial ALU support for lea
// (supports_fast_2op_lea()), whereas the {inc, dec} rules are only generally
// beneficial for processors with full ALU support
// (VM_Version::supports_fast_3op_lea()) and Intel Cascade Lake.

peephole
%{
  peeppredicate(VM_Version::supports_fast_2op_lea());
  peepmatch (addI_rReg);
  peepprocedure (lea_coalesce_reg);
  peepreplace (leaI_rReg_rReg_peep());
%}

peephole
%{
  peeppredicate(VM_Version::supports_fast_2op_lea());
  peepmatch (addI_rReg_imm);
  peepprocedure (lea_coalesce_imm);
  peepreplace (leaI_rReg_immI_peep());
%}

peephole
%{
  peeppredicate(VM_Version::supports_fast_3op_lea() ||
                VM_Version::is_intel_cascade_lake());
  peepmatch (incI_rReg);
  peepprocedure (lea_coalesce_imm);
  peepreplace (leaI_rReg_immI_peep());
%}

peephole
%{
  peeppredicate(VM_Version::supports_fast_3op_lea() ||
                VM_Version::is_intel_cascade_lake());
  peepmatch (decI_rReg);
  peepprocedure (lea_coalesce_imm);
  peepreplace (leaI_rReg_immI_peep());
%}

peephole
%{
  peeppredicate(VM_Version::supports_fast_2op_lea());
  peepmatch (salI_rReg_immI2);
  peepprocedure (lea_coalesce_imm);
  peepreplace (leaI_rReg_immI2_peep());
%}

peephole
%{
  peeppredicate(VM_Version::supports_fast_2op_lea());
  peepmatch (addL_rReg);
  peepprocedure (lea_coalesce_reg);
  peepreplace (leaL_rReg_rReg_peep());
%}

peephole
%{
  peeppredicate(VM_Version::supports_fast_2op_lea());
  peepmatch (addL_rReg_imm);
  peepprocedure (lea_coalesce_imm);
  peepreplace (leaL_rReg_immL32_peep());
%}

peephole
%{
  peeppredicate(VM_Version::supports_fast_3op_lea() ||
                VM_Version::is_intel_cascade_lake());
  peepmatch (incL_rReg);
  peepprocedure (lea_coalesce_imm);
  peepreplace (leaL_rReg_immL32_peep());
%}

peephole
%{
  peeppredicate(VM_Version::supports_fast_3op_lea() ||
                VM_Version::is_intel_cascade_lake());
  peepmatch (decL_rReg);
  peepprocedure (lea_coalesce_imm);
  peepreplace (leaL_rReg_immL32_peep());
%}

peephole
%{
  peeppredicate(VM_Version::supports_fast_2op_lea());
  peepmatch (salL_rReg_immI2);
  peepprocedure (lea_coalesce_imm);
  peepreplace (leaL_rReg_immI2_peep());
%}

peephole
%{
  peepmatch (leaPCompressedOopOffset);
  peepprocedure (lea_remove_redundant);
%}

peephole
%{
  peepmatch (leaP8Narrow);
  peepprocedure (lea_remove_redundant);
%}

peephole
%{
  peepmatch (leaP32Narrow);
  peepprocedure (lea_remove_redundant);
%}

// These peephole rules matches instructions which set flags and are followed by a testI/L_reg
// The test instruction is redudanent in case the downstream instuctions (like JCC or CMOV) only use flags that are already set by the previous instruction

//int variant
peephole
%{
  peepmatch (testI_reg);
  peepprocedure (test_may_remove);
%}

//long variant
peephole
%{
  peepmatch (testL_reg);
  peepprocedure (test_may_remove);
%}


//----------SMARTSPILL RULES---------------------------------------------------
// These must follow all instruction definitions as they use the names
// defined in the instructions definitions.
